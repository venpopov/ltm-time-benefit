{
  "hash": "b0d00d3e3b12cd7e7af66c3d9242f71e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Main results\"\nformat: html\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(targets)\nlibrary(GGally)\nlibrary(kableExtra)\n# load \"R/*\" scripts and saved R objects from the targets pi\ntar_source()\ntar_load(c(exp1_data, exp2_data, exp1_data_agg, exp2_data_agg, fits1))\n```\n:::\n\n\n## Overview\n\nHere I apply the model described in the May 13th draft of the paper to the data. I will first ignore the first chunk in the optimization, then include it. I will also try different priors on the parameters to understand the paramater space. Final results from different choices summarized at the end.\n\n## Ignoring first chunk in the optimiziation\n\n### Basic estimation of Exp1\n\nLet's apply the modeling approach reported in the paper. We ignore the first chunk (SP1-3) while evaluating the likelihood. Eda did this because the model as implemented predicts the same performance for known and random chunks.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntar_load(exp1_data_agg)\nstart <- paper_params()\n(est <- estimate_model(start, data = exp1_data_agg, exclude_sp1 = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$start\n    prop prop_ltm      tau     gain     rate \n    0.21     0.55     0.14    25.00     0.02 \n\n$par\n        prop     prop_ltm          tau         gain         rate \n 0.108898668  0.575632670  0.091783835 95.188660503  0.009362919 \n\n$convergence\n[1] 0\n\n$counts\nfunction gradient \n    2080       NA \n\n$value\n[1] 31.92645\n```\n\n\n:::\n\n```{.r .cell-code}\nexp1_data_agg$pred <- predict(est, exp1_data_agg, group_by = c(\"chunk\", \"gap\"))\nexp1_data_agg |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/model 1-1.png){width=672}\n:::\n:::\n\n\nI get startlingly different paramemter estimates. Much lower `prop` and `rate` and `tau`, higher `gain`.\n\n### Trying different starting values\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the fits of the first simulation, calculate the deviance(s) and predictions\ntar_load(fits1)\nfits1 <- fits1 |>\n  mutate(\n    deviance = pmap_dbl(\n      list(fit, data, exclude_sp1),\n      ~ overall_deviance(params = `..1`$par, data = `..2`, exclude_sp1 = `..3`)\n    ),\n    pred = map2(fit, data, ~ predict(.x, .y, group_by = c(\"chunk\", \"gap\")))\n  )\n```\n:::\n\n\nI've run this with many different starting values. We tend to end up in different regions of the parameter space (the top result close to the paper's estimates):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits1 |>\n  filter(priors_scenario == \"none\", exclude_sp1 == TRUE, exp == 1, deviance <= 50, convergence == 0) |>\n  select(prop:convergence) |>\n  arrange(gain) |>\n  mutate_all(round, 3) |>\n  print(n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 55 × 7\n    prop prop_ltm  rate   tau  gain deviance convergence\n   <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl>\n 1 0.176    0.582 0.015 0.133  38.2     32.4           0\n 2 0.175    0.578 0.015 0.132  38.8     32.4           0\n 3 0.171    0.582 0.015 0.13   40.3     32.4           0\n 4 0.165    0.581 0.014 0.127  43.1     32.3           0\n 5 0.137    0.578 0.012 0.11   61.8     32.1           0\n 6 0.126    0.577 0.011 0.103  71.8     32.0           0\n 7 0.125    0.577 0.011 0.103  72.6     32.0           0\n 8 0.123    0.577 0.011 0.101  75.2     32.0           0\n 9 0.118    0.576 0.01  0.098  81.8     32.0           0\n10 0.118    0.577 0.01  0.098  82.0     32.0           0\n11 0.117    0.576 0.01  0.097  83.3     32.0           0\n12 0.106    0.575 0.009 0.09   99.4     31.9           0\n13 0.106    0.575 0.009 0.09   99.6     31.9           0\n14 0.106    0.576 0.009 0.09   99.7     31.9           0\n15 0.106    0.575 0.009 0.09   99.8     31.9           0\n16 0.106    0.576 0.009 0.09   99.8     31.9           0\n17 0.106    0.576 0.009 0.09   99.9     31.9           0\n18 0.106    0.575 0.009 0.09   99.9     31.9           0\n19 0.106    0.576 0.009 0.09   99.9     31.9           0\n20 0.106    0.575 0.009 0.09   99.9     31.9           0\n21 0.106    0.575 0.009 0.09   99.9     31.9           0\n22 0.106    0.575 0.009 0.09  100.      31.9           0\n23 0.106    0.576 0.009 0.09  100.      31.9           0\n24 0.106    0.576 0.009 0.09  100.      31.9           0\n25 0.106    0.576 0.009 0.09  100.      31.9           0\n26 0.106    0.576 0.009 0.09  100.      31.9           0\n27 0.106    0.576 0.009 0.09  100.      31.9           0\n28 0.106    0.577 0.009 0.09  100.      31.9           0\n29 0.106    0.575 0.009 0.09  100.      31.9           0\n30 0.106    0.576 0.009 0.09  100.      31.9           0\n31 0.106    0.574 0.009 0.09  100.      31.9           0\n32 0.106    0.575 0.009 0.09  100.      31.9           0\n33 0.106    0.576 0.009 0.09  100.      31.9           0\n34 0.106    0.575 0.009 0.09  100.      31.9           0\n35 0.106    0.575 0.009 0.09  100.      31.9           0\n36 0.106    0.576 0.009 0.09  100.      31.9           0\n37 0.106    0.574 0.009 0.09  100.      31.9           0\n38 0.106    0.576 0.009 0.09  100.      31.9           0\n39 0.106    0.576 0.009 0.09  100.      31.9           0\n40 0.106    0.575 0.009 0.09  100.      31.9           0\n41 0.106    0.576 0.009 0.09  100.      31.9           0\n42 0.106    0.575 0.009 0.09  100.      31.9           0\n43 0.106    0.575 0.009 0.09  100.      31.9           0\n44 0.106    0.574 0.009 0.09  100.      31.9           0\n45 0.106    0.576 0.009 0.09  100.      31.9           0\n46 0.106    0.576 0.009 0.09  100.      31.9           0\n47 0.106    0.575 0.009 0.09  100       31.9           0\n48 0.106    0.575 0.009 0.09  100       31.9           0\n49 0.106    0.575 0.009 0.09  100       31.9           0\n50 0.106    0.575 0.009 0.09  100       31.9           0\n51 0.106    0.576 0.009 0.09  100       31.9           0\n52 0.106    0.576 0.009 0.09  100       31.9           0\n53 0.106    0.576 0.009 0.09  100       31.9           0\n54 0.106    0.575 0.009 0.09  100       31.9           0\n55 0.106    0.571 0.009 0.09  100       31.9           0\n```\n\n\n:::\n:::\n\n\n### Trying priors of the gain parameter\n\nOne way to deal with that is to put a prior on the gain parameter to keep it near 25. I know priors are usually a bayesian thing, but they work with ML optimization just as well. On the next set of simulations, I used a Normal(25, 0.1) prior on the gain parameter (could have also fixed it to this value, but this gives me mroe control).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits1 |>\n  filter(priors_scenario == \"gain\", exclude_sp1 == TRUE, exp == 1, deviance <= 50, convergence == 0) |>\n  select(prop:convergence) |>\n  arrange(gain) |>\n  mutate_all(round, 3) |>\n  print(n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 23 × 7\n    prop prop_ltm  rate   tau  gain deviance convergence\n   <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl>\n 1 0.222    0.587 0.019 0.154  25.0     33.0           0\n 2 0.222    0.588 0.018 0.154  25.0     33.0           0\n 3 0.222    0.588 0.019 0.154  25       33.0           0\n 4 0.222    0.588 0.019 0.154  25       33.0           0\n 5 0.222    0.588 0.019 0.154  25       33.0           0\n 6 0.222    0.588 0.019 0.154  25       33.0           0\n 7 0.222    0.588 0.019 0.154  25       33.0           0\n 8 0.222    0.588 0.019 0.154  25       33.0           0\n 9 0.222    0.588 0.019 0.154  25.0     33.0           0\n10 0.222    0.588 0.019 0.154  25.0     33.0           0\n11 0.222    0.588 0.019 0.154  25.0     33.0           0\n12 0.222    0.588 0.019 0.154  25.0     33.0           0\n13 0.222    0.588 0.019 0.154  25.0     33.0           0\n14 0.208    0.398 0.031 0.154  25.0     47.7           0\n15 0.222    0.587 0.019 0.154  25.0     33.0           0\n16 0.222    0.587 0.019 0.154  25.0     33.0           0\n17 0.222    0.587 0.019 0.154  25.0     33.0           0\n18 0.222    0.588 0.019 0.154  25.0     33.0           0\n19 0.222    0.588 0.019 0.154  25.0     33.0           0\n20 0.222    0.588 0.019 0.154  25.0     33.0           0\n21 0.222    0.588 0.019 0.154  25.0     33.0           0\n22 0.222    0.588 0.019 0.154  25.0     33.0           0\n23 0.222    0.588 0.019 0.154  25.0     33.0           0\n```\n\n\n:::\n:::\n\n\nSo we do get at least some parameters that are close to that reported in the paper. The predictions with those parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp1_data_agg$pred <- fits1 |>\n  filter(priors_scenario == \"gain\", exclude_sp1 == TRUE, exp == 1, convergence == 0) |>\n  arrange(deviance) |>\n  pluck(\"pred\", 1)\n\nexp1_data_agg |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n### Trying priors on the rate parameter\n\nMaybe there is a region with a higher rate that we have not explored? Let's try a prior on the rate parameter, ~ Normal(0.1, 0.01).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits1 |>\n  filter(priors_scenario == \"rate\", exclude_sp1 == TRUE, exp == 1, convergence == 0) |>\n  select(prop:convergence) |>\n  arrange(deviance) |>\n  mutate_all(round, 3) |>\n  print(n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 100 × 7\n     prop prop_ltm  rate   tau  gain deviance convergence\n    <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl>\n  1 0.435    0.608 0.049 0.202  7.59     43.1           0\n  2 0.434    0.607 0.049 0.202  7.60     43.1           0\n  3 0.435    0.607 0.049 0.202  7.59     43.1           0\n  4 0.435    0.608 0.049 0.202  7.59     43.1           0\n  5 0.435    0.607 0.049 0.202  7.59     43.1           0\n  6 0.435    0.608 0.049 0.202  7.59     43.1           0\n  7 0.434    0.607 0.049 0.202  7.59     43.1           0\n  8 0.435    0.607 0.049 0.202  7.58     43.1           0\n  9 0.435    0.607 0.049 0.202  7.59     43.1           0\n 10 0.435    0.607 0.049 0.202  7.59     43.1           0\n 11 0.435    0.608 0.049 0.202  7.59     43.1           0\n 12 0.435    0.607 0.049 0.202  7.59     43.1           0\n 13 0.435    0.607 0.049 0.202  7.58     43.1           0\n 14 0.435    0.607 0.049 0.202  7.59     43.1           0\n 15 0.435    0.607 0.049 0.202  7.59     43.1           0\n 16 0.435    0.607 0.049 0.202  7.58     43.1           0\n 17 0.435    0.607 0.049 0.202  7.58     43.1           0\n 18 0.435    0.607 0.049 0.202  7.59     43.1           0\n 19 0.435    0.607 0.049 0.202  7.59     43.1           0\n 20 0.435    0.607 0.049 0.202  7.59     43.1           0\n 21 0.435    0.607 0.049 0.202  7.58     43.1           0\n 22 0.435    0.608 0.049 0.202  7.59     43.1           0\n 23 0.435    0.607 0.049 0.202  7.58     43.1           0\n 24 0.435    0.607 0.049 0.202  7.58     43.1           0\n 25 0.435    0.607 0.049 0.202  7.59     43.1           0\n 26 0.435    0.607 0.049 0.202  7.59     43.1           0\n 27 0.435    0.607 0.049 0.202  7.58     43.1           0\n 28 0.435    0.607 0.049 0.202  7.58     43.1           0\n 29 0.435    0.607 0.049 0.202  7.58     43.1           0\n 30 0.435    0.607 0.049 0.202  7.58     43.1           0\n 31 0.435    0.607 0.049 0.202  7.59     43.1           0\n 32 0.435    0.607 0.049 0.202  7.58     43.1           0\n 33 0.435    0.607 0.049 0.202  7.58     43.1           0\n 34 0.435    0.607 0.049 0.202  7.58     43.1           0\n 35 0.435    0.607 0.049 0.202  7.58     43.1           0\n 36 0.435    0.607 0.049 0.202  7.58     43.1           0\n 37 0.435    0.607 0.049 0.202  7.59     43.1           0\n 38 0.435    0.607 0.049 0.202  7.59     43.1           0\n 39 0.435    0.607 0.049 0.202  7.58     43.1           0\n 40 0.435    0.607 0.049 0.202  7.58     43.1           0\n 41 0.435    0.607 0.049 0.202  7.58     43.1           0\n 42 0.435    0.607 0.049 0.202  7.59     43.1           0\n 43 0.435    0.607 0.049 0.202  7.58     43.1           0\n 44 0.435    0.607 0.049 0.202  7.59     43.1           0\n 45 0.435    0.607 0.049 0.202  7.58     43.1           0\n 46 0.435    0.607 0.049 0.202  7.59     43.1           0\n 47 0.435    0.607 0.049 0.202  7.58     43.1           0\n 48 0.435    0.608 0.049 0.202  7.58     43.1           0\n 49 0.435    0.607 0.049 0.202  7.58     43.1           0\n 50 0.435    0.608 0.049 0.202  7.58     43.1           0\n 51 0.435    0.607 0.049 0.202  7.58     43.1           0\n 52 0.435    0.607 0.049 0.202  7.59     43.1           0\n 53 0.435    0.607 0.049 0.202  7.58     43.1           0\n 54 0.435    0.607 0.049 0.202  7.59     43.1           0\n 55 0.435    0.607 0.049 0.202  7.58     43.1           0\n 56 0.435    0.607 0.049 0.202  7.58     43.1           0\n 57 0.435    0.607 0.049 0.202  7.58     43.1           0\n 58 0.435    0.607 0.049 0.202  7.58     43.1           0\n 59 0.435    0.607 0.049 0.202  7.58     43.1           0\n 60 0.435    0.607 0.049 0.202  7.58     43.1           0\n 61 0.435    0.607 0.049 0.202  7.58     43.1           0\n 62 0.435    0.607 0.049 0.202  7.58     43.1           0\n 63 0.435    0.608 0.049 0.202  7.59     43.1           0\n 64 0.435    0.608 0.049 0.202  7.58     43.1           0\n 65 0.435    0.607 0.049 0.202  7.58     43.1           0\n 66 0.435    0.607 0.049 0.202  7.58     43.1           0\n 67 0.435    0.607 0.049 0.202  7.58     43.1           0\n 68 0.435    0.607 0.049 0.202  7.58     43.1           0\n 69 0.435    0.607 0.049 0.202  7.58     43.1           0\n 70 0.435    0.607 0.049 0.202  7.58     43.1           0\n 71 0.435    0.607 0.049 0.202  7.58     43.2           0\n 72 0.435    0.608 0.049 0.202  7.58     43.2           0\n 73 0.435    0.607 0.049 0.202  7.58     43.2           0\n 74 0.435    0.607 0.049 0.202  7.58     43.2           0\n 75 0.435    0.607 0.049 0.202  7.58     43.2           0\n 76 0.435    0.608 0.049 0.202  7.58     43.2           0\n 77 0.435    0.608 0.049 0.202  7.58     43.2           0\n 78 0.435    0.607 0.049 0.202  7.58     43.2           0\n 79 0.435    0.608 0.049 0.202  7.57     43.2           0\n 80 0.435    0.607 0.049 0.202  7.57     43.2           0\n 81 0.39     0.375 0.071 0.218  7.90     55.8           0\n 82 0.39     0.375 0.071 0.218  7.88     55.8           0\n 83 0.39     0.375 0.071 0.218  7.88     55.8           0\n 84 0.39     0.375 0.071 0.218  7.88     55.8           0\n 85 0.39     0.375 0.071 0.218  7.87     55.8           0\n 86 0.39     0.375 0.071 0.218  7.88     55.8           0\n 87 0.39     0.375 0.071 0.218  7.88     55.8           0\n 88 0.39     0.375 0.071 0.218  7.87     55.8           0\n 89 0.39     0.375 0.071 0.218  7.87     55.8           0\n 90 0.39     0.375 0.071 0.218  7.88     55.8           0\n 91 0.39     0.375 0.071 0.218  7.87     55.8           0\n 92 0.39     0.375 0.071 0.218  7.87     55.8           0\n 93 0.39     0.375 0.071 0.218  7.87     55.8           0\n 94 0.39     0.375 0.071 0.218  7.87     55.8           0\n 95 0.39     0.375 0.071 0.218  7.88     55.8           0\n 96 0.39     0.375 0.071 0.218  7.87     55.8           0\n 97 0.39     0.375 0.071 0.218  7.87     55.8           0\n 98 0.391    0.375 0.071 0.218  7.86     55.8           0\n 99 0.39     0.375 0.071 0.218  7.87     55.8           0\n100 0.386    0.017 0.081 0.226  7.39     75.0           0\n```\n\n\n:::\n:::\n\n\nDeviance is quite much higher. Predictions?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- fits1 |>\n  filter(priors_scenario == \"rate\", exclude_sp1 == TRUE, exp == 1, convergence == 0) |>\n  arrange(deviance) |>\n  pluck(\"fit\", 1)\n\nfit$par\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      prop   prop_ltm       rate        tau       gain \n0.43456497 0.60758909 0.04906632 0.20157962 7.58996852 \n```\n\n\n:::\n\n```{.r .cell-code}\nexp1_data_agg$pred <- predict(fit, exp1_data_agg, group_by = c(\"chunk\", \"gap\"))\n\nexp1_data_agg |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nGeater mismatch. Let's include error bars of the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp1_data |>\n  group_by(id, chunk, gap, itemtype) |>\n  summarise(\n    n_total = dplyr::n(),\n    n_correct = sum(cor),\n    p_correct = mean(cor)\n  ) |>\n  ungroup() |>\n  left_join(\n    select(exp1_data_agg, chunk, gap, itemtype, pred),\n    by = c(\"chunk\", \"gap\", \"itemtype\")\n  ) |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  stat_summary() +\n  stat_summary(aes(y = pred), linetype = \"dashed\", geom = \"line\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'id', 'chunk', 'gap'. You can override using the `.groups` argument.\nNo summary function supplied, defaulting to `mean_se()`\nNo summary function supplied, defaulting to `mean_se()`\nNo summary function supplied, defaulting to `mean_se()`\nNo summary function supplied, defaulting to `mean_se()`\nNo summary function supplied, defaulting to `mean_se()`\nNo summary function supplied, defaulting to `mean_se()`\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nParaneters seem consistent with the data (see [my notes](../notes.qmd)).\n\n## Including the first chunk in the optimization\n\nThe reports above followed the approach in the current draft and excluded the first chunk from the calculation of the likelihood when optimizing the parameters. Let's include it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- paper_params()\n(est <- estimate_model(start, data = exp1_data_agg, exclude_sp1 = FALSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$start\n    prop prop_ltm      tau     gain     rate \n    0.21     0.55     0.14    25.00     0.02 \n\n$par\n       prop    prop_ltm         tau        gain        rate \n 0.30248729  0.63594776  0.17698525 15.21043755  0.02143605 \n\n$convergence\n[1] 0\n\n$counts\nfunction gradient \n    1276       NA \n\n$value\n[1] 144.8785\n```\n\n\n:::\n\n```{.r .cell-code}\nexp1_data_agg$pred <- predict(est, exp1_data_agg, group_by = c(\"chunk\", \"gap\"))\nexp1_data_agg |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nIn this case I didn't have to use many starting values - the result is reached from almost everywhere:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits1 |>\n  filter(priors_scenario == \"none\", exclude_sp1 == FALSE, exp == 1, convergence == 0) |>\n  arrange(deviance) |>\n  select(prop:convergence) |>\n  print(n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 100 × 7\n     prop prop_ltm   rate    tau  gain deviance convergence\n    <dbl>    <dbl>  <dbl>  <dbl> <dbl>    <dbl>       <int>\n  1 0.303    0.636 0.0214 0.177  15.2      145.           0\n  2 0.302    0.636 0.0214 0.177  15.2      145.           0\n  3 0.302    0.636 0.0214 0.177  15.2      145.           0\n  4 0.302    0.636 0.0214 0.177  15.2      145.           0\n  5 0.302    0.636 0.0214 0.177  15.2      145.           0\n  6 0.302    0.636 0.0214 0.177  15.2      145.           0\n  7 0.302    0.636 0.0214 0.177  15.2      145.           0\n  8 0.302    0.636 0.0214 0.177  15.2      145.           0\n  9 0.302    0.636 0.0214 0.177  15.2      145.           0\n 10 0.303    0.636 0.0214 0.177  15.2      145.           0\n 11 0.302    0.636 0.0214 0.177  15.2      145.           0\n 12 0.302    0.636 0.0214 0.177  15.2      145.           0\n 13 0.303    0.636 0.0214 0.177  15.2      145.           0\n 14 0.303    0.636 0.0214 0.177  15.2      145.           0\n 15 0.302    0.636 0.0214 0.177  15.2      145.           0\n 16 0.302    0.636 0.0214 0.177  15.2      145.           0\n 17 0.302    0.636 0.0214 0.177  15.2      145.           0\n 18 0.302    0.636 0.0214 0.177  15.2      145.           0\n 19 0.302    0.636 0.0214 0.177  15.2      145.           0\n 20 0.302    0.636 0.0214 0.177  15.2      145.           0\n 21 0.302    0.636 0.0214 0.177  15.2      145.           0\n 22 0.303    0.636 0.0215 0.177  15.2      145.           0\n 23 0.302    0.636 0.0214 0.177  15.2      145.           0\n 24 0.302    0.636 0.0214 0.177  15.2      145.           0\n 25 0.302    0.636 0.0214 0.177  15.2      145.           0\n 26 0.303    0.636 0.0214 0.177  15.2      145.           0\n 27 0.303    0.636 0.0215 0.177  15.2      145.           0\n 28 0.303    0.636 0.0214 0.177  15.2      145.           0\n 29 0.303    0.636 0.0215 0.177  15.2      145.           0\n 30 0.303    0.636 0.0215 0.177  15.2      145.           0\n 31 0.303    0.636 0.0214 0.177  15.2      145.           0\n 32 0.302    0.636 0.0214 0.177  15.2      145.           0\n 33 0.302    0.636 0.0214 0.177  15.2      145.           0\n 34 0.302    0.636 0.0214 0.177  15.2      145.           0\n 35 0.302    0.636 0.0214 0.177  15.2      145.           0\n 36 0.303    0.636 0.0214 0.177  15.2      145.           0\n 37 0.303    0.636 0.0214 0.177  15.2      145.           0\n 38 0.302    0.636 0.0214 0.177  15.2      145.           0\n 39 0.302    0.636 0.0214 0.177  15.2      145.           0\n 40 0.303    0.636 0.0214 0.177  15.2      145.           0\n 41 0.303    0.636 0.0214 0.177  15.2      145.           0\n 42 0.302    0.636 0.0214 0.177  15.2      145.           0\n 43 0.302    0.636 0.0214 0.177  15.2      145.           0\n 44 0.303    0.636 0.0215 0.177  15.2      145.           0\n 45 0.303    0.636 0.0214 0.177  15.2      145.           0\n 46 0.302    0.636 0.0214 0.177  15.2      145.           0\n 47 0.302    0.636 0.0214 0.177  15.2      145.           0\n 48 0.302    0.636 0.0214 0.177  15.2      145.           0\n 49 0.303    0.636 0.0214 0.177  15.2      145.           0\n 50 0.302    0.636 0.0214 0.177  15.2      145.           0\n 51 0.302    0.636 0.0214 0.177  15.3      145.           0\n 52 0.303    0.636 0.0215 0.177  15.2      145.           0\n 53 0.303    0.636 0.0215 0.177  15.2      145.           0\n 54 0.302    0.636 0.0214 0.177  15.2      145.           0\n 55 0.302    0.636 0.0214 0.177  15.2      145.           0\n 56 0.303    0.636 0.0214 0.177  15.2      145.           0\n 57 0.303    0.636 0.0215 0.177  15.1      145.           0\n 58 0.302    0.636 0.0214 0.177  15.2      145.           0\n 59 0.302    0.636 0.0214 0.177  15.2      145.           0\n 60 0.302    0.636 0.0214 0.177  15.2      145.           0\n 61 0.303    0.636 0.0214 0.177  15.2      145.           0\n 62 0.303    0.636 0.0215 0.177  15.2      145.           0\n 63 0.302    0.636 0.0214 0.177  15.2      145.           0\n 64 0.302    0.636 0.0214 0.177  15.2      145.           0\n 65 0.302    0.636 0.0214 0.177  15.2      145.           0\n 66 0.302    0.636 0.0214 0.177  15.2      145.           0\n 67 0.302    0.636 0.0214 0.177  15.2      145.           0\n 68 0.302    0.636 0.0214 0.177  15.2      145.           0\n 69 0.303    0.636 0.0214 0.177  15.2      145.           0\n 70 0.303    0.636 0.0214 0.177  15.2      145.           0\n 71 0.303    0.636 0.0214 0.177  15.2      145.           0\n 72 0.302    0.636 0.0214 0.177  15.2      145.           0\n 73 0.302    0.636 0.0214 0.177  15.2      145.           0\n 74 0.302    0.636 0.0214 0.177  15.2      145.           0\n 75 0.302    0.636 0.0214 0.177  15.2      145.           0\n 76 0.303    0.636 0.0214 0.177  15.2      145.           0\n 77 0.302    0.636 0.0214 0.177  15.3      145.           0\n 78 0.548    0.515 0.258  0.285   5.92     381.           0\n 79 0.548    0.534 0.291  0.295   6.16     381.           0\n 80 0.548    0.512 0.253  0.283   5.89     381.           0\n 81 0.548    0.535 0.294  0.296   6.18     381.           0\n 82 0.549    0.551 0.321  0.305   6.40     381.           0\n 83 0.549    0.491 0.217  0.272   5.64     381.           0\n 84 0.549    0.549 0.318  0.304   6.37     381.           0\n 85 0.549    0.569 0.353  0.315   6.67     381.           0\n 86 0.548    0.536 0.294  0.296   6.18     381.           0\n 87 0.548    0.512 0.254  0.283   5.89     381.           0\n 88 0.548    0.575 0.362  0.317   6.76     381.           0\n 89 0.548    0.492 0.220  0.273   5.66     381.           0\n 90 0.548    0.515 0.260  0.285   5.93     381.           0\n 91 0.549    0.554 0.327  0.306   6.44     381.           0\n 92 0.548    0.604 0.413  0.334   7.26     381.           0\n 93 0.548    0.517 0.263  0.286   5.95     381.           0\n 94 0.548    0.565 0.345  0.312   6.60     381.           0\n 95 0.548    0.540 0.303  0.299   6.25     381.           0\n 96 0.548    0.513 0.256  0.284   5.91     381.           0\n 97 0.548    0.560 0.337  0.309   6.53     381.           0\n 98 0.549    0.556 0.330  0.307   6.48     381.           0\n 99 0.548    0.556 0.330  0.308   6.48     381.           0\n100 0.106    0.473 0.275  0.0736 24.3     1663.           0\n```\n\n\n:::\n:::\n\n\n\n### With prior on rate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits1 |>\n  filter(priors_scenario == \"rate\", exclude_sp1 == FALSE, exp == 1, convergence == 0) |>\n  mutate(deviance = map2_dbl(fit, exclude_sp1, function(x, y) {\n    overall_deviance(x$par, exp1_data_agg, exclude_sp1 = y)\n  })) |>\n  select(prop:convergence) |>\n  arrange(deviance) |>\n  mutate_all(round, 3) |>\n  print(n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 98 × 7\n    prop prop_ltm  rate   tau  gain deviance convergence\n   <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl>\n 1 0.475    0.645 0.049 0.197  6.91     153.           0\n 2 0.475    0.645 0.049 0.197  6.91     153.           0\n 3 0.475    0.645 0.049 0.197  6.91     153.           0\n 4 0.475    0.645 0.049 0.197  6.91     153.           0\n 5 0.475    0.645 0.049 0.197  6.91     153.           0\n 6 0.475    0.645 0.049 0.197  6.91     153.           0\n 7 0.475    0.644 0.049 0.197  6.90     153.           0\n 8 0.475    0.644 0.049 0.197  6.91     153.           0\n 9 0.475    0.644 0.049 0.197  6.91     153.           0\n10 0.475    0.645 0.049 0.197  6.90     153.           0\n11 0.475    0.644 0.049 0.197  6.91     153.           0\n12 0.475    0.645 0.049 0.197  6.91     153.           0\n13 0.475    0.645 0.049 0.197  6.91     153.           0\n14 0.475    0.644 0.049 0.197  6.91     153.           0\n15 0.475    0.645 0.049 0.197  6.91     153.           0\n16 0.475    0.645 0.049 0.197  6.91     153.           0\n17 0.475    0.644 0.049 0.197  6.91     153.           0\n18 0.475    0.645 0.049 0.197  6.91     153.           0\n19 0.475    0.645 0.049 0.197  6.91     153.           0\n20 0.475    0.645 0.049 0.197  6.91     153.           0\n21 0.475    0.645 0.049 0.197  6.90     153.           0\n22 0.475    0.644 0.049 0.197  6.91     153.           0\n23 0.475    0.645 0.049 0.197  6.91     153.           0\n24 0.475    0.645 0.049 0.197  6.90     153.           0\n25 0.475    0.645 0.049 0.197  6.90     153.           0\n26 0.475    0.645 0.049 0.197  6.90     153.           0\n27 0.475    0.645 0.049 0.197  6.91     153.           0\n28 0.475    0.645 0.049 0.197  6.90     153.           0\n29 0.475    0.645 0.049 0.197  6.91     153.           0\n30 0.475    0.645 0.049 0.197  6.91     153.           0\n31 0.475    0.644 0.049 0.197  6.91     153.           0\n32 0.475    0.645 0.049 0.197  6.91     153.           0\n33 0.475    0.645 0.049 0.197  6.91     153.           0\n34 0.475    0.645 0.049 0.197  6.91     153.           0\n35 0.475    0.645 0.049 0.197  6.90     153.           0\n36 0.475    0.644 0.049 0.197  6.91     153.           0\n37 0.475    0.645 0.049 0.197  6.91     153.           0\n38 0.475    0.645 0.049 0.197  6.90     153.           0\n39 0.475    0.645 0.049 0.197  6.91     153.           0\n40 0.475    0.644 0.049 0.197  6.90     153.           0\n41 0.475    0.645 0.049 0.197  6.91     153.           0\n42 0.475    0.645 0.049 0.197  6.90     153.           0\n43 0.475    0.644 0.049 0.197  6.91     153.           0\n44 0.475    0.645 0.049 0.197  6.90     153.           0\n45 0.475    0.645 0.049 0.197  6.90     153.           0\n46 0.475    0.645 0.049 0.197  6.90     153.           0\n47 0.475    0.644 0.049 0.197  6.91     153.           0\n48 0.475    0.645 0.049 0.197  6.91     153.           0\n49 0.475    0.645 0.049 0.197  6.90     153.           0\n50 0.475    0.645 0.049 0.197  6.90     153.           0\n51 0.475    0.645 0.049 0.197  6.91     153.           0\n52 0.475    0.645 0.049 0.197  6.91     153.           0\n53 0.475    0.644 0.049 0.197  6.91     153.           0\n54 0.475    0.645 0.049 0.197  6.90     153.           0\n55 0.475    0.645 0.049 0.197  6.91     153.           0\n56 0.475    0.644 0.049 0.197  6.91     153.           0\n57 0.475    0.645 0.049 0.197  6.90     153.           0\n58 0.475    0.644 0.049 0.197  6.91     153.           0\n59 0.475    0.644 0.049 0.197  6.90     153.           0\n60 0.475    0.645 0.049 0.197  6.91     153.           0\n61 0.475    0.645 0.049 0.197  6.90     153.           0\n62 0.475    0.645 0.049 0.197  6.90     153.           0\n63 0.475    0.645 0.049 0.197  6.90     153.           0\n64 0.475    0.645 0.049 0.197  6.90     153.           0\n65 0.475    0.645 0.049 0.197  6.90     153.           0\n66 0.475    0.645 0.049 0.197  6.91     153.           0\n67 0.475    0.645 0.049 0.197  6.90     153.           0\n68 0.475    0.645 0.049 0.197  6.91     153.           0\n69 0.475    0.644 0.049 0.197  6.90     153.           0\n70 0.475    0.645 0.049 0.197  6.90     153.           0\n71 0.475    0.645 0.049 0.197  6.90     153.           0\n72 0.476    0.644 0.049 0.197  6.89     153.           0\n73 0.475    0.645 0.049 0.197  6.91     153.           0\n74 0.475    0.645 0.049 0.197  6.90     153.           0\n75 0.475    0.645 0.049 0.197  6.90     153.           0\n76 0.475    0.645 0.049 0.197  6.90     153.           0\n77 0.475    0.644 0.049 0.197  6.90     153.           0\n78 0.475    0.644 0.049 0.197  6.90     153.           0\n79 0.475    0.644 0.049 0.197  6.91     153.           0\n80 0.475    0.645 0.049 0.197  6.90     153.           0\n81 0.475    0.645 0.049 0.197  6.90     153.           0\n82 0.475    0.644 0.049 0.197  6.90     153.           0\n83 0.475    0.645 0.049 0.197  6.91     153.           0\n84 0.475    0.645 0.049 0.197  6.90     153.           0\n85 0.475    0.645 0.049 0.197  6.91     153.           0\n86 0.475    0.645 0.049 0.197  6.90     153.           0\n87 0.475    0.645 0.049 0.197  6.90     153.           0\n88 0.476    0.644 0.049 0.197  6.90     153.           0\n89 0.475    0.645 0.049 0.197  6.90     153.           0\n90 0.475    0.644 0.049 0.197  6.90     153.           0\n91 0.475    0.645 0.049 0.197  6.90     153.           0\n92 0.476    0.644 0.049 0.197  6.9      153.           0\n93 0.475    0.645 0.049 0.197  6.91     153.           0\n94 0.476    0.645 0.049 0.197  6.90     153.           0\n95 0.475    0.645 0.049 0.197  6.90     153.           0\n96 0.476    0.644 0.049 0.197  6.90     153.           0\n97 0.476    0.645 0.049 0.197  6.89     153.           0\n98 0.476    0.645 0.049 0.197  6.90     153.           0\n```\n\n\n:::\n\n```{.r .cell-code}\nfit <- fits1 |>\n  filter(priors_scenario == \"rate\", exclude_sp1 == FALSE, exp == 1, convergence == 0) |>\n  arrange(deviance) |>\n  pluck(\"fit\", 1)\n\nexp1_data_agg$pred <- predict(fit, exp1_data_agg, group_by = c(\"chunk\", \"gap\"))\n\nexp1_data_agg |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n## Repeat for expereiment 2\n\nBasic estimation (ignoring first chunk):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntar_load(exp2_data_agg)\nstart <- paper_params(exp = 2)\n(est <- estimate_model(start, data = exp2_data_agg, exclude_sp1 = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$start\n    prop prop_ltm      tau     gain     rate \n   0.170    0.400    0.135   25.000    0.025 \n\n$par\n       prop    prop_ltm         tau        gain        rate \n 0.16975119  0.48023531  0.13651974 29.75877754  0.02233212 \n\n$convergence\n[1] 0\n\n$counts\nfunction gradient \n     246       NA \n\n$value\n[1] 56.36045\n```\n\n\n:::\n\n```{.r .cell-code}\nexp2_data_agg$pred <- predict(est, exp2_data_agg, group_by = c(\"chunk\", \"gap\"))\nexp2_data_agg |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nagain parameter estimates are different from the paper.\n\nHere are from multiple starting values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits <- fits1 |>\n  filter(priors_scenario == \"none\", exclude_sp1 == TRUE, exp == 2, convergence == 0) |>\n  select(prop:convergence, fit, data) |>\n  arrange(deviance) |>\n  mutate_if(is.numeric, round, 3) |>\n  print(n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 98 × 9\n    prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n   <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl> <list>     <list>           \n 1 0.105    0.815 0.007 0.089  87.8     40.5           0 <srl_rcl_> <tibble [12 × 8]>\n 2 0.105    0.816 0.007 0.089  86.9     40.5           0 <srl_rcl_> <tibble [12 × 8]>\n 3 0.105    0.815 0.007 0.089  87.7     40.5           0 <srl_rcl_> <tibble [12 × 8]>\n 4 0.105    0.816 0.007 0.089  87.2     40.5           0 <srl_rcl_> <tibble [12 × 8]>\n 5 0.105    0.815 0.007 0.089  88.1     40.5           0 <srl_rcl_> <tibble [12 × 8]>\n 6 0.106    0.816 0.007 0.09   85.3     40.5           0 <srl_rcl_> <tibble [12 × 8]>\n 7 0.109    0.816 0.007 0.092  81.6     40.5           0 <srl_rcl_> <tibble [12 × 8]>\n 8 0.128    0.818 0.008 0.105  59.5     40.5           0 <srl_rcl_> <tibble [12 × 8]>\n 9 0.135    0.818 0.009 0.109  54.5     40.5           0 <srl_rcl_> <tibble [12 × 8]>\n10 0.152    0.819 0.01  0.119  43.3     40.5           0 <srl_rcl_> <tibble [12 × 8]>\n11 0.162    0.481 0.021 0.132  32.4     56.4           0 <srl_rcl_> <tibble [12 × 8]>\n12 0.162    0.481 0.021 0.132  32.4     56.4           0 <srl_rcl_> <tibble [12 × 8]>\n13 0.162    0.481 0.021 0.132  32.4     56.4           0 <srl_rcl_> <tibble [12 × 8]>\n14 0.162    0.481 0.021 0.132  32.4     56.4           0 <srl_rcl_> <tibble [12 × 8]>\n15 0.162    0.481 0.021 0.132  32.5     56.4           0 <srl_rcl_> <tibble [12 × 8]>\n16 0.162    0.481 0.021 0.132  32.6     56.4           0 <srl_rcl_> <tibble [12 × 8]>\n17 0.163    0.481 0.021 0.132  32.4     56.4           0 <srl_rcl_> <tibble [12 × 8]>\n18 0.163    0.48  0.021 0.132  32.1     56.4           0 <srl_rcl_> <tibble [12 × 8]>\n19 0.163    0.48  0.021 0.132  32.3     56.4           0 <srl_rcl_> <tibble [12 × 8]>\n20 0.163    0.48  0.021 0.132  32.3     56.4           0 <srl_rcl_> <tibble [12 × 8]>\n21 0.162    0.481 0.021 0.132  32.4     56.4           0 <srl_rcl_> <tibble [12 × 8]>\n22 0.271    0.505 0.12  0.205  13.9     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n23 0.271    0.526 0.138 0.208  14.5     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n24 0.271    0.656 0.249 0.225  20.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n25 0.271    0.564 0.171 0.213  15.8     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n26 0.271    0.487 0.105 0.203  13.4     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n27 0.271    0.521 0.134 0.207  14.3     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n28 0.271    0.795 0.367 0.244  33.5     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n29 0.271    0.628 0.225 0.222  18.4     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n30 0.271    0.721 0.304 0.234  24.6     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n31 0.271    0.787 0.361 0.243  32.2     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n32 0.271    0.755 0.333 0.238  28.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n33 0.271    0.544 0.154 0.21   15.1     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n34 0.271    0.531 0.143 0.209  14.6     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n35 0.271    0.723 0.306 0.234  24.7     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n36 0.271    0.645 0.239 0.224  19.3     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n37 0.271    0.591 0.193 0.217  16.8     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n38 0.271    0.591 0.194 0.217  16.8     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n39 0.271    0.654 0.247 0.225  19.9     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n40 0.271    0.645 0.24  0.224  19.4     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n41 0.271    0.571 0.176 0.214  16.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n42 0.271    0.759 0.336 0.239  28.5     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n43 0.271    0.635 0.231 0.223  18.8     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n44 0.271    0.73  0.312 0.235  25.4     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n45 0.271    0.718 0.302 0.234  24.4     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n46 0.271    0.593 0.195 0.217  16.8     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n47 0.271    0.622 0.219 0.221  18.1     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n48 0.271    0.707 0.293 0.232  23.4     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n49 0.271    0.495 0.111 0.204  13.6     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n50 0.271    0.643 0.238 0.224  19.2     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n51 0.271    0.718 0.302 0.234  24.3     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n52 0.271    0.648 0.242 0.224  19.5     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n53 0.271    0.72  0.304 0.234  24.5     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n54 0.271    0.666 0.257 0.227  20.5     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n55 0.271    0.601 0.202 0.218  17.2     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n56 0.271    0.45  0.073 0.198  12.5     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n57 0.271    0.561 0.168 0.213  15.6     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n58 0.271    0.527 0.139 0.208  14.5     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n59 0.271    0.532 0.143 0.209  14.7     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n60 0.271    0.802 0.373 0.245  34.7     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n61 0.271    0.627 0.224 0.221  18.4     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n62 0.271    0.619 0.218 0.22   18.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n63 0.271    0.62  0.218 0.221  18.1     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n64 0.271    0.575 0.18  0.215  16.2     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n65 0.271    0.645 0.24  0.224  19.3     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n66 0.271    0.507 0.121 0.205  13.9     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n67 0.271    0.635 0.23  0.222  18.8     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n68 0.271    0.724 0.307 0.235  24.9     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n69 0.271    0.711 0.296 0.233  23.8     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n70 0.271    0.485 0.104 0.203  13.4     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n71 0.271    0.64  0.236 0.223  19.1     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n72 0.271    0.841 0.406 0.25   43.2     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n73 0.271    0.684 0.273 0.229  21.8     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n74 0.271    0.539 0.149 0.21   14.9     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n75 0.27     0.812 0.381 0.246  36.7     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n76 0.271    0.57  0.175 0.214  16.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n77 0.271    0.852 0.416 0.251  46.3     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n78 0.271    0.425 0.052 0.195  11.9     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n79 0.271    0.561 0.168 0.213  15.6     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n80 0.271    0.48  0.099 0.202  13.2     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n81 0.271    0.656 0.249 0.225  20.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n82 0.271    0.743 0.323 0.237  26.6     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n83 0.271    0.678 0.268 0.228  21.3     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n84 0.271    0.592 0.194 0.217  16.8     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n85 0.271    0.624 0.222 0.221  18.2     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n86 0.271    0.742 0.322 0.237  26.6     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n87 0.271    0.671 0.262 0.227  20.9     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n88 0.271    0.508 0.122 0.206  13.9     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n89 0.271    0.537 0.147 0.21   14.8     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n90 0.271    0.572 0.177 0.214  16.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n91 0.271    0.541 0.151 0.21   15.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n92 0.271    0.667 0.258 0.227  20.6     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n93 0.271    0.833 0.4   0.249  41.2     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n94 0.348    0.266 0.289 0.272  10.7     84.2           0 <srl_rcl_> <tibble [12 × 8]>\n95 0.348    0.279 0.291 0.272  10.8     84.2           0 <srl_rcl_> <tibble [12 × 8]>\n96 0.584    0.725 0.218 1       0      623.            0 <srl_rcl_> <tibble [12 × 8]>\n97 0.616    0.715 0.273 1       0      623.            0 <srl_rcl_> <tibble [12 × 8]>\n98 0.612    0.541 0.248 1       0      623.            0 <srl_rcl_> <tibble [12 × 8]>\n```\n\n\n:::\n:::\n\n\n### Problem with parameter identifiability\n\nrows 12-16 illustrate the problem with parameter identifiability quite well. They have nearly identical deviance, but very different parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(fits <- fits[c(12, 13, 14, 15, 16), ])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 9\n   prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n  <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl> <list>     <list>           \n1 0.162    0.481 0.021 0.132  32.4     56.4           0 <srl_rcl_> <tibble [12 × 8]>\n2 0.162    0.481 0.021 0.132  32.4     56.4           0 <srl_rcl_> <tibble [12 × 8]>\n3 0.162    0.481 0.021 0.132  32.4     56.4           0 <srl_rcl_> <tibble [12 × 8]>\n4 0.162    0.481 0.021 0.132  32.5     56.4           0 <srl_rcl_> <tibble [12 × 8]>\n5 0.162    0.481 0.021 0.132  32.6     56.4           0 <srl_rcl_> <tibble [12 × 8]>\n```\n\n\n:::\n\n```{.r .cell-code}\n# saveRDS(fits, \"output/five_parsets_exp2.rds\")\n```\n:::\n\n\nPlot the predictions all 5 sets of parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits |>\n  mutate(pred = map2(fit, data, \\(x, y) predict(x, y, group_by = c(\"chunk\", \"gap\")))) |>\n  unnest(c(data, pred)) |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = as.factor(round(prop, 3)))) +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_grid(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThe way parameters change suggest that increasing prop can be compensated by increasing rate, taun and decreasing gain. Here's a pair plot of these parameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits |>\n  select(prop, rate, tau, gain) |>\n  ggpairs(diag = list(continuous = \"blankDiag\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in cor(x, y): the standard deviation is zero\nWarning in cor(x, y): the standard deviation is zero\nWarning in cor(x, y): the standard deviation is zero\nWarning in cor(x, y): the standard deviation is zero\nWarning in cor(x, y): the standard deviation is zero\nWarning in cor(x, y): the standard deviation is zero\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nI'll investigate this in a separate notebook.\n\n### With prior on gain\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- fits1 |>\n  filter(priors_scenario == \"gain\", exclude_sp1 == TRUE, exp == 2, convergence == 0) |>\n  mutate(deviance = map2_dbl(fit, exclude_sp1, function(x, y) {\n    overall_deviance(x$par, exp2_data_agg, exclude_sp1 = y)\n  })) |>\n  select(prop:convergence, fit, data) |>\n  arrange(deviance) |>\n  mutate_if(is.numeric, round, 3) |>\n  print(n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 98 × 9\n    prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n   <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl> <list>     <list>           \n 1 0.205    0.824 0.013 0.146  25.0     40.7           0 <srl_rcl_> <tibble [12 × 8]>\n 2 0.205    0.824 0.013 0.146  25.0     40.7           0 <srl_rcl_> <tibble [12 × 8]>\n 3 0.205    0.824 0.013 0.146  25.0     40.7           0 <srl_rcl_> <tibble [12 × 8]>\n 4 0.205    0.824 0.013 0.146  25.0     40.7           0 <srl_rcl_> <tibble [12 × 8]>\n 5 0.204    0.824 0.013 0.146  25.0     40.7           0 <srl_rcl_> <tibble [12 × 8]>\n 6 0.205    0.824 0.013 0.146  25.0     40.7           0 <srl_rcl_> <tibble [12 × 8]>\n 7 0.186    0.479 0.025 0.146  25       56.4           0 <srl_rcl_> <tibble [12 × 8]>\n 8 0.186    0.479 0.025 0.146  25       56.4           0 <srl_rcl_> <tibble [12 × 8]>\n 9 0.186    0.479 0.025 0.146  25.0     56.4           0 <srl_rcl_> <tibble [12 × 8]>\n10 0.186    0.479 0.025 0.146  25.0     56.4           0 <srl_rcl_> <tibble [12 × 8]>\n11 0.186    0.479 0.025 0.146  25       56.4           0 <srl_rcl_> <tibble [12 × 8]>\n12 0.186    0.48  0.025 0.146  25.0     56.4           0 <srl_rcl_> <tibble [12 × 8]>\n13 0.271    0.726 0.309 0.235  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n14 0.271    0.726 0.308 0.235  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n15 0.271    0.725 0.308 0.234  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n16 0.271    0.726 0.308 0.235  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n17 0.272    0.727 0.31  0.235  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n18 0.272    0.727 0.31  0.235  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n19 0.272    0.726 0.31  0.235  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n20 0.271    0.725 0.307 0.234  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n21 0.271    0.725 0.308 0.234  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n22 0.271    0.726 0.309 0.235  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n23 0.27     0.724 0.306 0.234  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n24 0.271    0.725 0.307 0.234  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n25 0.27     0.724 0.305 0.233  25.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n26 0.27     0.724 0.305 0.233  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n27 0.269    0.722 0.303 0.233  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n28 0.27     0.723 0.304 0.233  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n29 0.272    0.727 0.309 0.235  25.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n30 0.269    0.721 0.302 0.232  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n31 0.272    0.727 0.31  0.235  25.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n32 0.269    0.72  0.302 0.232  25.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n33 0.276    0.735 0.323 0.24   25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n34 0.264    0.712 0.289 0.228  25.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n35 0.263    0.709 0.286 0.227  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n36 0.271    0.726 0.308 0.235  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n37 0.27     0.724 0.306 0.234  25.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n38 0.281    0.744 0.337 0.245  25.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n39 0.271    0.725 0.308 0.234  25.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n40 0.284    0.749 0.343 0.247  25.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n41 0.286    0.751 0.348 0.249  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n42 0.255    0.693 0.264 0.219  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n43 0.268    0.719 0.299 0.231  25.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n44 0.271    0.726 0.309 0.235  25.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n45 0.253    0.688 0.259 0.217  25.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n46 0.272    0.728 0.311 0.235  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n47 0.289    0.756 0.356 0.252  25.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n48 0.25     0.681 0.25  0.214  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n49 0.279    0.741 0.331 0.243  25.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n50 0.292    0.762 0.365 0.255  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n51 0.249    0.678 0.246 0.213  25.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n52 0.249    0.679 0.247 0.213  25.0     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n53 0.271    0.725 0.307 0.234  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n54 0.246    0.669 0.236 0.209  25.0     63.4           0 <srl_rcl_> <tibble [12 × 8]>\n55 0.272    0.727 0.31  0.235  25.0     63.4           0 <srl_rcl_> <tibble [12 × 8]>\n56 0.271    0.725 0.308 0.234  25.0     63.4           0 <srl_rcl_> <tibble [12 × 8]>\n57 0.271    0.726 0.308 0.234  25       63.4           0 <srl_rcl_> <tibble [12 × 8]>\n58 0.231    0.628 0.192 0.195  25.0     63.4           0 <srl_rcl_> <tibble [12 × 8]>\n59 0.271    0.725 0.308 0.234  25       63.4           0 <srl_rcl_> <tibble [12 × 8]>\n60 0.271    0.725 0.308 0.234  25       63.5           0 <srl_rcl_> <tibble [12 × 8]>\n61 0.224    0.606 0.17  0.188  25       63.5           0 <srl_rcl_> <tibble [12 × 8]>\n62 0.222    0.6   0.165 0.186  25       63.5           0 <srl_rcl_> <tibble [12 × 8]>\n63 0.221    0.596 0.161 0.185  25       63.5           0 <srl_rcl_> <tibble [12 × 8]>\n64 0.271    0.726 0.308 0.235  25       63.5           0 <srl_rcl_> <tibble [12 × 8]>\n65 0.204    0.529 0.105 0.167  25.0     63.6           0 <srl_rcl_> <tibble [12 × 8]>\n66 0.271    0.725 0.308 0.234  25.0     63.6           0 <srl_rcl_> <tibble [12 × 8]>\n67 0.203    0.526 0.102 0.167  25       63.6           0 <srl_rcl_> <tibble [12 × 8]>\n68 0.271    0.725 0.308 0.234  25       63.6           0 <srl_rcl_> <tibble [12 × 8]>\n69 0.2      0.514 0.094 0.164  25       63.6           0 <srl_rcl_> <tibble [12 × 8]>\n70 0.2      0.513 0.093 0.164  25.0     63.6           0 <srl_rcl_> <tibble [12 × 8]>\n71 0.195    0.487 0.074 0.159  25.0     63.7           0 <srl_rcl_> <tibble [12 × 8]>\n72 0.271    0.725 0.307 0.234  25       63.7           0 <srl_rcl_> <tibble [12 × 8]>\n73 0.191    0.468 0.061 0.155  25       63.7           0 <srl_rcl_> <tibble [12 × 8]>\n74 0.188    0.451 0.05  0.152  25.0     63.8           0 <srl_rcl_> <tibble [12 × 8]>\n75 0.271    0.726 0.309 0.236  25.0     63.8           0 <srl_rcl_> <tibble [12 × 8]>\n76 0.271    0.725 0.307 0.235  25.0     64.2           0 <srl_rcl_> <tibble [12 × 8]>\n77 0.271    0.726 0.309 0.234  25       64.3           0 <srl_rcl_> <tibble [12 × 8]>\n78 0.271    0.725 0.308 0.236  25.0     66.0           0 <srl_rcl_> <tibble [12 × 8]>\n79 0.974    0.994 0.003 0.008  25.0     69.2           0 <srl_rcl_> <tibble [12 × 8]>\n80 0.974    0.994 0.003 0.008  25.0     69.2           0 <srl_rcl_> <tibble [12 × 8]>\n81 0.348    0.221 0.522 0.315  25       84.2           0 <srl_rcl_> <tibble [12 × 8]>\n82 0.348    0.721 0.521 0.315  25       84.2           0 <srl_rcl_> <tibble [12 × 8]>\n83 0.348    0.195 0.521 0.315  25       84.2           0 <srl_rcl_> <tibble [12 × 8]>\n84 0.348    0.418 0.521 0.315  25.0     84.2           0 <srl_rcl_> <tibble [12 × 8]>\n85 0.348    0.204 0.522 0.315  25       84.2           0 <srl_rcl_> <tibble [12 × 8]>\n86 0.348    0.315 0.522 0.315  25.0     84.2           0 <srl_rcl_> <tibble [12 × 8]>\n87 0.348    0.237 0.521 0.315  25       84.3           0 <srl_rcl_> <tibble [12 × 8]>\n88 0.327    0.704 0.468 0.294  25       84.3           0 <srl_rcl_> <tibble [12 × 8]>\n89 0.348    0.112 0.522 0.316  25       84.3           0 <srl_rcl_> <tibble [12 × 8]>\n90 0.317    0.637 0.444 0.284  25       84.3           0 <srl_rcl_> <tibble [12 × 8]>\n91 0.348    0.303 0.521 0.315  25.0     84.3           0 <srl_rcl_> <tibble [12 × 8]>\n92 0.31     0.485 0.426 0.277  25       84.4           0 <srl_rcl_> <tibble [12 × 8]>\n93 0.303    0.612 0.409 0.271  25.0     84.4           0 <srl_rcl_> <tibble [12 × 8]>\n94 0.347    0.158 0.52  0.315  25.0     84.5           0 <srl_rcl_> <tibble [12 × 8]>\n95 0.348    0.58  0.52  0.314  25.0     84.6           0 <srl_rcl_> <tibble [12 × 8]>\n96 0.347    0.586 0.52  0.316  25.0     86.5           0 <srl_rcl_> <tibble [12 × 8]>\n97 0.346    0.505 0.517 0.315  25       88.7           0 <srl_rcl_> <tibble [12 × 8]>\n98 0.11     0.733 0.277 0.1    25      473.            0 <srl_rcl_> <tibble [12 × 8]>\n```\n\n\n:::\n:::\n\n\nI see three sets of parameters that are close in deviance (relatively):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits <- fit[c(1, 11, 13), ]\nfits\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 9\n   prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n  <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl> <list>     <list>           \n1 0.205    0.824 0.013 0.146  25.0     40.7           0 <srl_rcl_> <tibble [12 × 8]>\n2 0.186    0.479 0.025 0.146  25       56.4           0 <srl_rcl_> <tibble [12 × 8]>\n3 0.271    0.726 0.309 0.235  25       63.3           0 <srl_rcl_> <tibble [12 × 8]>\n```\n\n\n:::\n:::\n\n\nplots\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits |>\n  mutate(pred = map2(fit, data, \\(x, y) predict(x, y, group_by = c(\"chunk\", \"gap\")))) |>\n  unnest(c(data, pred)) |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = as.factor(round(prop, 3)))) +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_grid(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nthis case is particularly interesting. The bestfitting parameters produce almost no interaction. The other two sets of parameters produce a strong interaction, but misfit the overall data. \n\nFurther, the parameter set with rate 0.024 and 0.271 have quite similar fits despite very different parameter sets!\n\n### with prior on rate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- fits1 |>\n  filter(priors_scenario == \"rate\", exclude_sp1 == TRUE, exp == 2, convergence == 0) |>\n  mutate(deviance = map2_dbl(fit, exclude_sp1, function(x, y) {\n    overall_deviance(x$par, exp2_data_agg, exclude_sp1 = y)\n  })) |>\n  select(prop:convergence, fit, data) |>\n  arrange(deviance) |>\n  mutate_if(is.numeric, round, 3) |>\n  print(n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 99 × 9\n    prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n   <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl> <list>     <list>           \n 1 0.474    0.844 0.032 0.197  5.86     47.0           0 <srl_rcl_> <tibble [12 × 8]>\n 2 0.474    0.844 0.032 0.197  5.86     47.0           0 <srl_rcl_> <tibble [12 × 8]>\n 3 0.474    0.844 0.032 0.197  5.86     47.0           0 <srl_rcl_> <tibble [12 × 8]>\n 4 0.475    0.844 0.032 0.197  5.86     47.0           0 <srl_rcl_> <tibble [12 × 8]>\n 5 0.474    0.844 0.032 0.197  5.86     47.0           0 <srl_rcl_> <tibble [12 × 8]>\n 6 0.475    0.844 0.032 0.197  5.86     47.0           0 <srl_rcl_> <tibble [12 × 8]>\n 7 0.475    0.845 0.032 0.196  5.86     47.0           0 <srl_rcl_> <tibble [12 × 8]>\n 8 0.475    0.844 0.032 0.196  5.85     47.0           0 <srl_rcl_> <tibble [12 × 8]>\n 9 0.474    0.844 0.032 0.197  5.86     47.0           0 <srl_rcl_> <tibble [12 × 8]>\n10 0.475    0.844 0.032 0.197  5.86     47.0           0 <srl_rcl_> <tibble [12 × 8]>\n11 0.475    0.844 0.032 0.197  5.86     47.0           0 <srl_rcl_> <tibble [12 × 8]>\n12 0.475    0.844 0.032 0.196  5.86     47.0           0 <srl_rcl_> <tibble [12 × 8]>\n13 0.475    0.844 0.032 0.197  5.86     47.0           0 <srl_rcl_> <tibble [12 × 8]>\n14 0.475    0.844 0.032 0.197  5.86     47.0           0 <srl_rcl_> <tibble [12 × 8]>\n15 0.475    0.845 0.032 0.196  5.85     47.0           0 <srl_rcl_> <tibble [12 × 8]>\n16 0.475    0.844 0.032 0.196  5.85     47.0           0 <srl_rcl_> <tibble [12 × 8]>\n17 0.475    0.844 0.032 0.197  5.85     47.0           0 <srl_rcl_> <tibble [12 × 8]>\n18 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n19 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n20 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n21 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n22 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n23 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n24 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n25 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n26 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n27 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n28 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n29 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n30 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n31 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n32 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n33 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n34 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n35 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n36 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n37 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n38 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n39 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n40 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n41 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n42 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n43 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n44 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n45 0.271    0.482 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n46 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n47 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n48 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n49 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n50 0.271    0.481 0.1   0.202 13.3      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n51 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n52 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n53 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n54 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n55 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n56 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n57 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n58 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n59 0.271    0.482 0.1   0.202 13.3      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n60 0.272    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n61 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n62 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n63 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n64 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n65 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n66 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n67 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n68 0.27     0.481 0.1   0.202 13.3      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n69 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n70 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n71 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n72 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n73 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n74 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n75 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n76 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n77 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n78 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n79 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n80 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n81 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n82 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n83 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n84 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n85 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n86 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n87 0.271    0.481 0.1   0.202 13.2      63.3           0 <srl_rcl_> <tibble [12 × 8]>\n88 0.474    0.435 0.072 0.245  4.28     63.5           0 <srl_rcl_> <tibble [12 × 8]>\n89 0.474    0.435 0.072 0.245  4.28     63.5           0 <srl_rcl_> <tibble [12 × 8]>\n90 0.474    0.434 0.072 0.245  4.28     63.5           0 <srl_rcl_> <tibble [12 × 8]>\n91 0.474    0.434 0.072 0.245  4.28     63.5           0 <srl_rcl_> <tibble [12 × 8]>\n92 0.474    0.435 0.072 0.245  4.28     63.5           0 <srl_rcl_> <tibble [12 × 8]>\n93 0.474    0.435 0.072 0.245  4.28     63.5           0 <srl_rcl_> <tibble [12 × 8]>\n94 0.474    0.434 0.072 0.245  4.28     63.5           0 <srl_rcl_> <tibble [12 × 8]>\n95 0.474    0.435 0.072 0.245  4.28     63.5           0 <srl_rcl_> <tibble [12 × 8]>\n96 0.474    0.434 0.072 0.245  4.28     63.5           0 <srl_rcl_> <tibble [12 × 8]>\n97 0.474    0.435 0.072 0.245  4.28     63.5           0 <srl_rcl_> <tibble [12 × 8]>\n98 0.474    0.434 0.072 0.245  4.28     63.5           0 <srl_rcl_> <tibble [12 × 8]>\n99 0.509    0.398 0.102 0.258  3.72     68.2           0 <srl_rcl_> <tibble [12 × 8]>\n```\n\n\n:::\n:::\n\n\nplots\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits <- fit[c(28, 43), ] # previous 83\nfits\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 9\n   prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n  <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl> <list>     <list>           \n1 0.271    0.481   0.1 0.202  13.2     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n2 0.271    0.481   0.1 0.202  13.2     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n```\n\n\n:::\n\n```{.r .cell-code}\nfits |>\n  mutate(pred = map2(fit, data, \\(x, y) predict(x, y, group_by = c(\"chunk\", \"gap\")))) |>\n  unnest(c(data, pred)) |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = as.factor(round(prop, 3)))) +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_grid(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n### Including the first chunk in the optimization\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- paper_params(exp = 2)\n(est <- estimate_model(start, data = exp2_data_agg, exclude_sp1 = FALSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$start\n    prop prop_ltm      tau     gain     rate \n   0.170    0.400    0.135   25.000    0.025 \n\n$par\n      prop   prop_ltm        tau       gain       rate \n 0.1618193  0.8723027  0.1235086 43.4849440  0.0080991 \n\n$convergence\n[1] 0\n\n$counts\nfunction gradient \n     412       NA \n\n$value\n[1] 116.1887\n```\n\n\n:::\n\n```{.r .cell-code}\nexp2_data_agg$pred <- predict(est, exp2_data_agg, group_by = c(\"chunk\", \"gap\"))\nexp2_data_agg |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nfrom multiple starting values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits <- fits1 |>\n  filter(priors_scenario == \"none\", exclude_sp1 == FALSE, exp == 2, convergence == 0) |>\n  select(prop:convergence, fit, data) |>\n  arrange(deviance) |>\n  mutate_if(is.numeric, round, 3)\nhead(fits)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 9\n   prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n  <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl> <list>     <list>           \n1 0.38     0.862  0.02 0.189  8.98     110.           0 <srl_rcl_> <tibble [12 × 8]>\n2 0.38     0.862  0.02 0.189  8.98     110.           0 <srl_rcl_> <tibble [12 × 8]>\n3 0.38     0.862  0.02 0.189  8.98     110.           0 <srl_rcl_> <tibble [12 × 8]>\n4 0.379    0.862  0.02 0.189  8.98     110.           0 <srl_rcl_> <tibble [12 × 8]>\n5 0.38     0.862  0.02 0.189  8.98     110.           0 <srl_rcl_> <tibble [12 × 8]>\n6 0.38     0.862  0.02 0.189  8.98     110.           0 <srl_rcl_> <tibble [12 × 8]>\n```\n\n\n:::\n:::\n\n\n### With prior on rate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- fits1 |>\n  filter(priors_scenario == \"rate\", exclude_sp1 == FALSE, exp == 2, convergence == 0) |>\n  mutate(deviance = map2_dbl(fit, exclude_sp1, function(x, y) {\n    overall_deviance(x$par, exp2_data_agg, exclude_sp1 = y)\n  })) |>\n  select(prop:convergence, fit, data) |>\n  arrange(deviance) |>\n  mutate_if(is.numeric, round, 3)\nhead(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 9\n   prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n  <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl> <list>     <list>           \n1 0.501    0.857 0.032 0.192  5.56     114.           0 <srl_rcl_> <tibble [12 × 8]>\n2 0.501    0.857 0.032 0.192  5.55     114.           0 <srl_rcl_> <tibble [12 × 8]>\n3 0.502    0.857 0.032 0.192  5.55     114.           0 <srl_rcl_> <tibble [12 × 8]>\n4 0.501    0.858 0.032 0.192  5.56     114.           0 <srl_rcl_> <tibble [12 × 8]>\n5 0.501    0.857 0.032 0.192  5.55     114.           0 <srl_rcl_> <tibble [12 × 8]>\n6 0.501    0.858 0.032 0.192  5.55     114.           0 <srl_rcl_> <tibble [12 × 8]>\n```\n\n\n:::\n:::\n\n\nplot predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit |>\n  slice(1) |>\n  mutate(pred = map2(fit, data, \\(x, y) predict(x, y, group_by = c(\"chunk\", \"gap\")))) |>\n  unnest(c(data, pred)) |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = as.factor(round(prop, 3)))) +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_grid(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n## Summary\n\n- The parameters reported in the paper are not the best fitting\n- When I start from 100 different starting values, I get better fitting parameters, but with an even lower `rate`\n- I can reproduce the parameters from the paper if I fix the gain parameter to 25\n\n### Best fitting parameters\n\nGiven the different modeling choices (ignoring the first chunk or not, priors on the parameters)\n\nTODO: make this into a function for getting the final parameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal <- fits1 |>\n  filter(convergence == 0) |>\n  group_by(exp, priors_scenario, exclude_sp1) |>\n  arrange(deviance) |>\n  slice(1) |>\n  arrange(desc(exclude_sp1), exp, priors_scenario) |>\n  mutate(\n    deviance = round(deviance, 1),\n    priors_scenario = case_when(\n      priors_scenario == \"none\" ~ \"None\",\n      priors_scenario == \"gain\" ~ \"Gain ~ N(25, 0.1)\",\n      priors_scenario == \"rate\" ~ \"Rate ~ N(0.1, 0.01)\"\n    )\n  )\n\nfinal |>\n  select(exp, priors_scenario, exclude_sp1, prop:gain, deviance) |>\n  mutate_all(round, 3) |>\n  kbl() |>\n  kable_styling()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> exp </th>\n   <th style=\"text-align:left;\"> priors_scenario </th>\n   <th style=\"text-align:left;\"> exclude_sp1 </th>\n   <th style=\"text-align:right;\"> prop </th>\n   <th style=\"text-align:right;\"> prop_ltm </th>\n   <th style=\"text-align:right;\"> rate </th>\n   <th style=\"text-align:right;\"> tau </th>\n   <th style=\"text-align:right;\"> gain </th>\n   <th style=\"text-align:right;\"> deviance </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:left;\"> Gain ~ N(25, 0.1) </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n   <td style=\"text-align:right;\"> 0.222 </td>\n   <td style=\"text-align:right;\"> 0.588 </td>\n   <td style=\"text-align:right;\"> 0.019 </td>\n   <td style=\"text-align:right;\"> 0.154 </td>\n   <td style=\"text-align:right;\"> 25.002 </td>\n   <td style=\"text-align:right;\"> 33.0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:left;\"> None </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n   <td style=\"text-align:right;\"> 0.106 </td>\n   <td style=\"text-align:right;\"> 0.575 </td>\n   <td style=\"text-align:right;\"> 0.009 </td>\n   <td style=\"text-align:right;\"> 0.090 </td>\n   <td style=\"text-align:right;\"> 100.000 </td>\n   <td style=\"text-align:right;\"> 31.9 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:left;\"> Rate ~ N(0.1, 0.01) </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n   <td style=\"text-align:right;\"> 0.435 </td>\n   <td style=\"text-align:right;\"> 0.608 </td>\n   <td style=\"text-align:right;\"> 0.049 </td>\n   <td style=\"text-align:right;\"> 0.202 </td>\n   <td style=\"text-align:right;\"> 7.590 </td>\n   <td style=\"text-align:right;\"> 43.1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2 </td>\n   <td style=\"text-align:left;\"> Gain ~ N(25, 0.1) </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n   <td style=\"text-align:right;\"> 0.205 </td>\n   <td style=\"text-align:right;\"> 0.824 </td>\n   <td style=\"text-align:right;\"> 0.013 </td>\n   <td style=\"text-align:right;\"> 0.146 </td>\n   <td style=\"text-align:right;\"> 25.001 </td>\n   <td style=\"text-align:right;\"> 40.7 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2 </td>\n   <td style=\"text-align:left;\"> None </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n   <td style=\"text-align:right;\"> 0.105 </td>\n   <td style=\"text-align:right;\"> 0.815 </td>\n   <td style=\"text-align:right;\"> 0.007 </td>\n   <td style=\"text-align:right;\"> 0.089 </td>\n   <td style=\"text-align:right;\"> 87.787 </td>\n   <td style=\"text-align:right;\"> 40.5 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2 </td>\n   <td style=\"text-align:left;\"> Rate ~ N(0.1, 0.01) </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n   <td style=\"text-align:right;\"> 0.474 </td>\n   <td style=\"text-align:right;\"> 0.844 </td>\n   <td style=\"text-align:right;\"> 0.032 </td>\n   <td style=\"text-align:right;\"> 0.197 </td>\n   <td style=\"text-align:right;\"> 5.865 </td>\n   <td style=\"text-align:right;\"> 47.0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:left;\"> Gain ~ N(25, 0.1) </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 0.231 </td>\n   <td style=\"text-align:right;\"> 0.639 </td>\n   <td style=\"text-align:right;\"> 0.016 </td>\n   <td style=\"text-align:right;\"> 0.156 </td>\n   <td style=\"text-align:right;\"> 24.999 </td>\n   <td style=\"text-align:right;\"> 145.5 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:left;\"> None </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 0.303 </td>\n   <td style=\"text-align:right;\"> 0.636 </td>\n   <td style=\"text-align:right;\"> 0.021 </td>\n   <td style=\"text-align:right;\"> 0.177 </td>\n   <td style=\"text-align:right;\"> 15.206 </td>\n   <td style=\"text-align:right;\"> 144.9 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:left;\"> Rate ~ N(0.1, 0.01) </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 0.475 </td>\n   <td style=\"text-align:right;\"> 0.645 </td>\n   <td style=\"text-align:right;\"> 0.049 </td>\n   <td style=\"text-align:right;\"> 0.197 </td>\n   <td style=\"text-align:right;\"> 6.907 </td>\n   <td style=\"text-align:right;\"> 152.8 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2 </td>\n   <td style=\"text-align:left;\"> Gain ~ N(25, 0.1) </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 0.217 </td>\n   <td style=\"text-align:right;\"> 0.874 </td>\n   <td style=\"text-align:right;\"> 0.011 </td>\n   <td style=\"text-align:right;\"> 0.150 </td>\n   <td style=\"text-align:right;\"> 24.997 </td>\n   <td style=\"text-align:right;\"> 113.5 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2 </td>\n   <td style=\"text-align:left;\"> None </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 0.380 </td>\n   <td style=\"text-align:right;\"> 0.862 </td>\n   <td style=\"text-align:right;\"> 0.020 </td>\n   <td style=\"text-align:right;\"> 0.189 </td>\n   <td style=\"text-align:right;\"> 8.983 </td>\n   <td style=\"text-align:right;\"> 109.7 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2 </td>\n   <td style=\"text-align:left;\"> Rate ~ N(0.1, 0.01) </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 0.501 </td>\n   <td style=\"text-align:right;\"> 0.857 </td>\n   <td style=\"text-align:right;\"> 0.032 </td>\n   <td style=\"text-align:right;\"> 0.192 </td>\n   <td style=\"text-align:right;\"> 5.557 </td>\n   <td style=\"text-align:right;\"> 113.8 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n- prop_ltm very different between the two experiments (overfitting...)\n\n### Predictions\n\n(the two experiments are modeled separately)\n\n### All predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal |>\n  select(exp, rate, data, pred) |>\n  mutate(exp = paste0(\"Exp \", exp)) |>\n  unnest(c(data, pred)) |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = as.factor(round(rate, 3)))) +\n  scale_color_discrete(\"1st chunk LTM?\") +\n  scale_linetype_discrete(\"Rate\") +\n  facet_grid(exp ~ itemtype) +\n  theme_pub()\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-28-1.png){width=816}\n:::\n:::\n\n\n### Experiment 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal |>\n  filter(exp == 1) |>\n  arrange(rate) |>\n  select(rate, data, pred) |>\n  unnest(c(data, pred)) |>\n  mutate(rate = as.character(round(rate, 4))) |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = )) +\n  scale_color_discrete(\"1st chunk LTM?\") +\n  scale_linetype_discrete(\"Rate\") +\n  facet_grid(rate ~ itemtype) +\n  theme_pub()\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-29-1.png){width=816}\n:::\n:::\n\n\n### Experiment 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal |>\n  filter(exp == 2) |>\n  arrange(rate) |>\n  select(rate, data, pred) |>\n  unnest(c(data, pred)) |>\n  mutate(rate = as.character(round(rate, 4))) |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = )) +\n  scale_color_discrete(\"1st chunk LTM?\") +\n  scale_linetype_discrete(\"Rate\") +\n  facet_grid(rate ~ itemtype) +\n  theme_pub()\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-30-1.png){width=816}\n:::\n:::\n",
    "supporting": [
      "modelling_edas_approach_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}