{
  "hash": "cb2640d4e221bef524baa2c0927d935a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Main results\"\nformat: html\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(targets)\nlibrary(GGally)\nlibrary(kableExtra)\n# load \"R/*\" scripts and saved R objects from the targets pi\ntar_source()\ntar_load(c(exp1_data, exp2_data, exp1_data_agg, exp2_data_agg, fits1))\n```\n:::\n\n\n## Overview\n\nHere I apply the model described in the May 13th draft of the paper to the data. I will first ignore the first chunk in the optimization, then include it. I will also try different priors on the parameters to understand the paramater space. Final results from different choices summarized at the end.\n\n## Ignoring first chunk in the optimiziation\n\n### Basic estimation of Exp1\n\nLet's apply the modeling approach reported in the paper. We ignore the first chunk (SP1-3) while evaluating the likelihood. Eda did this because the model as implemented predicts the same performance for known and random chunks.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntar_load(exp1_data_agg)\nstart <- paper_params()\n(est <- estimate_model(start, data = exp1_data_agg, exclude_sp1 = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$start\n    prop prop_ltm      tau     gain     rate \n    0.21     0.55     0.14    25.00     0.02 \n\n$par\n        prop     prop_ltm          tau         gain         rate \n 0.108898668  0.575632670  0.091783835 95.188660503  0.009362919 \n\n$convergence\n[1] 0\n\n$counts\nfunction gradient \n    2080       NA \n\n$value\n[1] 31.92645\n```\n\n\n:::\n\n```{.r .cell-code}\nexp1_data_agg$pred <- predict(est, exp1_data_agg, group_by = c(\"chunk\", \"gap\"))\nexp1_data_agg |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/model 1-1.png){width=672}\n:::\n:::\n\n\nI get startlingly different paramemter estimates. Much lower `prop` and `rate` and `tau`, higher `gain`.\n\n### Trying different starting values\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the fits of the first simulation, calculate the deviance(s) and predictions\ntar_load(fits1)\nfits1 <- fits1 |>\n  mutate(\n    deviance = pmap_dbl(\n      list(fit, data, exclude_sp1),\n      ~ overall_deviance(params = `..1`$par, data = `..2`, exclude_sp1 = `..3`)\n    ),\n    pred = map2(fit, data, ~ predict(.x, .y, group_by = c(\"chunk\", \"gap\")))\n  )\n```\n:::\n\n\nI've run this with many different starting values. We tend to end up in different regions of the parameter space (the top result close to the paper's estimates):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits1 |>\n  filter(priors_scenario == \"\", exclude_sp1 == TRUE, exp == 1, deviance <= 50, convergence == 0) |>\n  select(prop:convergence) |>\n  arrange(gain) |>\n  mutate_all(round, 3) |>\n  print(n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 51 × 7\n    prop prop_ltm  rate   tau  gain deviance convergence\n   <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl>\n 1 0.207    0.586 0.017 0.147  28.4     32.8           0\n 2 0.198    0.585 0.017 0.143  30.9     32.6           0\n 3 0.184    0.583 0.016 0.136  35.3     32.5           0\n 4 0.174    0.582 0.015 0.131  39.2     32.4           0\n 5 0.17     0.582 0.014 0.129  41.1     32.3           0\n 6 0.141    0.579 0.012 0.113  58.2     32.1           0\n 7 0.126    0.578 0.011 0.103  71.6     32.0           0\n 8 0.124    0.578 0.011 0.102  74.4     32.0           0\n 9 0.106    0.574 0.009 0.09   99.8     31.9           0\n10 0.106    0.577 0.009 0.09   99.8     31.9           0\n11 0.106    0.576 0.009 0.09   99.9     31.9           0\n12 0.101    0.4   0.015 0.088  99.9     46.5           0\n13 0.106    0.575 0.009 0.09   99.9     31.9           0\n14 0.106    0.576 0.009 0.09   99.9     31.9           0\n15 0.106    0.575 0.009 0.09   99.9     31.9           0\n16 0.106    0.577 0.009 0.09   99.9     31.9           0\n17 0.106    0.578 0.009 0.09  100.      31.9           0\n18 0.106    0.578 0.009 0.09  100.      31.9           0\n19 0.106    0.575 0.009 0.09  100.      31.9           0\n20 0.106    0.576 0.009 0.09  100.      31.9           0\n21 0.106    0.576 0.009 0.09  100.      31.9           0\n22 0.106    0.574 0.009 0.09  100.      31.9           0\n23 0.106    0.576 0.009 0.09  100.      31.9           0\n24 0.106    0.575 0.009 0.09  100.      31.9           0\n25 0.106    0.575 0.009 0.09  100.      31.9           0\n26 0.106    0.575 0.009 0.09  100.      31.9           0\n27 0.106    0.575 0.009 0.09  100.      31.9           0\n28 0.106    0.572 0.009 0.09  100.      31.9           0\n29 0.106    0.576 0.009 0.09  100.      31.9           0\n30 0.106    0.575 0.009 0.09  100.      31.9           0\n31 0.106    0.574 0.009 0.09  100.      31.9           0\n32 0.106    0.576 0.009 0.09  100.      31.9           0\n33 0.106    0.58  0.009 0.09  100.      31.9           0\n34 0.106    0.575 0.009 0.09  100.      31.9           0\n35 0.106    0.575 0.009 0.09  100.      31.9           0\n36 0.106    0.575 0.009 0.09  100.      31.9           0\n37 0.106    0.575 0.009 0.09  100.      31.9           0\n38 0.106    0.576 0.009 0.09  100.      31.9           0\n39 0.106    0.574 0.009 0.09  100.      31.9           0\n40 0.106    0.575 0.009 0.09  100.      31.9           0\n41 0.106    0.575 0.009 0.09  100.      31.9           0\n42 0.106    0.576 0.009 0.09  100.      31.9           0\n43 0.106    0.576 0.009 0.09  100.      31.9           0\n44 0.106    0.575 0.009 0.09  100.      31.9           0\n45 0.106    0.575 0.009 0.09  100.      31.9           0\n46 0.106    0.575 0.009 0.09  100       31.9           0\n47 0.106    0.575 0.009 0.09  100       31.9           0\n48 0.106    0.575 0.009 0.09  100       31.9           0\n49 0.106    0.575 0.009 0.09  100       31.9           0\n50 0.106    0.575 0.009 0.09  100       31.9           0\n51 0.106    0.576 0.009 0.09  100       31.9           0\n```\n\n\n:::\n:::\n\n\n### Trying priors of the gain parameter\n\nOne way to deal with that is to put a prior on the gain parameter to keep it near 25. I know priors are usually a bayesian thing, but they work with ML optimization just as well. On the next set of simulations, I used a Normal(25, 0.1) prior on the gain parameter (could have also fixed it to this value, but this gives me mroe control).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits1 |>\n  filter(priors_scenario == \"gain\", exclude_sp1 == TRUE, exp == 1, deviance <= 50, convergence == 0) |>\n  select(prop:convergence) |>\n  arrange(gain) |>\n  mutate_all(round, 3) |>\n  print(n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 17 × 7\n    prop prop_ltm  rate   tau  gain deviance convergence\n   <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl>\n 1 0.208    0.398 0.031 0.154  25.0     47.7           0\n 2 0.222    0.588 0.019 0.154  25.0     33.0           0\n 3 0.222    0.588 0.019 0.154  25       33.0           0\n 4 0.222    0.587 0.019 0.154  25       33.0           0\n 5 0.222    0.587 0.019 0.154  25       33.0           0\n 6 0.222    0.588 0.019 0.154  25       33.0           0\n 7 0.222    0.588 0.019 0.154  25.0     33.0           0\n 8 0.222    0.588 0.019 0.154  25.0     33.0           0\n 9 0.222    0.588 0.019 0.154  25.0     33.0           0\n10 0.222    0.588 0.019 0.154  25.0     33.0           0\n11 0.222    0.588 0.019 0.154  25.0     33.0           0\n12 0.222    0.588 0.019 0.154  25.0     33.0           0\n13 0.222    0.588 0.019 0.154  25.0     33.0           0\n14 0.222    0.588 0.019 0.154  25.0     33.0           0\n15 0.222    0.588 0.019 0.154  25.0     33.0           0\n16 0.222    0.588 0.019 0.154  25.0     33.0           0\n17 0.222    0.588 0.019 0.154  25.0     33.0           0\n```\n\n\n:::\n:::\n\n\nSo we do get at least some parameters that are close to that reported in the paper. The predictions with those parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp1_data_agg$pred <- fits1 |>\n  filter(priors_scenario == \"gain\", exclude_sp1 == TRUE, exp == 1, convergence == 0) |>\n  arrange(deviance) |>\n  pluck(\"pred\", 1)\n\nexp1_data_agg |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n### Trying priors on the rate parameter\n\nMaybe there is a region with a higher rate that we have not explored? Let's try a prior on the rate parameter, ~ Normal(0.1, 0.01).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits1 |>\n  filter(priors_scenario == \"rate\", exclude_sp1 == TRUE, exp == 1, convergence == 0) |>\n  select(prop:convergence) |>\n  arrange(deviance) |>\n  mutate_all(round, 3) |>\n  print(n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 100 × 7\n     prop prop_ltm  rate   tau  gain deviance convergence\n    <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl>\n  1 0.434    0.607 0.049 0.202  7.59     43.1           0\n  2 0.435    0.608 0.049 0.202  7.59     43.1           0\n  3 0.435    0.607 0.049 0.202  7.59     43.1           0\n  4 0.435    0.608 0.049 0.202  7.59     43.1           0\n  5 0.434    0.607 0.049 0.202  7.60     43.1           0\n  6 0.435    0.607 0.049 0.202  7.59     43.1           0\n  7 0.435    0.608 0.049 0.202  7.59     43.1           0\n  8 0.434    0.607 0.049 0.202  7.59     43.1           0\n  9 0.435    0.607 0.049 0.202  7.59     43.1           0\n 10 0.435    0.607 0.049 0.202  7.59     43.1           0\n 11 0.435    0.607 0.049 0.202  7.59     43.1           0\n 12 0.435    0.608 0.049 0.202  7.59     43.1           0\n 13 0.434    0.607 0.049 0.202  7.59     43.1           0\n 14 0.435    0.607 0.049 0.202  7.58     43.1           0\n 15 0.435    0.607 0.049 0.202  7.59     43.1           0\n 16 0.435    0.607 0.049 0.202  7.59     43.1           0\n 17 0.435    0.607 0.049 0.202  7.59     43.1           0\n 18 0.435    0.607 0.049 0.202  7.59     43.1           0\n 19 0.435    0.607 0.049 0.202  7.59     43.1           0\n 20 0.435    0.607 0.049 0.202  7.58     43.1           0\n 21 0.435    0.607 0.049 0.202  7.58     43.1           0\n 22 0.435    0.607 0.049 0.202  7.58     43.1           0\n 23 0.435    0.607 0.049 0.202  7.58     43.1           0\n 24 0.435    0.607 0.049 0.202  7.58     43.1           0\n 25 0.435    0.607 0.049 0.202  7.58     43.1           0\n 26 0.435    0.607 0.049 0.202  7.58     43.1           0\n 27 0.435    0.607 0.049 0.202  7.58     43.1           0\n 28 0.435    0.608 0.049 0.202  7.58     43.1           0\n 29 0.435    0.607 0.049 0.202  7.58     43.1           0\n 30 0.435    0.607 0.049 0.202  7.59     43.1           0\n 31 0.435    0.608 0.049 0.202  7.58     43.1           0\n 32 0.435    0.607 0.049 0.202  7.58     43.1           0\n 33 0.435    0.607 0.049 0.202  7.58     43.1           0\n 34 0.435    0.607 0.049 0.202  7.59     43.1           0\n 35 0.435    0.607 0.049 0.202  7.58     43.1           0\n 36 0.435    0.607 0.049 0.202  7.58     43.1           0\n 37 0.435    0.607 0.049 0.202  7.58     43.1           0\n 38 0.435    0.607 0.049 0.202  7.59     43.1           0\n 39 0.435    0.607 0.049 0.202  7.58     43.1           0\n 40 0.435    0.607 0.049 0.202  7.59     43.1           0\n 41 0.435    0.607 0.049 0.202  7.59     43.1           0\n 42 0.435    0.607 0.049 0.202  7.58     43.1           0\n 43 0.435    0.607 0.049 0.202  7.58     43.1           0\n 44 0.435    0.607 0.049 0.202  7.58     43.1           0\n 45 0.435    0.607 0.049 0.202  7.58     43.1           0\n 46 0.435    0.607 0.049 0.202  7.58     43.1           0\n 47 0.435    0.607 0.049 0.202  7.59     43.1           0\n 48 0.435    0.607 0.049 0.202  7.58     43.1           0\n 49 0.435    0.607 0.049 0.202  7.58     43.1           0\n 50 0.435    0.607 0.049 0.202  7.58     43.1           0\n 51 0.435    0.607 0.049 0.202  7.58     43.1           0\n 52 0.435    0.607 0.049 0.202  7.58     43.1           0\n 53 0.435    0.607 0.049 0.202  7.59     43.1           0\n 54 0.435    0.607 0.049 0.202  7.58     43.1           0\n 55 0.435    0.607 0.049 0.202  7.58     43.1           0\n 56 0.435    0.607 0.049 0.202  7.58     43.1           0\n 57 0.435    0.607 0.049 0.202  7.58     43.1           0\n 58 0.435    0.607 0.049 0.202  7.58     43.1           0\n 59 0.435    0.607 0.049 0.202  7.58     43.1           0\n 60 0.435    0.608 0.049 0.202  7.58     43.1           0\n 61 0.435    0.607 0.049 0.202  7.58     43.1           0\n 62 0.435    0.607 0.049 0.202  7.58     43.1           0\n 63 0.435    0.607 0.049 0.202  7.58     43.1           0\n 64 0.435    0.607 0.049 0.202  7.58     43.1           0\n 65 0.435    0.607 0.049 0.202  7.58     43.1           0\n 66 0.435    0.607 0.049 0.202  7.58     43.1           0\n 67 0.435    0.607 0.049 0.202  7.58     43.1           0\n 68 0.435    0.607 0.049 0.202  7.58     43.1           0\n 69 0.435    0.607 0.049 0.202  7.58     43.2           0\n 70 0.435    0.607 0.049 0.202  7.58     43.2           0\n 71 0.435    0.608 0.049 0.202  7.58     43.2           0\n 72 0.435    0.607 0.049 0.202  7.58     43.2           0\n 73 0.435    0.607 0.049 0.202  7.58     43.2           0\n 74 0.435    0.607 0.049 0.202  7.58     43.2           0\n 75 0.435    0.607 0.049 0.202  7.58     43.2           0\n 76 0.435    0.608 0.049 0.202  7.58     43.2           0\n 77 0.435    0.607 0.049 0.202  7.58     43.2           0\n 78 0.435    0.607 0.049 0.202  7.58     43.2           0\n 79 0.39     0.375 0.071 0.218  7.89     55.8           0\n 80 0.39     0.375 0.071 0.218  7.88     55.8           0\n 81 0.39     0.375 0.071 0.218  7.88     55.8           0\n 82 0.39     0.375 0.071 0.218  7.87     55.8           0\n 83 0.39     0.375 0.071 0.218  7.88     55.8           0\n 84 0.39     0.375 0.071 0.218  7.88     55.8           0\n 85 0.39     0.375 0.071 0.218  7.87     55.8           0\n 86 0.39     0.375 0.071 0.218  7.88     55.8           0\n 87 0.39     0.375 0.071 0.218  7.87     55.8           0\n 88 0.39     0.375 0.071 0.218  7.87     55.8           0\n 89 0.39     0.375 0.071 0.218  7.88     55.8           0\n 90 0.39     0.375 0.071 0.218  7.87     55.8           0\n 91 0.39     0.375 0.071 0.218  7.87     55.8           0\n 92 0.39     0.375 0.071 0.218  7.87     55.8           0\n 93 0.39     0.375 0.071 0.218  7.87     55.8           0\n 94 0.39     0.375 0.071 0.218  7.87     55.8           0\n 95 0.39     0.375 0.071 0.218  7.87     55.8           0\n 96 0.39     0.375 0.071 0.219  7.88     55.8           0\n 97 0.391    0.375 0.071 0.218  7.86     55.8           0\n 98 0.391    0.375 0.071 0.218  7.87     55.8           0\n 99 0.391    0.375 0.071 0.219  7.86     55.8           0\n100 0.546    0.281 0.151 0.254  4.08     92.1           0\n```\n\n\n:::\n:::\n\n\nDeviance is quite much higher. Predictions?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- fits1 |>\n  filter(priors_scenario == \"rate\", exclude_sp1 == TRUE, exp == 1, convergence == 0) |>\n  arrange(deviance) |>\n  pluck(\"fit\", 1)\n\nfit$par\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     prop  prop_ltm      rate       tau      gain \n0.4344079 0.6073675 0.0490612 0.2015930 7.5932354 \n```\n\n\n:::\n\n```{.r .cell-code}\nexp1_data_agg$pred <- predict(fit, exp1_data_agg, group_by = c(\"chunk\", \"gap\"))\n\nexp1_data_agg |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nGeater mismatch. Let's include error bars of the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp1_data |>\n  group_by(id, chunk, gap, itemtype) |>\n  summarise(\n    n_total = dplyr::n(),\n    n_correct = sum(cor),\n    p_correct = mean(cor)\n  ) |>\n  ungroup() |>\n  left_join(\n    select(exp1_data_agg, chunk, gap, itemtype, pred),\n    by = c(\"chunk\", \"gap\", \"itemtype\")\n  ) |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  stat_summary() +\n  stat_summary(aes(y = pred), linetype = \"dashed\", geom = \"line\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'id', 'chunk', 'gap'. You can override using the `.groups` argument.\nNo summary function supplied, defaulting to `mean_se()`\nNo summary function supplied, defaulting to `mean_se()`\nNo summary function supplied, defaulting to `mean_se()`\nNo summary function supplied, defaulting to `mean_se()`\nNo summary function supplied, defaulting to `mean_se()`\nNo summary function supplied, defaulting to `mean_se()`\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nParaneters seem consistent with the data (see [my notes](../notes.qmd)).\n\n## Including the first chunk in the optimization\n\nThe reports above followed the approach in the current draft and excluded the first chunk from the calculation of the likelihood when optimizing the parameters. Let's include it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- paper_params()\n(est <- estimate_model(start, data = exp1_data_agg, exclude_sp1 = FALSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$start\n    prop prop_ltm      tau     gain     rate \n    0.21     0.55     0.14    25.00     0.02 \n\n$par\n       prop    prop_ltm         tau        gain        rate \n 0.30248729  0.63594776  0.17698525 15.21043755  0.02143605 \n\n$convergence\n[1] 0\n\n$counts\nfunction gradient \n    1276       NA \n\n$value\n[1] 144.8785\n```\n\n\n:::\n\n```{.r .cell-code}\nexp1_data_agg$pred <- predict(est, exp1_data_agg, group_by = c(\"chunk\", \"gap\"))\nexp1_data_agg |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nIn this case I didn't have to use many starting values - the result is reached from almost everywhere:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits1 |>\n  filter(priors_scenario == \"\", exclude_sp1 == FALSE, exp == 1, convergence == 0) |>\n  arrange(deviance) |>\n  select(prop:convergence) |>\n  print(n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 99 × 7\n    prop prop_ltm   rate   tau  gain deviance convergence\n   <dbl>    <dbl>  <dbl> <dbl> <dbl>    <dbl>       <int>\n 1 0.302    0.636 0.0214 0.177 15.2      145.           0\n 2 0.302    0.636 0.0214 0.177 15.2      145.           0\n 3 0.302    0.636 0.0214 0.177 15.2      145.           0\n 4 0.303    0.636 0.0214 0.177 15.2      145.           0\n 5 0.302    0.636 0.0214 0.177 15.2      145.           0\n 6 0.303    0.636 0.0214 0.177 15.2      145.           0\n 7 0.302    0.636 0.0214 0.177 15.2      145.           0\n 8 0.303    0.636 0.0214 0.177 15.2      145.           0\n 9 0.302    0.636 0.0214 0.177 15.2      145.           0\n10 0.303    0.636 0.0214 0.177 15.2      145.           0\n11 0.303    0.636 0.0214 0.177 15.2      145.           0\n12 0.303    0.636 0.0214 0.177 15.2      145.           0\n13 0.302    0.636 0.0214 0.177 15.2      145.           0\n14 0.302    0.636 0.0214 0.177 15.2      145.           0\n15 0.303    0.636 0.0215 0.177 15.2      145.           0\n16 0.303    0.636 0.0214 0.177 15.2      145.           0\n17 0.303    0.636 0.0214 0.177 15.2      145.           0\n18 0.302    0.636 0.0214 0.177 15.2      145.           0\n19 0.303    0.636 0.0214 0.177 15.2      145.           0\n20 0.302    0.636 0.0214 0.177 15.2      145.           0\n21 0.303    0.636 0.0214 0.177 15.2      145.           0\n22 0.302    0.636 0.0214 0.177 15.2      145.           0\n23 0.302    0.636 0.0214 0.177 15.2      145.           0\n24 0.303    0.636 0.0214 0.177 15.2      145.           0\n25 0.302    0.636 0.0214 0.177 15.2      145.           0\n26 0.302    0.636 0.0214 0.177 15.2      145.           0\n27 0.302    0.636 0.0214 0.177 15.2      145.           0\n28 0.302    0.636 0.0214 0.177 15.2      145.           0\n29 0.302    0.636 0.0214 0.177 15.2      145.           0\n30 0.302    0.636 0.0214 0.177 15.2      145.           0\n31 0.303    0.636 0.0214 0.177 15.2      145.           0\n32 0.303    0.636 0.0214 0.177 15.2      145.           0\n33 0.302    0.636 0.0214 0.177 15.2      145.           0\n34 0.302    0.636 0.0214 0.177 15.2      145.           0\n35 0.303    0.636 0.0214 0.177 15.2      145.           0\n36 0.303    0.636 0.0214 0.177 15.2      145.           0\n37 0.302    0.636 0.0214 0.177 15.2      145.           0\n38 0.302    0.636 0.0214 0.177 15.2      145.           0\n39 0.302    0.636 0.0214 0.177 15.2      145.           0\n40 0.302    0.636 0.0214 0.177 15.2      145.           0\n41 0.302    0.636 0.0214 0.177 15.2      145.           0\n42 0.302    0.636 0.0214 0.177 15.2      145.           0\n43 0.302    0.636 0.0214 0.177 15.2      145.           0\n44 0.303    0.636 0.0215 0.177 15.2      145.           0\n45 0.302    0.636 0.0214 0.177 15.3      145.           0\n46 0.302    0.636 0.0214 0.177 15.2      145.           0\n47 0.302    0.636 0.0214 0.177 15.2      145.           0\n48 0.302    0.636 0.0214 0.177 15.2      145.           0\n49 0.302    0.636 0.0214 0.177 15.3      145.           0\n50 0.302    0.636 0.0214 0.177 15.2      145.           0\n51 0.303    0.636 0.0215 0.177 15.2      145.           0\n52 0.303    0.636 0.0215 0.177 15.2      145.           0\n53 0.302    0.636 0.0215 0.177 15.2      145.           0\n54 0.303    0.636 0.0214 0.177 15.2      145.           0\n55 0.303    0.636 0.0214 0.177 15.2      145.           0\n56 0.303    0.636 0.0214 0.177 15.2      145.           0\n57 0.303    0.636 0.0214 0.177 15.2      145.           0\n58 0.302    0.636 0.0214 0.177 15.2      145.           0\n59 0.302    0.636 0.0214 0.177 15.2      145.           0\n60 0.302    0.636 0.0214 0.177 15.2      145.           0\n61 0.302    0.636 0.0214 0.177 15.2      145.           0\n62 0.302    0.636 0.0214 0.177 15.2      145.           0\n63 0.302    0.636 0.0214 0.177 15.2      145.           0\n64 0.302    0.636 0.0214 0.177 15.2      145.           0\n65 0.303    0.636 0.0214 0.177 15.2      145.           0\n66 0.302    0.636 0.0214 0.177 15.2      145.           0\n67 0.302    0.636 0.0214 0.177 15.2      145.           0\n68 0.302    0.636 0.0214 0.177 15.2      145.           0\n69 0.303    0.636 0.0214 0.177 15.2      145.           0\n70 0.302    0.636 0.0214 0.177 15.2      145.           0\n71 0.302    0.636 0.0214 0.177 15.2      145.           0\n72 0.303    0.636 0.0214 0.177 15.2      145.           0\n73 0.302    0.636 0.0214 0.177 15.2      145.           0\n74 0.302    0.636 0.0214 0.177 15.2      145.           0\n75 0.302    0.636 0.0214 0.177 15.2      145.           0\n76 0.303    0.636 0.0214 0.177 15.2      145.           0\n77 0.302    0.636 0.0214 0.177 15.2      145.           0\n78 0.302    0.636 0.0214 0.177 15.2      145.           0\n79 0.303    0.636 0.0214 0.177 15.2      145.           0\n80 0.302    0.636 0.0214 0.177 15.2      145.           0\n81 0.303    0.636 0.0215 0.177 15.2      145.           0\n82 0.548    0.558 0.333  0.308  6.50     381.           0\n83 0.548    0.528 0.281  0.292  6.08     381.           0\n84 0.548    0.554 0.327  0.306  6.45     381.           0\n85 0.548    0.563 0.342  0.311  6.58     381.           0\n86 0.548    0.558 0.332  0.308  6.49     381.           0\n87 0.548    0.589 0.387  0.325  7.00     381.           0\n88 0.548    0.516 0.261  0.286  5.94     381.           0\n89 0.548    0.569 0.352  0.314  6.67     381.           0\n90 0.548    0.561 0.339  0.310  6.55     381.           0\n91 0.548    0.559 0.335  0.309  6.51     381.           0\n92 0.548    0.589 0.387  0.325  7.00     381.           0\n93 0.548    0.515 0.259  0.285  5.93     381.           0\n94 0.549    0.597 0.401  0.330  7.12     381.           0\n95 0.548    0.535 0.293  0.296  6.19     381.           0\n96 0.548    0.533 0.291  0.295  6.16     381.           0\n97 0.549    0.497 0.228  0.275  5.71     381.           0\n98 0.548    0.560 0.337  0.309  6.53     381.           0\n99 0.548    0.525 0.276  0.290  6.05     381.           0\n```\n\n\n:::\n:::\n\n\n\n### With prior on rate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits1 |>\n  filter(priors_scenario == \"rate\", exclude_sp1 == FALSE, exp == 1, convergence == 0) |>\n  mutate(deviance = map2_dbl(fit, exclude_sp1, function(x, y) {\n    overall_deviance(x$par, exp1_data_agg, exclude_sp1 = y)\n  })) |>\n  select(prop:convergence) |>\n  arrange(deviance) |>\n  mutate_all(round, 3) |>\n  print(n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 100 × 7\n     prop prop_ltm  rate   tau  gain deviance convergence\n    <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl>\n  1 0.475    0.645 0.049 0.197  6.92     153.           0\n  2 0.475    0.644 0.049 0.197  6.91     153.           0\n  3 0.475    0.645 0.049 0.197  6.91     153.           0\n  4 0.475    0.644 0.049 0.197  6.91     153.           0\n  5 0.475    0.645 0.049 0.197  6.91     153.           0\n  6 0.475    0.645 0.049 0.197  6.91     153.           0\n  7 0.475    0.645 0.049 0.197  6.91     153.           0\n  8 0.475    0.645 0.049 0.197  6.91     153.           0\n  9 0.475    0.644 0.049 0.197  6.91     153.           0\n 10 0.475    0.644 0.049 0.197  6.90     153.           0\n 11 0.475    0.645 0.049 0.197  6.91     153.           0\n 12 0.475    0.645 0.049 0.197  6.91     153.           0\n 13 0.475    0.644 0.049 0.197  6.91     153.           0\n 14 0.475    0.645 0.049 0.197  6.91     153.           0\n 15 0.475    0.645 0.049 0.197  6.91     153.           0\n 16 0.475    0.645 0.049 0.197  6.91     153.           0\n 17 0.475    0.644 0.049 0.197  6.91     153.           0\n 18 0.475    0.644 0.049 0.197  6.91     153.           0\n 19 0.475    0.645 0.049 0.197  6.91     153.           0\n 20 0.475    0.645 0.049 0.197  6.91     153.           0\n 21 0.475    0.644 0.049 0.197  6.91     153.           0\n 22 0.475    0.644 0.049 0.197  6.91     153.           0\n 23 0.475    0.645 0.049 0.197  6.91     153.           0\n 24 0.475    0.644 0.049 0.197  6.90     153.           0\n 25 0.475    0.644 0.049 0.197  6.91     153.           0\n 26 0.475    0.645 0.049 0.197  6.91     153.           0\n 27 0.475    0.644 0.049 0.197  6.90     153.           0\n 28 0.475    0.644 0.049 0.197  6.91     153.           0\n 29 0.475    0.645 0.049 0.197  6.91     153.           0\n 30 0.475    0.644 0.049 0.197  6.90     153.           0\n 31 0.475    0.644 0.049 0.197  6.91     153.           0\n 32 0.475    0.644 0.049 0.197  6.91     153.           0\n 33 0.475    0.644 0.049 0.197  6.91     153.           0\n 34 0.475    0.645 0.049 0.197  6.90     153.           0\n 35 0.475    0.644 0.049 0.197  6.91     153.           0\n 36 0.475    0.645 0.049 0.197  6.91     153.           0\n 37 0.475    0.644 0.049 0.197  6.91     153.           0\n 38 0.475    0.644 0.049 0.197  6.91     153.           0\n 39 0.475    0.645 0.049 0.197  6.91     153.           0\n 40 0.475    0.645 0.049 0.197  6.91     153.           0\n 41 0.475    0.645 0.049 0.197  6.91     153.           0\n 42 0.475    0.644 0.049 0.197  6.91     153.           0\n 43 0.475    0.644 0.049 0.197  6.90     153.           0\n 44 0.475    0.645 0.049 0.197  6.91     153.           0\n 45 0.475    0.644 0.049 0.197  6.91     153.           0\n 46 0.475    0.644 0.049 0.197  6.90     153.           0\n 47 0.475    0.645 0.049 0.197  6.90     153.           0\n 48 0.475    0.645 0.049 0.197  6.91     153.           0\n 49 0.475    0.645 0.049 0.197  6.91     153.           0\n 50 0.475    0.645 0.049 0.197  6.91     153.           0\n 51 0.475    0.644 0.049 0.197  6.91     153.           0\n 52 0.475    0.645 0.049 0.197  6.91     153.           0\n 53 0.475    0.645 0.049 0.197  6.90     153.           0\n 54 0.475    0.645 0.049 0.197  6.90     153.           0\n 55 0.475    0.644 0.049 0.197  6.90     153.           0\n 56 0.475    0.644 0.049 0.197  6.90     153.           0\n 57 0.475    0.645 0.049 0.197  6.91     153.           0\n 58 0.475    0.644 0.049 0.197  6.91     153.           0\n 59 0.475    0.645 0.049 0.197  6.90     153.           0\n 60 0.475    0.645 0.049 0.197  6.91     153.           0\n 61 0.475    0.645 0.049 0.197  6.91     153.           0\n 62 0.475    0.644 0.049 0.197  6.91     153.           0\n 63 0.475    0.645 0.049 0.197  6.90     153.           0\n 64 0.475    0.645 0.049 0.197  6.90     153.           0\n 65 0.475    0.645 0.049 0.197  6.90     153.           0\n 66 0.475    0.645 0.049 0.197  6.91     153.           0\n 67 0.475    0.645 0.049 0.197  6.91     153.           0\n 68 0.475    0.645 0.049 0.197  6.91     153.           0\n 69 0.475    0.645 0.049 0.197  6.90     153.           0\n 70 0.475    0.645 0.049 0.197  6.91     153.           0\n 71 0.475    0.644 0.049 0.197  6.90     153.           0\n 72 0.475    0.645 0.049 0.197  6.91     153.           0\n 73 0.475    0.644 0.049 0.197  6.90     153.           0\n 74 0.475    0.644 0.049 0.197  6.90     153.           0\n 75 0.475    0.644 0.049 0.197  6.90     153.           0\n 76 0.475    0.645 0.049 0.197  6.91     153.           0\n 77 0.475    0.645 0.049 0.197  6.91     153.           0\n 78 0.475    0.645 0.049 0.197  6.90     153.           0\n 79 0.475    0.645 0.049 0.197  6.90     153.           0\n 80 0.475    0.645 0.049 0.197  6.91     153.           0\n 81 0.475    0.645 0.049 0.197  6.91     153.           0\n 82 0.475    0.645 0.049 0.197  6.90     153.           0\n 83 0.475    0.645 0.049 0.197  6.91     153.           0\n 84 0.475    0.645 0.049 0.197  6.90     153.           0\n 85 0.475    0.645 0.049 0.197  6.91     153.           0\n 86 0.475    0.645 0.049 0.197  6.90     153.           0\n 87 0.475    0.645 0.049 0.197  6.90     153.           0\n 88 0.475    0.645 0.049 0.197  6.90     153.           0\n 89 0.475    0.644 0.049 0.197  6.90     153.           0\n 90 0.475    0.645 0.049 0.197  6.91     153.           0\n 91 0.475    0.645 0.049 0.197  6.90     153.           0\n 92 0.475    0.644 0.049 0.197  6.91     153.           0\n 93 0.475    0.644 0.049 0.197  6.90     153.           0\n 94 0.475    0.644 0.049 0.197  6.90     153.           0\n 95 0.475    0.644 0.049 0.197  6.90     153.           0\n 96 0.475    0.644 0.049 0.197  6.91     153.           0\n 97 0.475    0.644 0.049 0.197  6.91     153.           0\n 98 0.476    0.645 0.049 0.197  6.90     153.           0\n 99 0.476    0.645 0.049 0.197  6.89     153.           0\n100 0.476    0.645 0.049 0.197  6.89     153.           0\n```\n\n\n:::\n\n```{.r .cell-code}\nfit <- fits1 |>\n  filter(priors_scenario == \"rate\", exclude_sp1 == FALSE, exp == 1, convergence == 0) |>\n  arrange(deviance) |>\n  pluck(\"fit\", 1)\n\nexp1_data_agg$pred <- predict(fit, exp1_data_agg, group_by = c(\"chunk\", \"gap\"))\n\nexp1_data_agg |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n## Repeat for expereiment 2\n\nBasic estimation (ignoring first chunk):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntar_load(exp2_data_agg)\nstart <- paper_params(exp = 2)\n(est <- estimate_model(start, data = exp2_data_agg, exclude_sp1 = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$start\n    prop prop_ltm      tau     gain     rate \n   0.170    0.400    0.135   25.000    0.025 \n\n$par\n       prop    prop_ltm         tau        gain        rate \n 0.14126061  0.47910638  0.11945665 40.80721200  0.01854473 \n\n$convergence\n[1] 0\n\n$counts\nfunction gradient \n    1300       NA \n\n$value\n[1] 55.70663\n```\n\n\n:::\n\n```{.r .cell-code}\nexp2_data_agg$pred <- predict(est, exp2_data_agg, group_by = c(\"chunk\", \"gap\"))\nexp2_data_agg |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nagain parameter estimates are different from the paper.\n\nHere are from multiple starting values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits <- fits1 |>\n  filter(priors_scenario == \"\", exclude_sp1 == TRUE, exp == 2, convergence == 0) |>\n  select(prop:convergence, fit, data) |>\n  arrange(deviance) |>\n  mutate_if(is.numeric, round, 3) |>\n  print(n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 98 × 9\n    prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n   <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl> <list>     <list>           \n 1 0.096    0.815 0.006 0.083 100.      40.4           0 <srl_rcl_> <tibble [12 × 8]>\n 2 0.096    0.815 0.006 0.083 100.      40.4           0 <srl_rcl_> <tibble [12 × 8]>\n 3 0.096    0.816 0.006 0.083 100.      40.4           0 <srl_rcl_> <tibble [12 × 8]>\n 4 0.096    0.815 0.006 0.083  99.8     40.4           0 <srl_rcl_> <tibble [12 × 8]>\n 5 0.096    0.815 0.006 0.083  99.7     40.4           0 <srl_rcl_> <tibble [12 × 8]>\n 6 0.096    0.816 0.006 0.083  99.5     40.4           0 <srl_rcl_> <tibble [12 × 8]>\n 7 0.096    0.816 0.006 0.083  99.8     40.4           0 <srl_rcl_> <tibble [12 × 8]>\n 8 0.096    0.816 0.006 0.083 100.      40.4           0 <srl_rcl_> <tibble [12 × 8]>\n 9 0.096    0.815 0.006 0.083 100.      40.4           0 <srl_rcl_> <tibble [12 × 8]>\n10 0.096    0.815 0.006 0.083  99.8     40.4           0 <srl_rcl_> <tibble [12 × 8]>\n11 0.096    0.815 0.006 0.084  99.1     40.4           0 <srl_rcl_> <tibble [12 × 8]>\n12 0.098    0.815 0.006 0.084  97.0     40.4           0 <srl_rcl_> <tibble [12 × 8]>\n13 0.132    0.818 0.009 0.108  54.1     40.4           0 <srl_rcl_> <tibble [12 × 8]>\n14 0.155    0.82  0.01  0.123  40.1     40.5           0 <srl_rcl_> <tibble [12 × 8]>\n15 0.173    0.822 0.011 0.133  32.7     40.6           0 <srl_rcl_> <tibble [12 × 8]>\n16 0.215    0.826 0.013 0.154  22.0     40.8           0 <srl_rcl_> <tibble [12 × 8]>\n17 0.141    0.479 0.019 0.12   40.8     55.7           0 <srl_rcl_> <tibble [12 × 8]>\n18 0.141    0.479 0.019 0.12   40.7     55.7           0 <srl_rcl_> <tibble [12 × 8]>\n19 0.141    0.479 0.019 0.119  40.9     55.7           0 <srl_rcl_> <tibble [12 × 8]>\n20 0.141    0.479 0.019 0.119  40.9     55.7           0 <srl_rcl_> <tibble [12 × 8]>\n21 0.141    0.479 0.019 0.119  41.0     55.7           0 <srl_rcl_> <tibble [12 × 8]>\n22 0.142    0.479 0.019 0.12   40.6     55.7           0 <srl_rcl_> <tibble [12 × 8]>\n23 0.142    0.479 0.019 0.12   40.6     55.7           0 <srl_rcl_> <tibble [12 × 8]>\n24 0.141    0.479 0.018 0.119  41.0     55.7           0 <srl_rcl_> <tibble [12 × 8]>\n25 0.142    0.479 0.019 0.12   40.6     55.7           0 <srl_rcl_> <tibble [12 × 8]>\n26 0.141    0.479 0.018 0.119  41.1     55.7           0 <srl_rcl_> <tibble [12 × 8]>\n27 0.141    0.479 0.019 0.119  40.9     55.7           0 <srl_rcl_> <tibble [12 × 8]>\n28 0.141    0.479 0.019 0.119  40.8     55.7           0 <srl_rcl_> <tibble [12 × 8]>\n29 0.255    0.734 0.297 0.225  28.0     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n30 0.255    0.556 0.155 0.206  16.8     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n31 0.255    0.561 0.159 0.206  17.0     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n32 0.255    0.612 0.2   0.212  19.2     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n33 0.255    0.52  0.126 0.202  15.5     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n34 0.255    0.603 0.192 0.211  18.8     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n35 0.255    0.731 0.294 0.225  27.7     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n36 0.255    0.676 0.251 0.219  23.0     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n37 0.255    0.754 0.313 0.228  30.3     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n38 0.255    0.693 0.264 0.221  24.3     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n39 0.254    0.513 0.12  0.201  15.3     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n40 0.254    0.677 0.251 0.219  23.1     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n41 0.254    0.73  0.294 0.225  27.7     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n42 0.255    0.572 0.168 0.208  17.4     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n43 0.255    0.82  0.366 0.235  41.4     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n44 0.254    0.527 0.132 0.203  15.8     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n45 0.255    0.676 0.251 0.219  23.0     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n46 0.255    0.756 0.315 0.228  30.5     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n47 0.255    0.516 0.123 0.201  15.4     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n48 0.254    0.865 0.401 0.24   55.1     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n49 0.254    0.698 0.268 0.221  24.7     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n50 0.255    0.65  0.23  0.216  21.3     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n51 0.254    0.731 0.294 0.225  27.8     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n52 0.254    0.454 0.074 0.195  13.7     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n53 0.255    0.467 0.084 0.196  14.0     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n54 0.255    0.557 0.156 0.206  16.8     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n55 0.255    0.621 0.207 0.213  19.7     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n56 0.255    0.462 0.08  0.196  13.9     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n57 0.255    0.639 0.221 0.215  20.7     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n58 0.254    0.47  0.086 0.196  14.1     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n59 0.255    0.617 0.204 0.213  19.5     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n60 0.255    0.517 0.124 0.202  15.4     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n61 0.255    0.436 0.059 0.193  13.2     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n62 0.254    0.666 0.242 0.218  22.4     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n63 0.254    0.553 0.152 0.205  16.7     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n64 0.255    0.69  0.262 0.221  24.0     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n65 0.255    0.706 0.275 0.222  25.4     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n66 0.255    0.67  0.246 0.218  22.6     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n67 0.255    0.606 0.195 0.211  18.9     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n68 0.255    0.909 0.437 0.245  81.6     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n69 0.255    0.552 0.152 0.206  16.6     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n70 0.255    0.454 0.074 0.195  13.6     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n71 0.255    0.797 0.347 0.232  36.6     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n72 0.255    0.642 0.224 0.216  20.8     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n73 0.255    0.685 0.258 0.22   23.7     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n74 0.255    0.728 0.293 0.225  27.4     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n75 0.255    0.692 0.263 0.221  24.2     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n76 0.255    0.503 0.113 0.2    15.0     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n77 0.254    0.594 0.185 0.21   18.5     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n78 0.254    0.582 0.176 0.209  17.9     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n79 0.254    0.743 0.304 0.226  29.0     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n80 0.255    0.796 0.347 0.232  36.6     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n81 0.255    0.651 0.231 0.216  21.4     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n82 0.255    0.71  0.278 0.223  25.7     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n83 0.254    0.539 0.141 0.204  16.2     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n84 0.254    0.609 0.197 0.212  19.1     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n85 0.255    0.777 0.331 0.23   33.4     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n86 0.255    0.655 0.234 0.217  21.6     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n87 0.255    0.667 0.244 0.218  22.4     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n88 0.255    0.812 0.359 0.234  39.7     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n89 0.255    0.652 0.232 0.216  21.4     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n90 0.255    0.528 0.133 0.203  15.8     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n91 0.255    0.556 0.155 0.206  16.8     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n92 0.255    0.689 0.261 0.221  24.0     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n93 0.254    0.777 0.331 0.23   33.5     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n94 0.255    0.605 0.194 0.211  18.9     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n95 0.255    0.713 0.28  0.223  26.0     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n96 0.255    0.628 0.212 0.214  19.8     62.6           0 <srl_rcl_> <tibble [12 × 8]>\n97 0.523    0.74  0.127 1       0      547.            0 <srl_rcl_> <tibble [12 × 8]>\n98 0.564    0.435 0.227 1       0      547.            0 <srl_rcl_> <tibble [12 × 8]>\n```\n\n\n:::\n:::\n\n\n### Problem with parameter identifiability\n\nrows 12-16 illustrate the problem with parameter identifiability quite well. They have nearly identical deviance, but very different parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(fits <- fits[c(12, 13, 14, 15, 16), ])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 9\n   prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n  <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl> <list>     <list>           \n1 0.098    0.815 0.006 0.084  97.0     40.4           0 <srl_rcl_> <tibble [12 × 8]>\n2 0.132    0.818 0.009 0.108  54.1     40.4           0 <srl_rcl_> <tibble [12 × 8]>\n3 0.155    0.82  0.01  0.123  40.1     40.5           0 <srl_rcl_> <tibble [12 × 8]>\n4 0.173    0.822 0.011 0.133  32.7     40.6           0 <srl_rcl_> <tibble [12 × 8]>\n5 0.215    0.826 0.013 0.154  22.0     40.8           0 <srl_rcl_> <tibble [12 × 8]>\n```\n\n\n:::\n\n```{.r .cell-code}\n# saveRDS(fits, \"output/five_parsets_exp2.rds\")\n```\n:::\n\n\nPlot the predictions all 5 sets of parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits |>\n  mutate(pred = map2(fit, data, \\(x, y) predict(x, y, group_by = c(\"chunk\", \"gap\")))) |>\n  unnest(c(data, pred)) |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = as.factor(round(prop, 3)))) +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_grid(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThe way parameters change suggest that increasing prop can be compensated by increasing rate, taun and decreasing gain. Here's a pair plot of these parameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits |>\n  select(prop, rate, tau, gain) |>\n  ggpairs(diag = list(continuous = \"blankDiag\"))\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nI'll investigate this in a separate notebook.\n\n### With prior on gain\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- fits1 |>\n  filter(priors_scenario == \"gain\", exclude_sp1 == TRUE, exp == 2, convergence == 0) |>\n  mutate(deviance = map2_dbl(fit, exclude_sp1, function(x, y) {\n    overall_deviance(x$par, exp2_data_agg, exclude_sp1 = y)\n  })) |>\n  select(prop:convergence, fit, data) |>\n  arrange(deviance) |>\n  mutate_if(is.numeric, round, 3) |>\n  print(n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 97 × 9\n    prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n   <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl> <list>     <list>           \n 1 0.2      0.824 0.013 0.147  25.0     40.7           0 <srl_rcl_> <tibble [12 × 8]>\n 2 0.2      0.824 0.013 0.147  25.0     40.7           0 <srl_rcl_> <tibble [12 × 8]>\n 3 0.2      0.824 0.013 0.147  25       40.7           0 <srl_rcl_> <tibble [12 × 8]>\n 4 0.2      0.824 0.013 0.147  25       40.7           0 <srl_rcl_> <tibble [12 × 8]>\n 5 0.2      0.824 0.013 0.147  25.0     40.7           0 <srl_rcl_> <tibble [12 × 8]>\n 6 0.2      0.824 0.013 0.147  25       40.7           0 <srl_rcl_> <tibble [12 × 8]>\n 7 0.2      0.825 0.013 0.147  25       40.7           0 <srl_rcl_> <tibble [12 × 8]>\n 8 0.2      0.825 0.013 0.147  25       40.7           0 <srl_rcl_> <tibble [12 × 8]>\n 9 0.2      0.824 0.013 0.147  25.0     40.7           0 <srl_rcl_> <tibble [12 × 8]>\n10 0.203    0.992 0.013 0.145  25.0     52.4           0 <srl_rcl_> <tibble [12 × 8]>\n11 0.182    0.476 0.024 0.146  25       55.8           0 <srl_rcl_> <tibble [12 × 8]>\n12 0.182    0.476 0.024 0.146  25       55.8           0 <srl_rcl_> <tibble [12 × 8]>\n13 0.254    0.701 0.271 0.222  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n14 0.254    0.701 0.27  0.221  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n15 0.254    0.7   0.27  0.221  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n16 0.254    0.701 0.27  0.222  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n17 0.254    0.702 0.271 0.222  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n18 0.254    0.7   0.27  0.221  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n19 0.255    0.702 0.272 0.222  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n20 0.254    0.701 0.269 0.221  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n21 0.253    0.698 0.266 0.22   25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n22 0.254    0.7   0.268 0.221  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n23 0.256    0.705 0.275 0.223  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n24 0.253    0.698 0.266 0.22   25.0     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n25 0.254    0.699 0.269 0.221  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n26 0.252    0.696 0.264 0.219  25.0     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n27 0.256    0.706 0.276 0.223  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n28 0.263    0.72  0.295 0.23   25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n29 0.263    0.72  0.296 0.231  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n30 0.246    0.682 0.247 0.213  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n31 0.264    0.721 0.298 0.231  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n32 0.26     0.712 0.286 0.227  25.0     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n33 0.252    0.696 0.264 0.219  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n34 0.243    0.673 0.237 0.21   25.0     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n35 0.24     0.666 0.229 0.207  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n36 0.241    0.669 0.232 0.208  25.0     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n37 0.256    0.706 0.276 0.224  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n38 0.255    0.702 0.271 0.222  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n39 0.238    0.662 0.224 0.205  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n40 0.238    0.661 0.222 0.205  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n41 0.237    0.659 0.22  0.204  25.0     62.5           0 <srl_rcl_> <tibble [12 × 8]>\n42 0.235    0.654 0.215 0.203  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n43 0.274    0.74  0.325 0.241  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n44 0.255    0.703 0.273 0.223  25.0     62.6           0 <srl_rcl_> <tibble [12 × 8]>\n45 0.233    0.646 0.208 0.2    25.0     62.6           0 <srl_rcl_> <tibble [12 × 8]>\n46 0.254    0.702 0.271 0.222  25       62.6           0 <srl_rcl_> <tibble [12 × 8]>\n47 0.227    0.631 0.191 0.195  25.0     62.6           0 <srl_rcl_> <tibble [12 × 8]>\n48 0.228    0.632 0.192 0.195  25.0     62.6           0 <srl_rcl_> <tibble [12 × 8]>\n49 0.224    0.619 0.181 0.191  25       62.6           0 <srl_rcl_> <tibble [12 × 8]>\n50 0.22     0.608 0.17  0.188  25       62.6           0 <srl_rcl_> <tibble [12 × 8]>\n51 0.219    0.604 0.166 0.187  25       62.6           0 <srl_rcl_> <tibble [12 × 8]>\n52 0.214    0.587 0.15  0.182  25.0     62.6           0 <srl_rcl_> <tibble [12 × 8]>\n53 0.292    0.77  0.373 0.259  25.0     62.6           0 <srl_rcl_> <tibble [12 × 8]>\n54 0.264    0.721 0.297 0.231  25.0     62.6           0 <srl_rcl_> <tibble [12 × 8]>\n55 0.284    0.757 0.353 0.252  25.0     62.6           0 <srl_rcl_> <tibble [12 × 8]>\n56 0.254    0.702 0.271 0.222  25.0     62.6           0 <srl_rcl_> <tibble [12 × 8]>\n57 0.297    0.777 0.387 0.264  25.0     62.7           0 <srl_rcl_> <tibble [12 × 8]>\n58 0.206    0.555 0.125 0.174  25       62.7           0 <srl_rcl_> <tibble [12 × 8]>\n59 0.254    0.7   0.27  0.222  25.0     62.7           0 <srl_rcl_> <tibble [12 × 8]>\n60 0.255    0.702 0.271 0.222  25       62.7           0 <srl_rcl_> <tibble [12 × 8]>\n61 0.306    0.789 0.41  0.273  25.0     62.7           0 <srl_rcl_> <tibble [12 × 8]>\n62 0.195    0.508 0.088 0.163  25.0     62.7           0 <srl_rcl_> <tibble [12 × 8]>\n63 0.311    0.796 0.423 0.278  25       62.7           0 <srl_rcl_> <tibble [12 × 8]>\n64 0.194    0.498 0.082 0.161  25.0     62.7           0 <srl_rcl_> <tibble [12 × 8]>\n65 0.191    0.483 0.072 0.158  25.0     62.8           0 <srl_rcl_> <tibble [12 × 8]>\n66 0.318    0.804 0.44  0.285  25.0     62.8           0 <srl_rcl_> <tibble [12 × 8]>\n67 0.331    0.819 0.474 0.298  25.0     62.9           0 <srl_rcl_> <tibble [12 × 8]>\n68 0.332    0.819 0.476 0.299  25.0     62.9           0 <srl_rcl_> <tibble [12 × 8]>\n69 0.338    0.825 0.492 0.305  25.0     63.0           0 <srl_rcl_> <tibble [12 × 8]>\n70 0.255    0.703 0.272 0.222  25       63.1           0 <srl_rcl_> <tibble [12 × 8]>\n71 0.347    0.833 0.512 0.314  25       63.1           0 <srl_rcl_> <tibble [12 × 8]>\n72 0.254    0.7   0.27  0.222  25.0     63.2           0 <srl_rcl_> <tibble [12 × 8]>\n73 0.254    0.7   0.269 0.222  25       63.8           0 <srl_rcl_> <tibble [12 × 8]>\n74 0.254    0.7   0.268 0.222  25.0     63.9           0 <srl_rcl_> <tibble [12 × 8]>\n75 0.257    0.706 0.277 0.225  25.0     64.2           0 <srl_rcl_> <tibble [12 × 8]>\n76 0.334    0.56  0.494 0.305  25       82.6           0 <srl_rcl_> <tibble [12 × 8]>\n77 0.334    0.248 0.495 0.305  25       82.6           0 <srl_rcl_> <tibble [12 × 8]>\n78 0.335    0.054 0.496 0.306  25.0     82.6           0 <srl_rcl_> <tibble [12 × 8]>\n79 0.334    0.073 0.494 0.305  25       82.6           0 <srl_rcl_> <tibble [12 × 8]>\n80 0.334    0.437 0.493 0.305  25       82.6           0 <srl_rcl_> <tibble [12 × 8]>\n81 0.331    0.623 0.486 0.302  25       82.6           0 <srl_rcl_> <tibble [12 × 8]>\n82 0.338    0.594 0.503 0.309  25.0     82.6           0 <srl_rcl_> <tibble [12 × 8]>\n83 0.334    0.204 0.494 0.305  25.0     82.6           0 <srl_rcl_> <tibble [12 × 8]>\n84 0.324    0.588 0.468 0.295  25       82.6           0 <srl_rcl_> <tibble [12 × 8]>\n85 0.351    0.609 0.534 0.321  25       82.6           0 <srl_rcl_> <tibble [12 × 8]>\n86 0.334    0.055 0.495 0.305  25.0     82.6           0 <srl_rcl_> <tibble [12 × 8]>\n87 0.356    0.65  0.547 0.327  25       82.6           0 <srl_rcl_> <tibble [12 × 8]>\n88 0.317    0.502 0.45  0.287  25.0     82.6           0 <srl_rcl_> <tibble [12 × 8]>\n89 0.335    0.199 0.495 0.305  25       82.6           0 <srl_rcl_> <tibble [12 × 8]>\n90 0.288    0.604 0.378 0.259  25.0     82.8           0 <srl_rcl_> <tibble [12 × 8]>\n91 0.334    0.243 0.495 0.305  25       82.8           0 <srl_rcl_> <tibble [12 × 8]>\n92 0.335    0.29  0.496 0.306  25       83.0           0 <srl_rcl_> <tibble [12 × 8]>\n93 0.335    0.5   0.495 0.305  25.0     83.1           0 <srl_rcl_> <tibble [12 × 8]>\n94 0.335    0.073 0.496 0.305  25       83.8           0 <srl_rcl_> <tibble [12 × 8]>\n95 0.335    0.348 0.496 0.305  25.0     83.9           0 <srl_rcl_> <tibble [12 × 8]>\n96 0.334    0.426 0.494 0.304  25.0     84.5           0 <srl_rcl_> <tibble [12 × 8]>\n97 0.13     0.35  0.299 0.122  25      458.            0 <srl_rcl_> <tibble [12 × 8]>\n```\n\n\n:::\n:::\n\n\nI see three sets of parameters that are close in deviance (relatively):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits <- fit[c(1, 11, 13), ]\nfits\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 9\n   prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n  <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl> <list>     <list>           \n1 0.2      0.824 0.013 0.147  25.0     40.7           0 <srl_rcl_> <tibble [12 × 8]>\n2 0.182    0.476 0.024 0.146  25       55.8           0 <srl_rcl_> <tibble [12 × 8]>\n3 0.254    0.701 0.271 0.222  25       62.5           0 <srl_rcl_> <tibble [12 × 8]>\n```\n\n\n:::\n:::\n\n\nplots\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits |>\n  mutate(pred = map2(fit, data, \\(x, y) predict(x, y, group_by = c(\"chunk\", \"gap\")))) |>\n  unnest(c(data, pred)) |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = as.factor(round(prop, 3)))) +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_grid(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nthis case is particularly interesting. The bestfitting parameters produce almost no interaction. The other two sets of parameters produce a strong interaction, but misfit the overall data. \n\nFurther, the parameter set with rate 0.024 and 0.271 have quite similar fits despite very different parameter sets!\n\n### with prior on rate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- fits1 |>\n  filter(priors_scenario == \"rate\", exclude_sp1 == TRUE, exp == 2, convergence == 0) |>\n  mutate(deviance = map2_dbl(fit, exclude_sp1, function(x, y) {\n    overall_deviance(x$par, exp2_data_agg, exclude_sp1 = y)\n  })) |>\n  select(prop:convergence, fit, data) |>\n  arrange(deviance) |>\n  mutate_if(is.numeric, round, 3) |>\n  print(n = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 97 × 9\n    prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n   <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl> <list>     <list>           \n 1 0.472    0.845 0.032 0.207  5.68     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n 2 0.472    0.845 0.032 0.207  5.67     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n 3 0.472    0.845 0.032 0.207  5.67     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n 4 0.472    0.845 0.032 0.207  5.67     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n 5 0.472    0.845 0.032 0.207  5.67     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n 6 0.472    0.845 0.032 0.207  5.67     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n 7 0.472    0.845 0.032 0.207  5.67     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n 8 0.472    0.845 0.032 0.207  5.67     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n 9 0.472    0.845 0.032 0.207  5.67     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n10 0.472    0.845 0.032 0.207  5.67     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n11 0.472    0.845 0.032 0.207  5.67     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n12 0.472    0.845 0.032 0.207  5.67     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n13 0.472    0.845 0.032 0.207  5.67     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n14 0.472    0.845 0.032 0.207  5.67     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n15 0.472    0.845 0.032 0.207  5.67     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n16 0.472    0.845 0.032 0.207  5.67     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n17 0.472    0.845 0.032 0.207  5.67     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n18 0.472    0.845 0.032 0.207  5.67     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n19 0.472    0.845 0.032 0.207  5.67     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n20 0.472    0.845 0.032 0.207  5.67     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n21 0.472    0.845 0.032 0.207  5.67     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n22 0.473    0.845 0.032 0.207  5.66     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n23 0.473    0.845 0.032 0.207  5.66     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n24 0.472    0.845 0.032 0.207  5.66     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n25 0.472    0.845 0.032 0.207  5.66     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n26 0.473    0.845 0.032 0.207  5.66     47.2           0 <srl_rcl_> <tibble [12 × 8]>\n27 0.473    0.845 0.032 0.207  5.66     47.3           0 <srl_rcl_> <tibble [12 × 8]>\n28 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n29 0.254    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n30 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n31 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n32 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n33 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n34 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n35 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n36 0.254    0.487 0.1   0.198 14.6      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n37 0.254    0.487 0.1   0.198 14.6      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n38 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n39 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n40 0.254    0.487 0.1   0.198 14.6      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n41 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n42 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n43 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n44 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n45 0.254    0.487 0.1   0.198 14.6      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n46 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n47 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n48 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n49 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n50 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n51 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n52 0.254    0.487 0.1   0.198 14.6      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n53 0.254    0.487 0.1   0.198 14.6      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n54 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n55 0.254    0.487 0.1   0.198 14.6      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n56 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n57 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n58 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n59 0.254    0.487 0.1   0.198 14.6      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n60 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n61 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n62 0.255    0.487 0.1   0.199 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n63 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n64 0.254    0.487 0.1   0.198 14.6      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n65 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n66 0.254    0.487 0.1   0.198 14.6      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n67 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n68 0.254    0.487 0.1   0.198 14.6      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n69 0.254    0.487 0.1   0.198 14.6      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n70 0.254    0.488 0.1   0.198 14.6      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n71 0.255    0.487 0.1   0.199 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n72 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n73 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n74 0.254    0.488 0.1   0.198 14.6      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n75 0.255    0.486 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n76 0.256    0.486 0.1   0.199 14.3      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n77 0.254    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n78 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n79 0.254    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n80 0.254    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n81 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n82 0.255    0.486 0.1   0.198 14.6      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n83 0.473    0.43  0.072 0.259  4.12     63.2           0 <srl_rcl_> <tibble [12 × 8]>\n84 0.473    0.43  0.072 0.259  4.12     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n85 0.473    0.43  0.072 0.259  4.12     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n86 0.473    0.43  0.072 0.259  4.12     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n87 0.473    0.43  0.072 0.259  4.12     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n88 0.473    0.43  0.072 0.259  4.12     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n89 0.473    0.43  0.072 0.259  4.12     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n90 0.473    0.43  0.072 0.259  4.12     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n91 0.473    0.43  0.072 0.259  4.12     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n92 0.473    0.43  0.072 0.259  4.11     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n93 0.474    0.43  0.072 0.259  4.11     63.3           0 <srl_rcl_> <tibble [12 × 8]>\n94 0.526    0.419 0.08  0.265  3.40     65.8           0 <srl_rcl_> <tibble [12 × 8]>\n95 0.5      0.407 0.1   0.272  3.70     67.2           0 <srl_rcl_> <tibble [12 × 8]>\n96 0.548    0.404 0.085 0.269  3.13     67.5           0 <srl_rcl_> <tibble [12 × 8]>\n97 0.533    0.524 0.187 0.295  3.62     69.8           0 <srl_rcl_> <tibble [12 × 8]>\n```\n\n\n:::\n:::\n\n\nplots\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits <- fit[c(28, 83), ]\nfits\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 9\n   prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n  <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl> <list>     <list>           \n1 0.255    0.487 0.1   0.198 14.5      62.5           0 <srl_rcl_> <tibble [12 × 8]>\n2 0.473    0.43  0.072 0.259  4.12     63.2           0 <srl_rcl_> <tibble [12 × 8]>\n```\n\n\n:::\n\n```{.r .cell-code}\nfits |>\n  mutate(pred = map2(fit, data, \\(x, y) predict(x, y, group_by = c(\"chunk\", \"gap\")))) |>\n  unnest(c(data, pred)) |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = as.factor(round(prop, 3)))) +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_grid(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n### Including the first chunk in the optimization\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- paper_params(exp = 2)\n(est <- estimate_model(start, data = exp2_data_agg, exclude_sp1 = FALSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$start\n    prop prop_ltm      tau     gain     rate \n   0.170    0.400    0.135   25.000    0.025 \n\n$par\n       prop    prop_ltm         tau        gain        rate \n 0.34157589  0.85687042  0.19090110 10.21065272  0.01880501 \n\n$convergence\n[1] 0\n\n$counts\nfunction gradient \n     862       NA \n\n$value\n[1] 102.9928\n```\n\n\n:::\n\n```{.r .cell-code}\nexp2_data_agg$pred <- predict(est, exp2_data_agg, group_by = c(\"chunk\", \"gap\"))\nexp2_data_agg |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nfrom multiple starting values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits <- fits1 |>\n  filter(priors_scenario == \"\", exclude_sp1 == FALSE, exp == 2, convergence == 0) |>\n  select(prop:convergence, fit, data) |>\n  arrange(deviance) |>\n  mutate_if(is.numeric, round, 3)\nhead(fits)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 9\n   prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n  <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl> <list>     <list>           \n1 0.342    0.857 0.019 0.191  10.2     103.           0 <srl_rcl_> <tibble [12 × 8]>\n2 0.342    0.857 0.019 0.191  10.2     103.           0 <srl_rcl_> <tibble [12 × 8]>\n3 0.342    0.857 0.019 0.191  10.2     103.           0 <srl_rcl_> <tibble [12 × 8]>\n4 0.342    0.857 0.019 0.191  10.2     103.           0 <srl_rcl_> <tibble [12 × 8]>\n5 0.342    0.857 0.019 0.191  10.2     103.           0 <srl_rcl_> <tibble [12 × 8]>\n6 0.342    0.857 0.019 0.191  10.2     103.           0 <srl_rcl_> <tibble [12 × 8]>\n```\n\n\n:::\n:::\n\n\n### With prior on rate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- fits1 |>\n  filter(priors_scenario == \"rate\", exclude_sp1 == FALSE, exp == 2, convergence == 0) |>\n  mutate(deviance = map2_dbl(fit, exclude_sp1, function(x, y) {\n    overall_deviance(x$par, exp2_data_agg, exclude_sp1 = y)\n  })) |>\n  select(prop:convergence, fit, data) |>\n  arrange(deviance) |>\n  mutate_if(is.numeric, round, 3)\nhead(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 9\n   prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n  <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl> <list>     <list>           \n1 0.484    0.851 0.032 0.205  5.53     108.           0 <srl_rcl_> <tibble [12 × 8]>\n2 0.485    0.851 0.032 0.205  5.53     108.           0 <srl_rcl_> <tibble [12 × 8]>\n3 0.485    0.852 0.032 0.205  5.52     108.           0 <srl_rcl_> <tibble [12 × 8]>\n4 0.485    0.851 0.032 0.205  5.53     108.           0 <srl_rcl_> <tibble [12 × 8]>\n5 0.485    0.852 0.032 0.205  5.52     108.           0 <srl_rcl_> <tibble [12 × 8]>\n6 0.485    0.851 0.032 0.205  5.52     108.           0 <srl_rcl_> <tibble [12 × 8]>\n```\n\n\n:::\n:::\n\n\nplot predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit |>\n  slice(1) |>\n  mutate(pred = map2(fit, data, \\(x, y) predict(x, y, group_by = c(\"chunk\", \"gap\")))) |>\n  unnest(c(data, pred)) |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = as.factor(round(prop, 3)))) +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_grid(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n## Summary\n\n- The parameters reported in the paper are not the best fitting\n- When I start from 100 different starting values, I get better fitting parameters, but with an even lower `rate`\n- I can reproduce the parameters from the paper if I fix the gain parameter to 25\n\n### Best fitting parameters\n\nGiven the different modeling choices (ignoring the first chunk or not, priors on the parameters)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal <- fits1 |>\n  filter(convergence == 0) |>\n  group_by(exp, priors_scenario, exclude_sp1) |>\n  arrange(deviance) |>\n  slice(1) |>\n  arrange(desc(exclude_sp1), exp, priors_scenario) |>\n  mutate(\n    deviance = round(deviance, 1),\n    priors_scenario = case_when(\n      priors_scenario == \"\" ~ \"None\",\n      priors_scenario == \"gain\" ~ \"Gain ~ N(25, 0.1)\",\n      priors_scenario == \"rate\" ~ \"Rate ~ N(0.1, 0.01)\"\n    )\n  )\n\nfinal |>\n  select(exp, priors_scenario, exclude_sp1, prop:gain, deviance) |>\n  mutate_all(round, 3) |>\n  kbl() |>\n  kable_styling()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> exp </th>\n   <th style=\"text-align:left;\"> priors_scenario </th>\n   <th style=\"text-align:left;\"> exclude_sp1 </th>\n   <th style=\"text-align:right;\"> prop </th>\n   <th style=\"text-align:right;\"> prop_ltm </th>\n   <th style=\"text-align:right;\"> rate </th>\n   <th style=\"text-align:right;\"> tau </th>\n   <th style=\"text-align:right;\"> gain </th>\n   <th style=\"text-align:right;\"> deviance </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:left;\"> None </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n   <td style=\"text-align:right;\"> 0.106 </td>\n   <td style=\"text-align:right;\"> 0.575 </td>\n   <td style=\"text-align:right;\"> 0.009 </td>\n   <td style=\"text-align:right;\"> 0.090 </td>\n   <td style=\"text-align:right;\"> 100.000 </td>\n   <td style=\"text-align:right;\"> 31.9 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:left;\"> Gain ~ N(25, 0.1) </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n   <td style=\"text-align:right;\"> 0.222 </td>\n   <td style=\"text-align:right;\"> 0.588 </td>\n   <td style=\"text-align:right;\"> 0.019 </td>\n   <td style=\"text-align:right;\"> 0.154 </td>\n   <td style=\"text-align:right;\"> 25.002 </td>\n   <td style=\"text-align:right;\"> 33.0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:left;\"> Rate ~ N(0.1, 0.01) </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n   <td style=\"text-align:right;\"> 0.434 </td>\n   <td style=\"text-align:right;\"> 0.607 </td>\n   <td style=\"text-align:right;\"> 0.049 </td>\n   <td style=\"text-align:right;\"> 0.202 </td>\n   <td style=\"text-align:right;\"> 7.593 </td>\n   <td style=\"text-align:right;\"> 43.1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2 </td>\n   <td style=\"text-align:left;\"> None </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n   <td style=\"text-align:right;\"> 0.096 </td>\n   <td style=\"text-align:right;\"> 0.815 </td>\n   <td style=\"text-align:right;\"> 0.006 </td>\n   <td style=\"text-align:right;\"> 0.083 </td>\n   <td style=\"text-align:right;\"> 99.998 </td>\n   <td style=\"text-align:right;\"> 40.4 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2 </td>\n   <td style=\"text-align:left;\"> Gain ~ N(25, 0.1) </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n   <td style=\"text-align:right;\"> 0.200 </td>\n   <td style=\"text-align:right;\"> 0.824 </td>\n   <td style=\"text-align:right;\"> 0.013 </td>\n   <td style=\"text-align:right;\"> 0.147 </td>\n   <td style=\"text-align:right;\"> 25.001 </td>\n   <td style=\"text-align:right;\"> 40.7 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2 </td>\n   <td style=\"text-align:left;\"> Rate ~ N(0.1, 0.01) </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n   <td style=\"text-align:right;\"> 0.472 </td>\n   <td style=\"text-align:right;\"> 0.845 </td>\n   <td style=\"text-align:right;\"> 0.032 </td>\n   <td style=\"text-align:right;\"> 0.207 </td>\n   <td style=\"text-align:right;\"> 5.675 </td>\n   <td style=\"text-align:right;\"> 47.2 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:left;\"> None </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 0.302 </td>\n   <td style=\"text-align:right;\"> 0.636 </td>\n   <td style=\"text-align:right;\"> 0.021 </td>\n   <td style=\"text-align:right;\"> 0.177 </td>\n   <td style=\"text-align:right;\"> 15.216 </td>\n   <td style=\"text-align:right;\"> 144.9 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:left;\"> Gain ~ N(25, 0.1) </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 0.231 </td>\n   <td style=\"text-align:right;\"> 0.639 </td>\n   <td style=\"text-align:right;\"> 0.016 </td>\n   <td style=\"text-align:right;\"> 0.156 </td>\n   <td style=\"text-align:right;\"> 24.997 </td>\n   <td style=\"text-align:right;\"> 145.5 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:left;\"> Rate ~ N(0.1, 0.01) </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 0.475 </td>\n   <td style=\"text-align:right;\"> 0.645 </td>\n   <td style=\"text-align:right;\"> 0.049 </td>\n   <td style=\"text-align:right;\"> 0.197 </td>\n   <td style=\"text-align:right;\"> 6.915 </td>\n   <td style=\"text-align:right;\"> 152.8 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2 </td>\n   <td style=\"text-align:left;\"> None </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 0.342 </td>\n   <td style=\"text-align:right;\"> 0.857 </td>\n   <td style=\"text-align:right;\"> 0.019 </td>\n   <td style=\"text-align:right;\"> 0.191 </td>\n   <td style=\"text-align:right;\"> 10.209 </td>\n   <td style=\"text-align:right;\"> 103.0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2 </td>\n   <td style=\"text-align:left;\"> Gain ~ N(25, 0.1) </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 0.210 </td>\n   <td style=\"text-align:right;\"> 0.866 </td>\n   <td style=\"text-align:right;\"> 0.011 </td>\n   <td style=\"text-align:right;\"> 0.150 </td>\n   <td style=\"text-align:right;\"> 24.997 </td>\n   <td style=\"text-align:right;\"> 105.2 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2 </td>\n   <td style=\"text-align:left;\"> Rate ~ N(0.1, 0.01) </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 0.484 </td>\n   <td style=\"text-align:right;\"> 0.851 </td>\n   <td style=\"text-align:right;\"> 0.032 </td>\n   <td style=\"text-align:right;\"> 0.205 </td>\n   <td style=\"text-align:right;\"> 5.534 </td>\n   <td style=\"text-align:right;\"> 107.7 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n- prop_ltm very different between the two experiments (overfitting...)\n\n### Predictions\n\n(the two experiments are modeled separately)\n\n### All predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal |>\n  select(exp, rate, data, pred) |>\n  mutate(exp = paste0(\"Exp \", exp)) |>\n  unnest(c(data, pred)) |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = as.factor(round(rate, 3)))) +\n  scale_color_discrete(\"1st chunk LTM?\") +\n  scale_linetype_discrete(\"Rate\") +\n  facet_grid(exp ~ itemtype) +\n  theme_pub()\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-28-1.png){width=816}\n:::\n:::\n\n\n### Experiment 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal |>\n  filter(exp == 1) |>\n  arrange(rate) |>\n  select(rate, data, pred) |>\n  unnest(c(data, pred)) |>\n  mutate(rate = as.character(round(rate, 4))) |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = )) +\n  scale_color_discrete(\"1st chunk LTM?\") +\n  scale_linetype_discrete(\"Rate\") +\n  facet_grid(rate ~ itemtype) +\n  theme_pub()\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-29-1.png){width=816}\n:::\n:::\n\n\n### Experiment 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal |>\n  filter(exp == 2) |>\n  arrange(rate) |>\n  select(rate, data, pred) |>\n  unnest(c(data, pred)) |>\n  mutate(rate = as.character(round(rate, 4))) |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = )) +\n  scale_color_discrete(\"1st chunk LTM?\") +\n  scale_linetype_discrete(\"Rate\") +\n  facet_grid(rate ~ itemtype) +\n  theme_pub()\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-30-1.png){width=816}\n:::\n:::\n",
    "supporting": [
      "modelling_edas_approach_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}