{
  "hash": "b1fed5a18630b10923a0bdf8c4f547fc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Modeling with the approach reported in the draft\"\nformat: html\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(targets)\ntar_source()\ntar_load_everything()\n```\n:::\n\n\n## Ignoring first chunk in the optimiziation\n\n### Basic estimation of Exp1\n\nLet's apply the modeling approach reported in the paper. We ignore the first chunk (SP1-3) while evaluating the likelihood. Eda did this because the model as implemented predicts the same performance for known and random chunks.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- paper_params()\n(est <- estimate_model(start, data = exp1_data_agg, exclude_sp1 = TRUE))\n#> $start\n#>     prop prop_ltm      tau     gain     rate \n#>     0.21     0.55     0.14    25.00     0.02 \n#> \n#> $par\n#>         prop     prop_ltm          tau         gain         rate \n#>  0.108898668  0.575632670  0.091783835 95.188660503  0.009362919 \n#> \n#> $convergence\n#> [1] 0\n#> \n#> $counts\n#> function gradient \n#>     2080       NA \n#> \n#> $value\n#> [1] 31.92645\nexp1_data_agg$pred <- predict(est, exp1_data_agg, group_by = c(\"chunk\", \"gap\"))\nexp1_data_agg |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/model 1-1.png){width=672}\n:::\n:::\n\n\nI get startlingly different paramemter estimates. Much lower `prop` and `rate` and `tau`, higher `gain`.\n\n### Trying different starting values\n\nI've run this with many different starting values. We tend to end up in different regions of the parameter space (the top result close to the paper's estimates):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits1 |>\n  filter(priors_scenario == \"\", exclude_sp1 == TRUE, exp == 1, deviance <= 50, convergence == 0) |>\n  select(prop:convergence) |>\n  arrange(gain) |>\n  mutate_all(round, 3) |>\n  print(n = 100)\n#> # A tibble: 51 × 7\n#>     prop prop_ltm  rate   tau  gain deviance convergence\n#>    <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl>\n#>  1 0.207    0.586 0.017 0.147  28.4     32.8           0\n#>  2 0.198    0.585 0.017 0.143  30.9     32.6           0\n#>  3 0.184    0.583 0.016 0.136  35.3     32.5           0\n#>  4 0.174    0.582 0.015 0.131  39.2     32.4           0\n#>  5 0.17     0.582 0.014 0.129  41.1     32.3           0\n#>  6 0.141    0.579 0.012 0.113  58.2     32.1           0\n#>  7 0.126    0.578 0.011 0.103  71.6     32.0           0\n#>  8 0.124    0.578 0.011 0.102  74.4     32.0           0\n#>  9 0.106    0.574 0.009 0.09   99.8     31.9           0\n#> 10 0.106    0.577 0.009 0.09   99.8     31.9           0\n#> 11 0.106    0.576 0.009 0.09   99.9     31.9           0\n#> 12 0.101    0.4   0.015 0.088  99.9     46.5           0\n#> 13 0.106    0.575 0.009 0.09   99.9     31.9           0\n#> 14 0.106    0.576 0.009 0.09   99.9     31.9           0\n#> 15 0.106    0.575 0.009 0.09   99.9     31.9           0\n#> 16 0.106    0.577 0.009 0.09   99.9     31.9           0\n#> 17 0.106    0.578 0.009 0.09  100.      31.9           0\n#> 18 0.106    0.578 0.009 0.09  100.      31.9           0\n#> 19 0.106    0.575 0.009 0.09  100.      31.9           0\n#> 20 0.106    0.576 0.009 0.09  100.      31.9           0\n#> 21 0.106    0.576 0.009 0.09  100.      31.9           0\n#> 22 0.106    0.574 0.009 0.09  100.      31.9           0\n#> 23 0.106    0.576 0.009 0.09  100.      31.9           0\n#> 24 0.106    0.575 0.009 0.09  100.      31.9           0\n#> 25 0.106    0.575 0.009 0.09  100.      31.9           0\n#> 26 0.106    0.575 0.009 0.09  100.      31.9           0\n#> 27 0.106    0.575 0.009 0.09  100.      31.9           0\n#> 28 0.106    0.572 0.009 0.09  100.      31.9           0\n#> 29 0.106    0.576 0.009 0.09  100.      31.9           0\n#> 30 0.106    0.575 0.009 0.09  100.      31.9           0\n#> 31 0.106    0.574 0.009 0.09  100.      31.9           0\n#> 32 0.106    0.576 0.009 0.09  100.      31.9           0\n#> 33 0.106    0.58  0.009 0.09  100.      31.9           0\n#> 34 0.106    0.575 0.009 0.09  100.      31.9           0\n#> 35 0.106    0.575 0.009 0.09  100.      31.9           0\n#> 36 0.106    0.575 0.009 0.09  100.      31.9           0\n#> 37 0.106    0.575 0.009 0.09  100.      31.9           0\n#> 38 0.106    0.576 0.009 0.09  100.      31.9           0\n#> 39 0.106    0.574 0.009 0.09  100.      31.9           0\n#> 40 0.106    0.575 0.009 0.09  100.      31.9           0\n#> 41 0.106    0.575 0.009 0.09  100.      31.9           0\n#> 42 0.106    0.576 0.009 0.09  100.      31.9           0\n#> 43 0.106    0.576 0.009 0.09  100.      31.9           0\n#> 44 0.106    0.575 0.009 0.09  100.      31.9           0\n#> 45 0.106    0.575 0.009 0.09  100.      31.9           0\n#> 46 0.106    0.575 0.009 0.09  100       31.9           0\n#> 47 0.106    0.575 0.009 0.09  100       31.9           0\n#> 48 0.106    0.575 0.009 0.09  100       31.9           0\n#> 49 0.106    0.575 0.009 0.09  100       31.9           0\n#> 50 0.106    0.575 0.009 0.09  100       31.9           0\n#> 51 0.106    0.576 0.009 0.09  100       31.9           0\n```\n:::\n\n\n### Trying priors of the gain parameter\n\nOne way to deal with that is to put a prior on the gain parameter to keep it near 25. I know priors are usually a bayesian thing, but they work with ML optimization just as well. On the next set of simulations, I used a Normal(25, 0.1) prior on the gain parameter (could have also fixed it to this value, but this gives me mroe control).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits1 |>\n  filter(priors_scenario == \"gain\", exclude_sp1 == TRUE, exp == 1, deviance <= 50, convergence == 0) |>\n  mutate(deviance = map2_dbl(fit, exclude_sp1, function(x, y) {\n    overall_deviance(x$par, exp1_data_agg, exclude_sp1 = y)\n  })) |>\n  select(prop:convergence) |>\n  arrange(gain) |>\n  mutate_all(round, 3) |>\n  print(n = 100)\n#> # A tibble: 17 × 7\n#>     prop prop_ltm  rate   tau  gain deviance convergence\n#>    <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl>\n#>  1 0.208    0.398 0.031 0.154  25.0     47.7           0\n#>  2 0.222    0.588 0.019 0.154  25.0     33.0           0\n#>  3 0.222    0.588 0.019 0.154  25       33.0           0\n#>  4 0.222    0.587 0.019 0.154  25       33.0           0\n#>  5 0.222    0.587 0.019 0.154  25       33.0           0\n#>  6 0.222    0.588 0.019 0.154  25       33.0           0\n#>  7 0.222    0.588 0.019 0.154  25.0     33.0           0\n#>  8 0.222    0.588 0.019 0.154  25.0     33.0           0\n#>  9 0.222    0.588 0.019 0.154  25.0     33.0           0\n#> 10 0.222    0.588 0.019 0.154  25.0     33.0           0\n#> 11 0.222    0.588 0.019 0.154  25.0     33.0           0\n#> 12 0.222    0.588 0.019 0.154  25.0     33.0           0\n#> 13 0.222    0.588 0.019 0.154  25.0     33.0           0\n#> 14 0.222    0.588 0.019 0.154  25.0     33.0           0\n#> 15 0.222    0.588 0.019 0.154  25.0     33.0           0\n#> 16 0.222    0.588 0.019 0.154  25.0     33.0           0\n#> 17 0.222    0.588 0.019 0.154  25.0     33.0           0\n```\n:::\n\n\nSo we do get at least some parameters that are close to that reported in the paper. The predictions with those parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- fits1 |>\n  filter(priors_scenario == \"gain\", exclude_sp1 == TRUE, exp == 1, convergence == 0) |>\n  arrange(deviance) |>\n  pluck(\"fit\", 1)\n\nexp1_data_agg$pred <- predict(fit, exp1_data_agg, group_by = c(\"chunk\", \"gap\"))\n\nexp1_data_agg |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n### Trying priors on the rate parameter\n\nMaybe there is a region with a higher rate that we have not explored? Let's try a prior on the rate parameter, ~ Normal(0.1, 0.01).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits1 |>\n  filter(priors_scenario == \"rate\", exclude_sp1 == TRUE, exp == 1, convergence == 0) |>\n  mutate(deviance = map2_dbl(fit, exclude_sp1, function(x, y) {\n    overall_deviance(x$par, exp1_data_agg, exclude_sp1 = y)\n  })) |>\n  select(prop:convergence) |>\n  arrange(deviance) |>\n  mutate_all(round, 3) |>\n  print(n = 100)\n#> # A tibble: 100 × 7\n#>      prop prop_ltm  rate   tau  gain deviance convergence\n#>     <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl>\n#>   1 0.434    0.607 0.049 0.202  7.59     43.1           0\n#>   2 0.435    0.608 0.049 0.202  7.59     43.1           0\n#>   3 0.435    0.607 0.049 0.202  7.59     43.1           0\n#>   4 0.435    0.608 0.049 0.202  7.59     43.1           0\n#>   5 0.434    0.607 0.049 0.202  7.60     43.1           0\n#>   6 0.435    0.607 0.049 0.202  7.59     43.1           0\n#>   7 0.435    0.608 0.049 0.202  7.59     43.1           0\n#>   8 0.434    0.607 0.049 0.202  7.59     43.1           0\n#>   9 0.435    0.607 0.049 0.202  7.59     43.1           0\n#>  10 0.435    0.607 0.049 0.202  7.59     43.1           0\n#>  11 0.435    0.607 0.049 0.202  7.59     43.1           0\n#>  12 0.435    0.608 0.049 0.202  7.59     43.1           0\n#>  13 0.434    0.607 0.049 0.202  7.59     43.1           0\n#>  14 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  15 0.435    0.607 0.049 0.202  7.59     43.1           0\n#>  16 0.435    0.607 0.049 0.202  7.59     43.1           0\n#>  17 0.435    0.607 0.049 0.202  7.59     43.1           0\n#>  18 0.435    0.607 0.049 0.202  7.59     43.1           0\n#>  19 0.435    0.607 0.049 0.202  7.59     43.1           0\n#>  20 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  21 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  22 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  23 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  24 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  25 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  26 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  27 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  28 0.435    0.608 0.049 0.202  7.58     43.1           0\n#>  29 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  30 0.435    0.607 0.049 0.202  7.59     43.1           0\n#>  31 0.435    0.608 0.049 0.202  7.58     43.1           0\n#>  32 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  33 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  34 0.435    0.607 0.049 0.202  7.59     43.1           0\n#>  35 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  36 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  37 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  38 0.435    0.607 0.049 0.202  7.59     43.1           0\n#>  39 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  40 0.435    0.607 0.049 0.202  7.59     43.1           0\n#>  41 0.435    0.607 0.049 0.202  7.59     43.1           0\n#>  42 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  43 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  44 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  45 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  46 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  47 0.435    0.607 0.049 0.202  7.59     43.1           0\n#>  48 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  49 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  50 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  51 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  52 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  53 0.435    0.607 0.049 0.202  7.59     43.1           0\n#>  54 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  55 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  56 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  57 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  58 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  59 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  60 0.435    0.608 0.049 0.202  7.58     43.1           0\n#>  61 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  62 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  63 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  64 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  65 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  66 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  67 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  68 0.435    0.607 0.049 0.202  7.58     43.1           0\n#>  69 0.435    0.607 0.049 0.202  7.58     43.2           0\n#>  70 0.435    0.607 0.049 0.202  7.58     43.2           0\n#>  71 0.435    0.608 0.049 0.202  7.58     43.2           0\n#>  72 0.435    0.607 0.049 0.202  7.58     43.2           0\n#>  73 0.435    0.607 0.049 0.202  7.58     43.2           0\n#>  74 0.435    0.607 0.049 0.202  7.58     43.2           0\n#>  75 0.435    0.607 0.049 0.202  7.58     43.2           0\n#>  76 0.435    0.608 0.049 0.202  7.58     43.2           0\n#>  77 0.435    0.607 0.049 0.202  7.58     43.2           0\n#>  78 0.435    0.607 0.049 0.202  7.58     43.2           0\n#>  79 0.39     0.375 0.071 0.218  7.89     55.8           0\n#>  80 0.39     0.375 0.071 0.218  7.88     55.8           0\n#>  81 0.39     0.375 0.071 0.218  7.88     55.8           0\n#>  82 0.39     0.375 0.071 0.218  7.87     55.8           0\n#>  83 0.39     0.375 0.071 0.218  7.88     55.8           0\n#>  84 0.39     0.375 0.071 0.218  7.88     55.8           0\n#>  85 0.39     0.375 0.071 0.218  7.87     55.8           0\n#>  86 0.39     0.375 0.071 0.218  7.88     55.8           0\n#>  87 0.39     0.375 0.071 0.218  7.87     55.8           0\n#>  88 0.39     0.375 0.071 0.218  7.87     55.8           0\n#>  89 0.39     0.375 0.071 0.218  7.88     55.8           0\n#>  90 0.39     0.375 0.071 0.218  7.87     55.8           0\n#>  91 0.39     0.375 0.071 0.218  7.87     55.8           0\n#>  92 0.39     0.375 0.071 0.218  7.87     55.8           0\n#>  93 0.39     0.375 0.071 0.218  7.87     55.8           0\n#>  94 0.39     0.375 0.071 0.218  7.87     55.8           0\n#>  95 0.39     0.375 0.071 0.218  7.87     55.8           0\n#>  96 0.39     0.375 0.071 0.219  7.88     55.8           0\n#>  97 0.391    0.375 0.071 0.218  7.86     55.8           0\n#>  98 0.391    0.375 0.071 0.218  7.87     55.8           0\n#>  99 0.391    0.375 0.071 0.219  7.86     55.8           0\n#> 100 0.546    0.281 0.151 0.254  4.08     92.1           0\n```\n:::\n\n\nDeviance is quite much higher. Predictions?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- fits1 |>\n  filter(priors_scenario == \"rate\", exclude_sp1 == TRUE, exp == 1, convergence == 0) |>\n  mutate(deviance = map2_dbl(fit, exclude_sp1, function(x, y) {\n    overall_deviance(x$par, exp1_data_agg, exclude_sp1 = y)\n  })) |>\n  arrange(deviance) |>\n  pluck(\"fit\", 1)\n\nfit$par\n#>      prop  prop_ltm      rate       tau      gain \n#> 0.4344079 0.6073675 0.0490612 0.2015930 7.5932354\n\nexp1_data_agg$pred <- predict(fit, exp1_data_agg, group_by = c(\"chunk\", \"gap\"))\n\nexp1_data_agg |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nGeater mismatch. Let's include error bars of the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp1_data |>\n  group_by(id, chunk, gap, itemtype) |>\n  summarise(\n    n_total = dplyr::n(),\n    n_correct = sum(cor),\n    p_correct = mean(cor)\n  ) |>\n  ungroup() |>\n  left_join(\n    select(exp1_data_agg, chunk, gap, itemtype, pred),\n    by = c(\"chunk\", \"gap\", \"itemtype\")\n  ) |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  stat_summary() +\n  stat_summary(aes(y = pred), linetype = \"dashed\", geom = \"line\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n#> `summarise()` has grouped output by 'id', 'chunk', 'gap'. You can override using the `.groups` argument.\n#> No summary function supplied, defaulting to `mean_se()`\n#> No summary function supplied, defaulting to `mean_se()`\n#> No summary function supplied, defaulting to `mean_se()`\n#> No summary function supplied, defaulting to `mean_se()`\n#> No summary function supplied, defaulting to `mean_se()`\n#> No summary function supplied, defaulting to `mean_se()`\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nParaneters seem consistent with the data (see [my notes](../dev/notes.md)).\n\n## Including the first chunk in the optimization\n\nThe reports above followed the approach in the current draft and excluded the first chunk from the calculation of the likelihood when optimizing the parameters. Let's include it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- paper_params()\n(est <- estimate_model(start, data = exp1_data_agg, exclude_sp1 = FALSE))\n#> $start\n#>     prop prop_ltm      tau     gain     rate \n#>     0.21     0.55     0.14    25.00     0.02 \n#> \n#> $par\n#>        prop    prop_ltm         tau        gain        rate \n#>  0.30248729  0.63594776  0.17698525 15.21043755  0.02143605 \n#> \n#> $convergence\n#> [1] 0\n#> \n#> $counts\n#> function gradient \n#>     1276       NA \n#> \n#> $value\n#> [1] 144.8785\nexp1_data_agg$pred <- predict(est, exp1_data_agg, group_by = c(\"chunk\", \"gap\"))\nexp1_data_agg |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nIn this case I didn't have to use many starting values - the result is reached from almost everywhere:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits1 |>\n  filter(priors_scenario == \"\", exclude_sp1 == FALSE, exp == 1, convergence == 0) |>\n  arrange(deviance) |>\n  select(prop:convergence) |>\n  print(n = 100)\n#> # A tibble: 99 × 7\n#>     prop prop_ltm   rate   tau  gain deviance convergence\n#>    <dbl>    <dbl>  <dbl> <dbl> <dbl>    <dbl>       <int>\n#>  1 0.302    0.636 0.0214 0.177 15.2      145.           0\n#>  2 0.302    0.636 0.0214 0.177 15.2      145.           0\n#>  3 0.302    0.636 0.0214 0.177 15.2      145.           0\n#>  4 0.303    0.636 0.0214 0.177 15.2      145.           0\n#>  5 0.303    0.636 0.0214 0.177 15.2      145.           0\n#>  6 0.302    0.636 0.0214 0.177 15.2      145.           0\n#>  7 0.302    0.636 0.0214 0.177 15.2      145.           0\n#>  8 0.303    0.636 0.0214 0.177 15.2      145.           0\n#>  9 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 10 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 11 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 12 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 13 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 14 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 15 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 16 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 17 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 18 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 19 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 20 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 21 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 22 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 23 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 24 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 25 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 26 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 27 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 28 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 29 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 30 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 31 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 32 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 33 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 34 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 35 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 36 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 37 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 38 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 39 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 40 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 41 0.303    0.636 0.0215 0.177 15.2      145.           0\n#> 42 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 43 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 44 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 45 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 46 0.303    0.636 0.0215 0.177 15.2      145.           0\n#> 47 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 48 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 49 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 50 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 51 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 52 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 53 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 54 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 55 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 56 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 57 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 58 0.303    0.636 0.0215 0.177 15.2      145.           0\n#> 59 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 60 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 61 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 62 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 63 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 64 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 65 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 66 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 67 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 68 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 69 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 70 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 71 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 72 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 73 0.303    0.636 0.0215 0.177 15.2      145.           0\n#> 74 0.302    0.636 0.0214 0.177 15.3      145.           0\n#> 75 0.302    0.636 0.0214 0.177 15.3      145.           0\n#> 76 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 77 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 78 0.302    0.636 0.0214 0.177 15.2      145.           0\n#> 79 0.303    0.636 0.0215 0.177 15.2      145.           0\n#> 80 0.302    0.636 0.0215 0.177 15.2      145.           0\n#> 81 0.303    0.636 0.0214 0.177 15.2      145.           0\n#> 82 0.548    0.558 0.333  0.308  6.50     381.           0\n#> 83 0.548    0.533 0.291  0.295  6.16     381.           0\n#> 84 0.548    0.528 0.281  0.292  6.08     381.           0\n#> 85 0.548    0.554 0.327  0.306  6.45     381.           0\n#> 86 0.548    0.563 0.342  0.311  6.58     381.           0\n#> 87 0.548    0.558 0.332  0.308  6.49     381.           0\n#> 88 0.548    0.589 0.387  0.325  7.00     381.           0\n#> 89 0.548    0.559 0.335  0.309  6.51     381.           0\n#> 90 0.548    0.525 0.276  0.290  6.05     381.           0\n#> 91 0.548    0.516 0.261  0.286  5.94     381.           0\n#> 92 0.548    0.560 0.337  0.309  6.53     381.           0\n#> 93 0.548    0.569 0.352  0.314  6.67     381.           0\n#> 94 0.548    0.561 0.339  0.310  6.55     381.           0\n#> 95 0.549    0.497 0.228  0.275  5.71     381.           0\n#> 96 0.548    0.589 0.387  0.325  7.00     381.           0\n#> 97 0.548    0.515 0.259  0.285  5.93     381.           0\n#> 98 0.549    0.597 0.401  0.330  7.12     381.           0\n#> 99 0.548    0.535 0.293  0.296  6.19     381.           0\n```\n:::\n\n\n\n### With prior on rate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits1 |>\n  filter(priors_scenario == \"rate\", exclude_sp1 == FALSE, exp == 1, convergence == 0) |>\n  mutate(deviance = map2_dbl(fit, exclude_sp1, function(x, y) {\n    overall_deviance(x$par, exp1_data_agg, exclude_sp1 = y)\n  })) |>\n  select(prop:convergence) |>\n  arrange(deviance) |>\n  mutate_all(round, 3) |>\n  print(n = 100)\n#> # A tibble: 100 × 7\n#>      prop prop_ltm  rate   tau  gain deviance convergence\n#>     <dbl>    <dbl> <dbl> <dbl> <dbl>    <dbl>       <dbl>\n#>   1 0.475    0.645 0.049 0.197  6.92     153.           0\n#>   2 0.475    0.644 0.049 0.197  6.91     153.           0\n#>   3 0.475    0.645 0.049 0.197  6.91     153.           0\n#>   4 0.475    0.644 0.049 0.197  6.91     153.           0\n#>   5 0.475    0.645 0.049 0.197  6.91     153.           0\n#>   6 0.475    0.645 0.049 0.197  6.91     153.           0\n#>   7 0.475    0.645 0.049 0.197  6.91     153.           0\n#>   8 0.475    0.645 0.049 0.197  6.91     153.           0\n#>   9 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  10 0.475    0.644 0.049 0.197  6.90     153.           0\n#>  11 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  12 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  13 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  14 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  15 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  16 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  17 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  18 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  19 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  20 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  21 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  22 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  23 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  24 0.475    0.644 0.049 0.197  6.90     153.           0\n#>  25 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  26 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  27 0.475    0.644 0.049 0.197  6.90     153.           0\n#>  28 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  29 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  30 0.475    0.644 0.049 0.197  6.90     153.           0\n#>  31 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  32 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  33 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  34 0.475    0.645 0.049 0.197  6.90     153.           0\n#>  35 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  36 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  37 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  38 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  39 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  40 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  41 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  42 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  43 0.475    0.644 0.049 0.197  6.90     153.           0\n#>  44 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  45 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  46 0.475    0.644 0.049 0.197  6.90     153.           0\n#>  47 0.475    0.645 0.049 0.197  6.90     153.           0\n#>  48 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  49 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  50 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  51 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  52 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  53 0.475    0.645 0.049 0.197  6.90     153.           0\n#>  54 0.475    0.645 0.049 0.197  6.90     153.           0\n#>  55 0.475    0.644 0.049 0.197  6.90     153.           0\n#>  56 0.475    0.644 0.049 0.197  6.90     153.           0\n#>  57 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  58 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  59 0.475    0.645 0.049 0.197  6.90     153.           0\n#>  60 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  61 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  62 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  63 0.475    0.645 0.049 0.197  6.90     153.           0\n#>  64 0.475    0.645 0.049 0.197  6.90     153.           0\n#>  65 0.475    0.645 0.049 0.197  6.90     153.           0\n#>  66 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  67 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  68 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  69 0.475    0.645 0.049 0.197  6.90     153.           0\n#>  70 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  71 0.475    0.644 0.049 0.197  6.90     153.           0\n#>  72 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  73 0.475    0.644 0.049 0.197  6.90     153.           0\n#>  74 0.475    0.644 0.049 0.197  6.90     153.           0\n#>  75 0.475    0.644 0.049 0.197  6.90     153.           0\n#>  76 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  77 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  78 0.475    0.645 0.049 0.197  6.90     153.           0\n#>  79 0.475    0.645 0.049 0.197  6.90     153.           0\n#>  80 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  81 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  82 0.475    0.645 0.049 0.197  6.90     153.           0\n#>  83 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  84 0.475    0.645 0.049 0.197  6.90     153.           0\n#>  85 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  86 0.475    0.645 0.049 0.197  6.90     153.           0\n#>  87 0.475    0.645 0.049 0.197  6.90     153.           0\n#>  88 0.475    0.645 0.049 0.197  6.90     153.           0\n#>  89 0.475    0.644 0.049 0.197  6.90     153.           0\n#>  90 0.475    0.645 0.049 0.197  6.91     153.           0\n#>  91 0.475    0.645 0.049 0.197  6.90     153.           0\n#>  92 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  93 0.475    0.644 0.049 0.197  6.90     153.           0\n#>  94 0.475    0.644 0.049 0.197  6.90     153.           0\n#>  95 0.475    0.644 0.049 0.197  6.90     153.           0\n#>  96 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  97 0.475    0.644 0.049 0.197  6.91     153.           0\n#>  98 0.476    0.645 0.049 0.197  6.90     153.           0\n#>  99 0.476    0.645 0.049 0.197  6.89     153.           0\n#> 100 0.476    0.645 0.049 0.197  6.89     153.           0\n\nfit <- fits1 |>\n  filter(priors_scenario == \"rate\", exclude_sp1 == FALSE, exp == 1, convergence == 0) |>\n  arrange(deviance) |>\n  pluck(\"fit\", 1)\n\nexp1_data_agg$pred <- predict(fit, exp1_data_agg, group_by = c(\"chunk\", \"gap\"))\n\nexp1_data_agg |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n## Experiment 2\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- paper_params(exp = 2)\n(est <- estimate_model(start, data = exp2_data_agg, exclude_sp1 = TRUE))\n#> $start\n#>     prop prop_ltm      tau     gain     rate \n#>    0.170    0.400    0.135   25.000    0.025 \n#> \n#> $par\n#>        prop    prop_ltm         tau        gain        rate \n#>  0.14126061  0.47910638  0.11945665 40.80721200  0.01854473 \n#> \n#> $convergence\n#> [1] 0\n#> \n#> $counts\n#> function gradient \n#>     1300       NA \n#> \n#> $value\n#> [1] 55.70663\nexp2_data_agg$pred <- predict(est, exp2_data_agg, group_by = c(\"chunk\", \"gap\"))\nexp2_data_agg |>\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n```\n\n::: {.cell-output-display}\n![](modelling_edas_approach_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "modelling_edas_approach_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}