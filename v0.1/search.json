[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This is a collection of development notes and reports related to the modelling of free time and chunking proactive benefits in working memory.\nWorking title: How does long-term memory help working memory? A comparison of long-term memory and free time benefitson working memory\nAuthors: Eda Mizrak, Vencislav Popov and Klaus Oberauer",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#project-information",
    "href": "index.html#project-information",
    "title": "About",
    "section": "",
    "text": "This is a collection of development notes and reports related to the modelling of free time and chunking proactive benefits in working memory.\nWorking title: How does long-term memory help working memory? A comparison of long-term memory and free time benefitson working memory\nAuthors: Eda Mizrak, Vencislav Popov and Klaus Oberauer",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#how-to-download-and-reproduce-the-analyses",
    "href": "index.html#how-to-download-and-reproduce-the-analyses",
    "title": "About",
    "section": "How to download and reproduce the analyses",
    "text": "How to download and reproduce the analyses\nAll the code and data used in this project can be found in the GitHub repository. You can download the repository as a zip file and extract it on your computer, or you can clone the repository using git by running the following command in your terminal:\ngit clone https://github.com/venpopov/ltmTimeBenefit.git\nTo reproduce the analyses in this project, you will need to have R and maybe RStudio installed on your computer. You can download R from the Comprehensive R Archive Network (CRAN), and RStudio from the RStudio website. If you are not using RStudio, you need to install Quarto to render the quarto reports (this website).\nThe project is structured so that you can replicate it on any machine without having to worry about R package versions, file paths, etc. Once you have R and RStudio installed, you can open the ltmTimeBenefit.Rproj file in RStudio, which will set the working directory to the base directory of the project. We use the renv package to record the specific R packages and their versions necessary for this project. When you open the project in RStudio, the renv will first install itself (if it is not already installed). If for some reason the renv package is not installed, you can install it by running install.packages(\"renv\") in the R console.\nAfter the renv package is installed, you can install all the packages used in the project by running renv::restore(). This will install all the packages listed in the renv.lock file in a separate library specific to this project.\nWe use the targets package to manage the workflow of the project. You can run the entire workflow by running targets::tar_make(). This will generate all computational outputs in the projectm which can then be used to generate the reports or do further analyses.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#folder-structure",
    "href": "index.html#folder-structure",
    "title": "About",
    "section": "Folder structure",
    "text": "Folder structure\nThe project is organized in the following main folders:\n\ndata-raw: raw data files\noutput: .rds files containing computational outputs\nquarto: source files for the quarto reports. This entire website is generated from these files.\nR: custom R functions (no interactive code)",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#website-navigation",
    "href": "index.html#website-navigation",
    "title": "About",
    "section": "Website navigation",
    "text": "Website navigation\nIn the side bar you will find:\n\nDevelopment notes: an unorganized collection of notebooks. These are mostly for my own reference during model development.\nNotebooks: a collection of notebooks that are more organized and presentable. These are intended to be shared with others.\nReports: a collection of reports that summarize the development of the model and the results of the model.\nFunction reference: documentation of custom functions used in the project.\n\nPS: I am experimenting with quarto websites for project documentation and reporting. This is a first attempt!",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "docs/reference/inv_logit.html",
    "href": "docs/reference/inv_logit.html",
    "title": "Inverse Logit Transformation",
    "section": "",
    "text": "This function applies the inverse logit transformation to a given value.\n\n\n\ninv_logit(x, lb = 0, ub = 1)\n\n\n\n\nx: The input value to be transformed.\nlb: The lower bound of the transformed value. Default is 0.\nub: The upper bound of the transformed value. Default is 1.\n\n\n\n\nThe transformed value between the lower and upper bounds.\n\n\n\ninv_logit(0) # returns 0.5\ninv_logit(0, lb = 0, ub = 10) # returns 5",
    "crumbs": [
      "Function reference",
      "Inverse Logit Transformation"
    ]
  },
  {
    "objectID": "docs/reference/inv_logit.html#description",
    "href": "docs/reference/inv_logit.html#description",
    "title": "Inverse Logit Transformation",
    "section": "",
    "text": "This function applies the inverse logit transformation to a given value.",
    "crumbs": [
      "Function reference",
      "Inverse Logit Transformation"
    ]
  },
  {
    "objectID": "docs/reference/inv_logit.html#usage",
    "href": "docs/reference/inv_logit.html#usage",
    "title": "Inverse Logit Transformation",
    "section": "",
    "text": "inv_logit(x, lb = 0, ub = 1)",
    "crumbs": [
      "Function reference",
      "Inverse Logit Transformation"
    ]
  },
  {
    "objectID": "docs/reference/inv_logit.html#arguments",
    "href": "docs/reference/inv_logit.html#arguments",
    "title": "Inverse Logit Transformation",
    "section": "",
    "text": "x: The input value to be transformed.\nlb: The lower bound of the transformed value. Default is 0.\nub: The upper bound of the transformed value. Default is 1.",
    "crumbs": [
      "Function reference",
      "Inverse Logit Transformation"
    ]
  },
  {
    "objectID": "docs/reference/inv_logit.html#value",
    "href": "docs/reference/inv_logit.html#value",
    "title": "Inverse Logit Transformation",
    "section": "",
    "text": "The transformed value between the lower and upper bounds.",
    "crumbs": [
      "Function reference",
      "Inverse Logit Transformation"
    ]
  },
  {
    "objectID": "docs/reference/inv_logit.html#examples",
    "href": "docs/reference/inv_logit.html#examples",
    "title": "Inverse Logit Transformation",
    "section": "",
    "text": "inv_logit(0) # returns 0.5\ninv_logit(0, lb = 0, ub = 10) # returns 5",
    "crumbs": [
      "Function reference",
      "Inverse Logit Transformation"
    ]
  },
  {
    "objectID": "docs/reference/get_data.html",
    "href": "docs/reference/get_data.html",
    "title": "get_data function",
    "section": "",
    "text": "This function takes a file path as input and returns preprocessed data.\n\n\n\nget_data(path, ...)\n\n\n\n\npath: The file path to the RData file.\n\n\n\n\nextract_object_from_rdata preprocess_data\n\n\n\nThe preprocessed data.",
    "crumbs": [
      "Function reference",
      "get_data function"
    ]
  },
  {
    "objectID": "docs/reference/get_data.html#description",
    "href": "docs/reference/get_data.html#description",
    "title": "get_data function",
    "section": "",
    "text": "This function takes a file path as input and returns preprocessed data.",
    "crumbs": [
      "Function reference",
      "get_data function"
    ]
  },
  {
    "objectID": "docs/reference/get_data.html#usage",
    "href": "docs/reference/get_data.html#usage",
    "title": "get_data function",
    "section": "",
    "text": "get_data(path, ...)",
    "crumbs": [
      "Function reference",
      "get_data function"
    ]
  },
  {
    "objectID": "docs/reference/get_data.html#arguments",
    "href": "docs/reference/get_data.html#arguments",
    "title": "get_data function",
    "section": "",
    "text": "path: The file path to the RData file.",
    "crumbs": [
      "Function reference",
      "get_data function"
    ]
  },
  {
    "objectID": "docs/reference/get_data.html#seealso",
    "href": "docs/reference/get_data.html#seealso",
    "title": "get_data function",
    "section": "",
    "text": "extract_object_from_rdata preprocess_data",
    "crumbs": [
      "Function reference",
      "get_data function"
    ]
  },
  {
    "objectID": "docs/reference/get_data.html#value",
    "href": "docs/reference/get_data.html#value",
    "title": "get_data function",
    "section": "",
    "text": "The preprocessed data.",
    "crumbs": [
      "Function reference",
      "get_data function"
    ]
  },
  {
    "objectID": "docs/reference/logit.html",
    "href": "docs/reference/logit.html",
    "title": "Logit Transformation",
    "section": "",
    "text": "This function performs a logit transformation on a given variable.\n\n\n\nlogit(x, lb = 0, ub = 1)\n\n\n\n\nx: The variable to be transformed.\nlb: The lower bound of the variable (default is 0).\nub: The upper bound of the variable (default is 1).\n\n\n\n\nThe logit-transformed variable.\n\n\n\nlogit(0.5) # returns 0\nlogit(0.25, lb = 0, ub = 0.5) # returns 0\nlogit(0.75, lb = 0.5, ub = 1) # returns 0",
    "crumbs": [
      "Function reference",
      "Logit Transformation"
    ]
  },
  {
    "objectID": "docs/reference/logit.html#description",
    "href": "docs/reference/logit.html#description",
    "title": "Logit Transformation",
    "section": "",
    "text": "This function performs a logit transformation on a given variable.",
    "crumbs": [
      "Function reference",
      "Logit Transformation"
    ]
  },
  {
    "objectID": "docs/reference/logit.html#usage",
    "href": "docs/reference/logit.html#usage",
    "title": "Logit Transformation",
    "section": "",
    "text": "logit(x, lb = 0, ub = 1)",
    "crumbs": [
      "Function reference",
      "Logit Transformation"
    ]
  },
  {
    "objectID": "docs/reference/logit.html#arguments",
    "href": "docs/reference/logit.html#arguments",
    "title": "Logit Transformation",
    "section": "",
    "text": "x: The variable to be transformed.\nlb: The lower bound of the variable (default is 0).\nub: The upper bound of the variable (default is 1).",
    "crumbs": [
      "Function reference",
      "Logit Transformation"
    ]
  },
  {
    "objectID": "docs/reference/logit.html#value",
    "href": "docs/reference/logit.html#value",
    "title": "Logit Transformation",
    "section": "",
    "text": "The logit-transformed variable.",
    "crumbs": [
      "Function reference",
      "Logit Transformation"
    ]
  },
  {
    "objectID": "docs/reference/logit.html#examples",
    "href": "docs/reference/logit.html#examples",
    "title": "Logit Transformation",
    "section": "",
    "text": "logit(0.5) # returns 0\nlogit(0.25, lb = 0, ub = 0.5) # returns 0\nlogit(0.75, lb = 0.5, ub = 1) # returns 0",
    "crumbs": [
      "Function reference",
      "Logit Transformation"
    ]
  },
  {
    "objectID": "docs/reference/run_or_load.html",
    "href": "docs/reference/run_or_load.html",
    "title": "Execute an expression and save the result to a file or load the result from a file if it already exists.",
    "section": "",
    "text": "This function allows you to either run an expression or load the result from a file.\n\n\n\nrun_or_load(expression, file, ..., force = FALSE)\n\n\n\n\nexpression: The expression to be evaluated or loaded.\nfile: The file path where the result will be saved or loaded from.\n...: Additional arguments to be passed to the expression.\nforce: Logical value indicating whether to force the evaluation of the expression, even if the file exists.\n\n\n\n\nThe result of the expression.\n\n\n\n# Run an expression and save the result to a file\nfile &lt;- tempfile(fileext = \".rds\")\nrun_or_load(rnorm(1e7), file)\nrun_or_load(rnorm(1e7), file) # loads the result from the file",
    "crumbs": [
      "Function reference",
      "Execute an expression and save the result to a file or load the result from a file if it already exists."
    ]
  },
  {
    "objectID": "docs/reference/run_or_load.html#description",
    "href": "docs/reference/run_or_load.html#description",
    "title": "Execute an expression and save the result to a file or load the result from a file if it already exists.",
    "section": "",
    "text": "This function allows you to either run an expression or load the result from a file.",
    "crumbs": [
      "Function reference",
      "Execute an expression and save the result to a file or load the result from a file if it already exists."
    ]
  },
  {
    "objectID": "docs/reference/run_or_load.html#usage",
    "href": "docs/reference/run_or_load.html#usage",
    "title": "Execute an expression and save the result to a file or load the result from a file if it already exists.",
    "section": "",
    "text": "run_or_load(expression, file, ..., force = FALSE)",
    "crumbs": [
      "Function reference",
      "Execute an expression and save the result to a file or load the result from a file if it already exists."
    ]
  },
  {
    "objectID": "docs/reference/run_or_load.html#arguments",
    "href": "docs/reference/run_or_load.html#arguments",
    "title": "Execute an expression and save the result to a file or load the result from a file if it already exists.",
    "section": "",
    "text": "expression: The expression to be evaluated or loaded.\nfile: The file path where the result will be saved or loaded from.\n...: Additional arguments to be passed to the expression.\nforce: Logical value indicating whether to force the evaluation of the expression, even if the file exists.",
    "crumbs": [
      "Function reference",
      "Execute an expression and save the result to a file or load the result from a file if it already exists."
    ]
  },
  {
    "objectID": "docs/reference/run_or_load.html#value",
    "href": "docs/reference/run_or_load.html#value",
    "title": "Execute an expression and save the result to a file or load the result from a file if it already exists.",
    "section": "",
    "text": "The result of the expression.",
    "crumbs": [
      "Function reference",
      "Execute an expression and save the result to a file or load the result from a file if it already exists."
    ]
  },
  {
    "objectID": "docs/reference/run_or_load.html#examples",
    "href": "docs/reference/run_or_load.html#examples",
    "title": "Execute an expression and save the result to a file or load the result from a file if it already exists.",
    "section": "",
    "text": "# Run an expression and save the result to a file\nfile &lt;- tempfile(fileext = \".rds\")\nrun_or_load(rnorm(1e7), file)\nrun_or_load(rnorm(1e7), file) # loads the result from the file",
    "crumbs": [
      "Function reference",
      "Execute an expression and save the result to a file or load the result from a file if it already exists."
    ]
  },
  {
    "objectID": "docs/reference/gen_boot_dataset.html",
    "href": "docs/reference/gen_boot_dataset.html",
    "title": "Generate a bootstrapped dataset",
    "section": "",
    "text": "This function takes in a dataset and generates a bootstrapped dataset by randomly sampling unique IDs with replacement. For each ID and condition, it calculates the proportion of correct responses and then generates a new dataset by sampling from a binomial distribution with the calculated proportion of correct responses.\n\n\n\ngen_boot_dataset(data)\n\n\n\n\ndata: The input dataset\n\n\n\n\nA bootstrapped dataset with aggregated summary statistics\n\n\n\ndata &lt;- read.csv(\"data.csv\")\nboot_data &lt;- gen_boot_dataset(data)",
    "crumbs": [
      "Function reference",
      "Generate a bootstrapped dataset"
    ]
  },
  {
    "objectID": "docs/reference/gen_boot_dataset.html#description",
    "href": "docs/reference/gen_boot_dataset.html#description",
    "title": "Generate a bootstrapped dataset",
    "section": "",
    "text": "This function takes in a dataset and generates a bootstrapped dataset by randomly sampling unique IDs with replacement. For each ID and condition, it calculates the proportion of correct responses and then generates a new dataset by sampling from a binomial distribution with the calculated proportion of correct responses.",
    "crumbs": [
      "Function reference",
      "Generate a bootstrapped dataset"
    ]
  },
  {
    "objectID": "docs/reference/gen_boot_dataset.html#usage",
    "href": "docs/reference/gen_boot_dataset.html#usage",
    "title": "Generate a bootstrapped dataset",
    "section": "",
    "text": "gen_boot_dataset(data)",
    "crumbs": [
      "Function reference",
      "Generate a bootstrapped dataset"
    ]
  },
  {
    "objectID": "docs/reference/gen_boot_dataset.html#arguments",
    "href": "docs/reference/gen_boot_dataset.html#arguments",
    "title": "Generate a bootstrapped dataset",
    "section": "",
    "text": "data: The input dataset",
    "crumbs": [
      "Function reference",
      "Generate a bootstrapped dataset"
    ]
  },
  {
    "objectID": "docs/reference/gen_boot_dataset.html#value",
    "href": "docs/reference/gen_boot_dataset.html#value",
    "title": "Generate a bootstrapped dataset",
    "section": "",
    "text": "A bootstrapped dataset with aggregated summary statistics",
    "crumbs": [
      "Function reference",
      "Generate a bootstrapped dataset"
    ]
  },
  {
    "objectID": "docs/reference/gen_boot_dataset.html#examples",
    "href": "docs/reference/gen_boot_dataset.html#examples",
    "title": "Generate a bootstrapped dataset",
    "section": "",
    "text": "data &lt;- read.csv(\"data.csv\")\nboot_data &lt;- gen_boot_dataset(data)",
    "crumbs": [
      "Function reference",
      "Generate a bootstrapped dataset"
    ]
  },
  {
    "objectID": "docs/reference/calcdev.html",
    "href": "docs/reference/calcdev.html",
    "title": "Calculate the deviance of a model",
    "section": "",
    "text": "This function calculates the deviance of a serial_recall model given a set of parameters and data. The deviance is a measure of how well the model fits the data.\n\n\n\ncalcdev(params, dat, exclude_sp1 = FALSE, ...)\n\n\n\n\nparams: A named vector of model parameters\ndat: A data frame containing the data\nexclude_sp1: A logical indicating whether to exclude the first item from the likelihood calculation\n\n\n\n\nThe deviance of the model\n\n\n\nparams &lt;- c(prop = 0.5, prop_ltm = 0.3, tau = 0.2, gain = 1, rate = 0.4)\ndata &lt;- data.frame(\n  ISI = c(100, 200, 300), item_in_ltm = c(TRUE, FALSE, TRUE),\n  n_correct = c(10, 15, 20), n_total = c(20, 20, 20)\n)\ncalcdev(params, data)",
    "crumbs": [
      "Function reference",
      "Calculate the deviance of a model"
    ]
  },
  {
    "objectID": "docs/reference/calcdev.html#description",
    "href": "docs/reference/calcdev.html#description",
    "title": "Calculate the deviance of a model",
    "section": "",
    "text": "This function calculates the deviance of a serial_recall model given a set of parameters and data. The deviance is a measure of how well the model fits the data.",
    "crumbs": [
      "Function reference",
      "Calculate the deviance of a model"
    ]
  },
  {
    "objectID": "docs/reference/calcdev.html#usage",
    "href": "docs/reference/calcdev.html#usage",
    "title": "Calculate the deviance of a model",
    "section": "",
    "text": "calcdev(params, dat, exclude_sp1 = FALSE, ...)",
    "crumbs": [
      "Function reference",
      "Calculate the deviance of a model"
    ]
  },
  {
    "objectID": "docs/reference/calcdev.html#arguments",
    "href": "docs/reference/calcdev.html#arguments",
    "title": "Calculate the deviance of a model",
    "section": "",
    "text": "params: A named vector of model parameters\ndat: A data frame containing the data\nexclude_sp1: A logical indicating whether to exclude the first item from the likelihood calculation",
    "crumbs": [
      "Function reference",
      "Calculate the deviance of a model"
    ]
  },
  {
    "objectID": "docs/reference/calcdev.html#value",
    "href": "docs/reference/calcdev.html#value",
    "title": "Calculate the deviance of a model",
    "section": "",
    "text": "The deviance of the model",
    "crumbs": [
      "Function reference",
      "Calculate the deviance of a model"
    ]
  },
  {
    "objectID": "docs/reference/calcdev.html#examples",
    "href": "docs/reference/calcdev.html#examples",
    "title": "Calculate the deviance of a model",
    "section": "",
    "text": "params &lt;- c(prop = 0.5, prop_ltm = 0.3, tau = 0.2, gain = 1, rate = 0.4)\ndata &lt;- data.frame(\n  ISI = c(100, 200, 300), item_in_ltm = c(TRUE, FALSE, TRUE),\n  n_correct = c(10, 15, 20), n_total = c(20, 20, 20)\n)\ncalcdev(params, data)",
    "crumbs": [
      "Function reference",
      "Calculate the deviance of a model"
    ]
  },
  {
    "objectID": "docs/reference/boot_est.html",
    "href": "docs/reference/boot_est.html",
    "title": "Perform bootstrapped estimation",
    "section": "",
    "text": "This function performs bootstrapped estimation using the provided data and parameters. It uses the serial recall model for estimation\n\n\n\nboot_est(data, start, exclude_sp1 = TRUE, growth = \"asy\", ...)\n\n\n\n\ndata: The input data for estimation.\nstart: The starting values for estimation. If not provided, default values will be used.\nexclude_sp1: Logical value indicating whether to exclude the first item in the data. Default is TRUE.\ngrowth: The growth type for estimation. Default is “asy”.\n...: Additional arguments to be passed to the estimation function.\n\n\n\n\nA data frame containing the estimated model parameters.\n\n\n\ndata &lt;- read.csv(\"data.csv\")\nboot_est(data)",
    "crumbs": [
      "Function reference",
      "Perform bootstrapped estimation"
    ]
  },
  {
    "objectID": "docs/reference/boot_est.html#description",
    "href": "docs/reference/boot_est.html#description",
    "title": "Perform bootstrapped estimation",
    "section": "",
    "text": "This function performs bootstrapped estimation using the provided data and parameters. It uses the serial recall model for estimation",
    "crumbs": [
      "Function reference",
      "Perform bootstrapped estimation"
    ]
  },
  {
    "objectID": "docs/reference/boot_est.html#usage",
    "href": "docs/reference/boot_est.html#usage",
    "title": "Perform bootstrapped estimation",
    "section": "",
    "text": "boot_est(data, start, exclude_sp1 = TRUE, growth = \"asy\", ...)",
    "crumbs": [
      "Function reference",
      "Perform bootstrapped estimation"
    ]
  },
  {
    "objectID": "docs/reference/boot_est.html#arguments",
    "href": "docs/reference/boot_est.html#arguments",
    "title": "Perform bootstrapped estimation",
    "section": "",
    "text": "data: The input data for estimation.\nstart: The starting values for estimation. If not provided, default values will be used.\nexclude_sp1: Logical value indicating whether to exclude the first item in the data. Default is TRUE.\ngrowth: The growth type for estimation. Default is “asy”.\n...: Additional arguments to be passed to the estimation function.",
    "crumbs": [
      "Function reference",
      "Perform bootstrapped estimation"
    ]
  },
  {
    "objectID": "docs/reference/boot_est.html#value",
    "href": "docs/reference/boot_est.html#value",
    "title": "Perform bootstrapped estimation",
    "section": "",
    "text": "A data frame containing the estimated model parameters.",
    "crumbs": [
      "Function reference",
      "Perform bootstrapped estimation"
    ]
  },
  {
    "objectID": "docs/reference/boot_est.html#examples",
    "href": "docs/reference/boot_est.html#examples",
    "title": "Perform bootstrapped estimation",
    "section": "",
    "text": "data &lt;- read.csv(\"data.csv\")\nboot_est(data)",
    "crumbs": [
      "Function reference",
      "Perform bootstrapped estimation"
    ]
  },
  {
    "objectID": "docs/notebooks/model-explore.html",
    "href": "docs/notebooks/model-explore.html",
    "title": "Exploring model predictions",
    "section": "",
    "text": "The motivation for the paper was based on the assumption that the resource model should predict an interaction between the proactive benefits of free time and chunking:\n\n“If the LTM benefit arises because encoding a chunk consumes less of an encoding resource, then that benefit should diminish with longer free time, because over time the resource grows back towards its maximal level regardless of how much of it has been consumed by encoding the initial triplet of letters.”\n\nAs we discussed, I was not sure if the model actually predicts this. Exploring the model’s predictions for the memory strength latent variable reveals that this is not what the model predicts. As long as resources have not recovered to their maximum level, the model predicts only additive effects of chunking and free time.",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Exploring model predictions"
    ]
  },
  {
    "objectID": "docs/notebooks/model-explore.html#overview",
    "href": "docs/notebooks/model-explore.html#overview",
    "title": "Exploring model predictions",
    "section": "",
    "text": "The motivation for the paper was based on the assumption that the resource model should predict an interaction between the proactive benefits of free time and chunking:\n\n“If the LTM benefit arises because encoding a chunk consumes less of an encoding resource, then that benefit should diminish with longer free time, because over time the resource grows back towards its maximal level regardless of how much of it has been consumed by encoding the initial triplet of letters.”\n\nAs we discussed, I was not sure if the model actually predicts this. Exploring the model’s predictions for the memory strength latent variable reveals that this is not what the model predicts. As long as resources have not recovered to their maximum level, the model predicts only additive effects of chunking and free time.",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Exploring model predictions"
    ]
  },
  {
    "objectID": "docs/notebooks/model-explore.html#an-example",
    "href": "docs/notebooks/model-explore.html#an-example",
    "title": "Exploring model predictions",
    "section": "An example",
    "text": "An example\nThe plot below shows the predicted recall probability (top panels) and memory strength (bottom panels) as a function of the gap between the presentation of the first and second triplet of letters.\n\n\nCode\nlibrary(tidyverse)\nlibrary(targets)\ntar_source()\ntar_load(\"exp3_data_agg\")\n\nplot_predictions &lt;- function(params, data) {\n  class(params) &lt;- \"serial_recall_pars\"\n  data |&gt;\n    mutate(\n      Probability = predict(params, data, group_by = c(\"gap\", \"chunk\")),\n      Strength = predict(params, data, group_by = c(\"gap\", \"chunk\"), type = \"strength\")\n    ) |&gt;\n    pivot_longer(c(Probability, Strength), names_to = \"type\", values_to = \"value\") |&gt;\n    ggplot(aes(gap, value, color = chunk, group = chunk)) +\n    geom_line() +\n    scale_color_discrete(\"1st chunk LTM?\") +\n    facet_grid(type~itemtype, scales = \"free\") +\n    theme_test(base_size = 14)\n}\n\nparams &lt;- c(prop = 0.4, prop_ltm = 0.55, tau = 0.25, gain = 25, rate = 0.05)\nplot_predictions(params, exp3_data_agg)\n\n\n\n\n\n\n\n\n\nLook first at the bottom panels showing raw memory strength. The lines are exactly parallel until a gap ~ 4500 ms. At that point the model has recovered all the resources consumed by encoding the chunked first triplet of letters. An interaction only occurs after this, because there are still resources left to recover from encoding the random first triplet of letters.\nIn contrast, we see an interaction in the predicted recall probability (top panels) because the model predicts a sigmoidal relationship between memory strength and recall probability. This is a consequence of the logistic function used to map memory strength to recall probability. And as you can see, the interaction actually goes in the opposite direction when performance is low in the third tripplet.",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Exploring model predictions"
    ]
  },
  {
    "objectID": "docs/notebooks/model-explore.html#the-math",
    "href": "docs/notebooks/model-explore.html#the-math",
    "title": "Exploring model predictions",
    "section": "The math",
    "text": "The math\nWe didn’t need a simulation to tell us - we should have done simple math with the model a long time ago. Assuming no upper limit for simplicity, here is the predicted memory strength for the second tripplet:\n\n\n\nChunk type\nGap\nMemory strength\n\n\n\n\nrandom\nshortgap\n\\(p \\cdot (1-p + r \\cdot t_{short})\\)\n\n\nknown\nshortgap\n\\(p \\cdot (1-p \\cdot p_{ltm} + r \\cdot t_{short})\\)\n\n\nrandom\nlonggap\n\\(p \\cdot (1-p + r \\cdot t_{long})\\)\n\n\nknown\nlonggap\n\\(p \\cdot (1-p \\cdot p_{ltm} + r \\cdot t_{long})\\)\n\n\n\nTherefore the difference between known and random chunks separately for short and long gaps is the same:\n\nshortgap: \\(p^2 \\cdot (1 - p_{ltm})\\)\nlonggap: \\(p^2 \\cdot (1 - p_{ltm})\\)\n\nbecause the terms \\(r \\cdot t_{short}\\) and \\(r \\cdot t_{long}\\) cancel out if resources have not fully recovered. This is why the model predicts only additive effects of chunking and free time.\nThe model does predict an interaction if resources recover fully before the second tripplet is presented. But this also means that there will be no primacy effect from the first to the second tripplet, which is not what we observe in the data.\nIn summary, the premise on which our introduction is currently built is not supported by the model.",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Exploring model predictions"
    ]
  },
  {
    "objectID": "docs/notebooks/model-explore.html#an-interactive-shiny-app",
    "href": "docs/notebooks/model-explore.html#an-interactive-shiny-app",
    "title": "Exploring model predictions",
    "section": "An interactive shiny app",
    "text": "An interactive shiny app\nI found it very useful to be able to quickly explore the model’s predictions. I created a shiny app that allows you to explore the model’s predictions for the proactive benefits of chunking and free time. You can use the sliders below to control the parameters of the model and see how the predicted probability of recall changes as a function of the conditions in the experiment.\n\n#| standalone: true\n#| viewerHeight: 700\nlibrary(shiny)\nlibrary(shinylive)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# define functions\npred_prob &lt;- function(\n    setsize, ISI = rep(0.5, setsize), item_in_ltm = rep(TRUE, setsize),\n    prop = 0.2, prop_ltm = 0.5, tau = 0.15, gain = 25, rate = 0.1,\n    r_max = 1, lambda = 1, growth = \"linear\", type = \"response\") {\n  R &lt;- r_max\n  strengths &lt;- vector(\"numeric\", length = setsize)\n  p_recall &lt;- vector(\"numeric\", length = setsize)\n  prop_ltm &lt;- ifelse(item_in_ltm, prop_ltm, 1)\n\n  for (item in 1:setsize) {\n    # strength of the item and recall probability\n    strengths[item] &lt;- (prop * R)^lambda\n\n    # amount of resources consumed by the item\n    r_cost &lt;- strengths[item]^(1 / lambda) * prop_ltm[item]\n    R &lt;- R - r_cost\n\n    # recover resources\n    R &lt;- switch(growth,\n      \"linear\" = min(r_max, R + rate * ISI[item]),\n      \"asy\" = R + (r_max - R) * (1 - exp(-rate * ISI[item]))\n    )\n  }\n\n  if (type == \"response\") {\n    1 / (1 + exp(-(strengths - tau) * gain))\n  } else {\n    strengths\n  }\n}\n\npredict_resmodel &lt;- function(object, data, group_by, type = \"response\", ...) {\n  if (missing(group_by)) {\n    pred &lt;- pred_prob(\n        setsize = nrow(data),\n        ISI = data$ISI,\n        item_in_ltm = data$item_in_ltm,\n        prop = object[\"prop\"],\n        prop_ltm = object[\"prop_ltm\"],\n        tau = object[\"tau\"],\n        gain = object[\"gain\"],\n        rate = object[\"rate\"],\n        type = type,\n        ...\n      )\n    return(pred)\n  }\n\n  by &lt;- do.call(paste, c(data[, group_by], sep = \"_\"))\n  out &lt;- lapply(split(data, by), function(x) {\n    x$pred_tmp_col295 &lt;- predict_resmodel(object, x, type = type, ...)\n    x\n  })\n  out &lt;- do.call(rbind, out)\n  out &lt;- suppressMessages(dplyr::left_join(data, out))\n  out$pred_tmp_col295\n}\n\ndata &lt;- expand.grid(chunk = c(\"known\", \"random\"),\n                    gap = seq(500, 6000, by = 225),\n                    itemtype = c(\"SP1-3\", \"SP4-6\", \"SP7-9\"))\n\ndata$ISI &lt;- ifelse(data$itemtype == \"SP1-3\", data$gap/1000, 0.5)\ndata$item_in_ltm &lt;- ifelse(data$itemtype == \"SP1-3\", data$chunk == \"known\", FALSE)                     \nshinyApp(\n  ui = fluidPage(\n    titlePanel(\"Interactive Plot\"),\n    sidebarLayout(\n      sidebarPanel(\n        sliderInput(\"prop\", \"prop:\", min = 0, max = 1, value = 0.2),\n        sliderInput(\"prop_ltm\", \"prop_ltm:\", min = 0, max = 1, value = 0.55),\n        sliderInput(\"rate\", \"rate:\", min = 0, max = 1, value = 0.02),\n        sliderInput(\"gain\", \"gain:\", min = 1, max = 100, value = 25),\n        sliderInput(\"tau\", \"tau:\", min = 0, max = 1, value = 0.14),\n      ),\n      mainPanel(\n        plotOutput(\"distPlot\")\n      )\n    )\n  ),\n  server = function(input, output) {\n    output$distPlot &lt;- renderPlot({\n      par &lt;- c(prop = input$prop, prop_ltm = input$prop_ltm, rate = input$rate, gain = input$gain, tau = input$tau)\n      data |&gt;\n        # TODO: can I reuse the computation?\n        mutate(\n          Probability = predict_resmodel(par, data = data, group_by = c(\"gap\", \"chunk\")),\n          Strength = predict_resmodel(par, data = data, group_by = c(\"gap\", \"chunk\"), type = \"strength\")\n        ) |&gt;\n        pivot_longer(c(Probability, Strength), names_to = \"type\", values_to = \"value\") |&gt;\n        ggplot(aes(gap, value, color = chunk, group = chunk)) +\n        geom_line() +\n        scale_color_discrete(\"1st chunk LTM?\") +\n        facet_grid(type~itemtype, scales = \"free\") +\n        theme_classic(base_size = 14)\n    })\n  }\n)",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Exploring model predictions"
    ]
  },
  {
    "objectID": "docs/notebooks/model-explore.html#why-the-best-fitting-model-has-a-very-low-recovery",
    "href": "docs/notebooks/model-explore.html#why-the-best-fitting-model-has-a-very-low-recovery",
    "title": "Exploring model predictions",
    "section": "Why the best fitting model has a very low recovery?",
    "text": "Why the best fitting model has a very low recovery?\nBoth in this experiment and in Mizrak & Oberauer (2022), the proactive benefit of time is global - it affects all subsequent items. With a couple of more simulations, we can see that the degree of local vs global benefit depends on the the prop depletion parameter.\n\nWith very high prop = 1, the second tripplet will depelete all remaining resources, and then resources will be equivalent for all subsequent items. This is the most local benefit.\n\n\nCode\nparams &lt;- c(prop = 1, prop_ltm = 0.7, tau = 0.15, gain = 5, rate = 0.1)\nplot_predictions(params, exp3_data_agg) + coord_cartesian(ylim = c(0, 1))\n\n\n\n\n\n\n\n\n\n\nWith very low prop = 0.1, the second tripplet will depelete only 10% of the remaining resources, and the proactive benefit will propagate to all subsequent items. This is the most global benefit.\n\n\nCode\nparams &lt;- c(prop = 0.1, prop_ltm = 0.5, tau = 0.08, gain = 80, rate = 0.007)\nplot_predictions(params, exp3_data_agg) \n\n\n\n\n\n\n\n\n\n\nAnd with a middle range prop = 0.5, the second tripplet will depelete 50% of the remaining resources, and preserving some of the proactive benefit, but reducing it for subsequent items:\n\n\nCode\nparams &lt;- c(prop = 0.6, prop_ltm = 0.5, tau = 0.3, gain = 10, rate = 0.05)\nplot_predictions(params, exp3_data_agg)\n\n\n\n\n\n\n\n\n\n\nSo what?\nsince we see mostly a global benefit in the data, in order for the model to fit well, it estimates a very low depletion rate (~ 0.2). But then to account for the primacy effect, and to prevent full recovery of resources between items with such low depletion, it also needs to estimate a very low recovery rate. Thus, all of our fits having slow recovery rates, are not related to accounting for the small interaction between chunking and free time, but rather to account for the global proactive benefits and primacy.",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Exploring model predictions"
    ]
  },
  {
    "objectID": "docs/notebooks/data_structure.html",
    "href": "docs/notebooks/data_structure.html",
    "title": "View the data structure",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(targets)\ntar_source()\ntar_load(c(exp1_data, exp2_data, exp1_data_agg, exp2_data_agg))\n\n\nThe datafiles for the two experiments are in objects exp1_data and exp2_data. Let’s first look at the structure of the data for experiment 1.\n\n\nCode\nhead(exp1_data)\n\n\n    id trial condition enccons serpos response duration cor  chunk  gap      rt itemtype\n1 7504     0         4       D      1        D  2576.88   1 random 3000 2.57688    SP1-3\n2 7504     0         4       P      2            309.44   0 random 3000 0.30944    SP1-3\n3 7504     0         4       Q      3        D   760.36   0 random 3000 0.76036    SP1-3\n4 7504     0         4       J      4        D  1939.52   0 random 3000 1.93952    SP4-6\n5 7504     0         4       G      5        D  3935.02   0 random 3000 3.93502    SP4-6\n6 7504     0         4       W      6        W  2010.84   1 random 3000 2.01084    SP4-6\n\n\nFrom Eda I know that the condition column is coded like this:\n\n\n\ncondition\nLTM\nISI\n\n\n\n\n1\nchunk\nshort\n\n\n2\nno-chunk\nshort\n\n\n3\nchunk\nlong\n\n\n4\nno-chunk\nlong\n\n\n\nLet’s confirm this:\n\n\nCode\nexp1_data |&gt;\n  select(condition, chunk, gap) |&gt;\n  unique() |&gt;\n  arrange(condition)\n\n\n  condition  chunk  gap\n1         1  known  500\n2         2 random  500\n3         3  known 3000\n4         4 random 3000\n\n\nthere are this many trials per participant:\n\n\nCode\nmax(as.numeric(exp1_data$trial)) + 1\n\n\n[1] 80",
    "crumbs": [
      "Notebooks",
      "Data",
      "View the data structure"
    ]
  },
  {
    "objectID": "docs/notebooks/data_structure.html#overview",
    "href": "docs/notebooks/data_structure.html#overview",
    "title": "View the data structure",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(targets)\ntar_source()\ntar_load(c(exp1_data, exp2_data, exp1_data_agg, exp2_data_agg))\n\n\nThe datafiles for the two experiments are in objects exp1_data and exp2_data. Let’s first look at the structure of the data for experiment 1.\n\n\nCode\nhead(exp1_data)\n\n\n    id trial condition enccons serpos response duration cor  chunk  gap      rt itemtype\n1 7504     0         4       D      1        D  2576.88   1 random 3000 2.57688    SP1-3\n2 7504     0         4       P      2            309.44   0 random 3000 0.30944    SP1-3\n3 7504     0         4       Q      3        D   760.36   0 random 3000 0.76036    SP1-3\n4 7504     0         4       J      4        D  1939.52   0 random 3000 1.93952    SP4-6\n5 7504     0         4       G      5        D  3935.02   0 random 3000 3.93502    SP4-6\n6 7504     0         4       W      6        W  2010.84   1 random 3000 2.01084    SP4-6\n\n\nFrom Eda I know that the condition column is coded like this:\n\n\n\ncondition\nLTM\nISI\n\n\n\n\n1\nchunk\nshort\n\n\n2\nno-chunk\nshort\n\n\n3\nchunk\nlong\n\n\n4\nno-chunk\nlong\n\n\n\nLet’s confirm this:\n\n\nCode\nexp1_data |&gt;\n  select(condition, chunk, gap) |&gt;\n  unique() |&gt;\n  arrange(condition)\n\n\n  condition  chunk  gap\n1         1  known  500\n2         2 random  500\n3         3  known 3000\n4         4 random 3000\n\n\nthere are this many trials per participant:\n\n\nCode\nmax(as.numeric(exp1_data$trial)) + 1\n\n\n[1] 80",
    "crumbs": [
      "Notebooks",
      "Data",
      "View the data structure"
    ]
  },
  {
    "objectID": "docs/notebooks/data_structure.html#aggregate-data",
    "href": "docs/notebooks/data_structure.html#aggregate-data",
    "title": "View the data structure",
    "section": "Aggregate data",
    "text": "Aggregate data\nThis is what the aggregated data looks like:\n\n\nCode\nexp1_data_agg\n\n\n# A tibble: 12 × 8\n   chunk    gap itemtype n_total n_correct p_correct   ISI item_in_ltm\n   &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;      \n 1 known    500 SP1-3       1855      1715     0.925   0.5 TRUE       \n 2 known    500 SP4-6       1858      1368     0.736   0.5 FALSE      \n 3 known    500 SP7-9       1851       943     0.509   0.5 FALSE      \n 4 known   3000 SP1-3       1859      1721     0.926   3   TRUE       \n 5 known   3000 SP4-6       1858      1443     0.777   0.5 FALSE      \n 6 known   3000 SP7-9       1856      1032     0.556   0.5 FALSE      \n 7 random   500 SP1-3       1853      1495     0.807   0.5 FALSE      \n 8 random   500 SP4-6       1856      1177     0.634   0.5 FALSE      \n 9 random   500 SP7-9       1855       727     0.392   0.5 FALSE      \n10 random  3000 SP1-3       1856      1553     0.837   3   FALSE      \n11 random  3000 SP4-6       1858      1287     0.693   0.5 FALSE      \n12 random  3000 SP7-9       1856       838     0.452   0.5 FALSE",
    "crumbs": [
      "Notebooks",
      "Data",
      "View the data structure"
    ]
  },
  {
    "objectID": "docs/notebooks/linear-recovery-random-variable.html",
    "href": "docs/notebooks/linear-recovery-random-variable.html",
    "title": "Linear recovery as a random variable",
    "section": "",
    "text": "In the simulations we have considered two different equations for the recovery of resources over time:\n\n\nResources recover linearly over time at a constant rate r until they reach a maximum value of 1:\n\\[\nR_{new} = \\min(1, R + r \\cdot t)\n\\]\nwhere \\(R\\) is the resource level before recovery, \\(t\\) is the recovery duration, and \\(r\\) is the recovery rate.\n\n\n\nAlternatively, resources could recover non-linearly over time at a rate that depends on the current resource level:\n\\[\nR_{new} = R + (1 - R) \\cdot (1 - e^{-r t}) = 1 - (1 - R)e^{-r t}\n\\]\nwhich can be simplified in the case of recovery starting from 0 to\n\\[\nR(t) = 1 - e^{-r t}\n\\]\nKlaus and I discussed the possibility that the recovery is linear, but the rate of recovery r is a random variable that varies over trials and individuals. Similarly to the evidence accumulation rate in the linear balistic accumulator models, at the aggregate level the recovery rate might be better modeled with the exponential recovery equation, even if the individual recovery is linear. In this notebook, I explore this idea further and derive analytical solutions for the average recovery over time under different assumptions about the distribution of r.",
    "crumbs": [
      "Notebooks",
      "Model 3: Non-linear recovery",
      "Linear recovery as a random variable"
    ]
  },
  {
    "objectID": "docs/notebooks/linear-recovery-random-variable.html#overview",
    "href": "docs/notebooks/linear-recovery-random-variable.html#overview",
    "title": "Linear recovery as a random variable",
    "section": "",
    "text": "In the simulations we have considered two different equations for the recovery of resources over time:\n\n\nResources recover linearly over time at a constant rate r until they reach a maximum value of 1:\n\\[\nR_{new} = \\min(1, R + r \\cdot t)\n\\]\nwhere \\(R\\) is the resource level before recovery, \\(t\\) is the recovery duration, and \\(r\\) is the recovery rate.\n\n\n\nAlternatively, resources could recover non-linearly over time at a rate that depends on the current resource level:\n\\[\nR_{new} = R + (1 - R) \\cdot (1 - e^{-r t}) = 1 - (1 - R)e^{-r t}\n\\]\nwhich can be simplified in the case of recovery starting from 0 to\n\\[\nR(t) = 1 - e^{-r t}\n\\]\nKlaus and I discussed the possibility that the recovery is linear, but the rate of recovery r is a random variable that varies over trials and individuals. Similarly to the evidence accumulation rate in the linear balistic accumulator models, at the aggregate level the recovery rate might be better modeled with the exponential recovery equation, even if the individual recovery is linear. In this notebook, I explore this idea further and derive analytical solutions for the average recovery over time under different assumptions about the distribution of r.",
    "crumbs": [
      "Notebooks",
      "Model 3: Non-linear recovery",
      "Linear recovery as a random variable"
    ]
  },
  {
    "objectID": "docs/notebooks/linear-recovery-random-variable.html#linear-recovery-with-a-random-variable-rate",
    "href": "docs/notebooks/linear-recovery-random-variable.html#linear-recovery-with-a-random-variable-rate",
    "title": "Linear recovery as a random variable",
    "section": "Linear recovery with a random variable rate",
    "text": "Linear recovery with a random variable rate\nAssume that resource recover linearly over time at a constant rate r until they reach a maximum value of 1. Assume that r is a random variable. Let’s plot the average resource recovery over time under different assumptions about the distribution of r.\n\n\nCode\npar(mar = c(4, 0.5, 2, 1), lwd = 2)\ncurve(\n  dunif(x, 0, 1),\n  from = 0, to = 4, n = 1001, col = \"black\",\n  xlab = \"r\", ylab = NA, main = \"Distribution of r\",\n  ylim = c(0, 1.5), yaxt = \"n\",\n)\ncurve(\n  truncnorm::dtruncnorm(x, a = 0, mean = 0.5, sd = 0.3),\n  from = 0, to = 4, n = 1001, col = \"red\", add = TRUE\n)\ncurve(\n  dgamma(x, shape = 1, scale = 1),\n  from = 0, to = 4, n = 1001, col = \"blue\", add = TRUE\n)\ncurve(\n  dgamma(x, shape = 3, rate = 5),\n  from = 0, to = 4, n = 1001, col = \"orange\", add = TRUE\n)\n# add legend\nlegend(\"topright\",\n  legend = c(\"Uniform(0, 1)\", \"Normal(0.5, 0.3)\", \"Exp(1)\", \"Gamma(3, 5)\"),\n  col = c(\"black\", \"red\", \"blue\", \"orange\"), lty = 1\n)\n\n\n\n\n\n\n\n\n\n\nr is uniformly distributed between 0 and 1.\nr is distributed as a truncated normal distribution with mean 0.5 and standard deviation 0.3\nr is distributed as an exponential distribution with rate 1\nr is distributed as a gamma distribution with shape 3 and rate 5\n\nRed line shows the average recovery over time. Black lines show individual trajectories of a few random samples of r. The full distributions of r are shown in the margin.\n\n\nCode\nset.seed(3)\nn &lt;- 1000\nr_unif &lt;- runif(n, 0, 1)\nr_truncnorm &lt;- truncnorm::rtruncnorm(n, a = 0, mean = 0.5, sd = 0.3)\nr_exp &lt;- rgamma(n, shape = 1, scale = 1)\nr_gamma &lt;- rgamma(n, shape = 3, rate = 5)\nt &lt;- seq(0, 5, length.out = 100)\n\nplot_linear_rv_recovery(r_unif, t, title = \"r ~ Uniform(0, 1)\") + \n  plot_linear_rv_recovery(r_truncnorm, t, title = \"r ~ Normal(0.5, 0.3), r &gt;= 0\") +\n  plot_linear_rv_recovery(r_exp, t, \"r ~ Exp(1)\") + \n  plot_linear_rv_recovery(r_gamma, t, \"r ~ Gamma(3, 5)\")",
    "crumbs": [
      "Notebooks",
      "Model 3: Non-linear recovery",
      "Linear recovery as a random variable"
    ]
  },
  {
    "objectID": "docs/notebooks/linear-recovery-random-variable.html#analytical-solutions-for-the-average-recovery-over-time",
    "href": "docs/notebooks/linear-recovery-random-variable.html#analytical-solutions-for-the-average-recovery-over-time",
    "title": "Linear recovery as a random variable",
    "section": "Analytical solutions for the average recovery over time",
    "text": "Analytical solutions for the average recovery over time\nFor simplicity, consider the case of recovery starting from 0. Given a continuous distribution of r, \\(f(r)\\), we can derive the average recovery over time by integrating two separate quantites over all possible values of r. We have to consider two cases:\n\nResources have not yet reached their maximum value of 1 and are recovering linearly at a rate r. This is the case for \\(r &lt; 1/t\\)\nResources have reached their maximum value of 1 and are no longer recovering. This is the case for \\(r \\geq 1/t\\)\n\nThe average recovery after time t is then given by the following combination of integrals:\n\\[\nR(t) = \\int_{0}^{1/t} r \\cdot t \\cdot f(r) \\, dr + \\int_{1/t}^{\\infty} 1 \\cdot f(r) \\, dr  \n\\]\nBelow I give the solution for the exponential and gamma distributions of r.\n\nExponential distribution\nLet’s consider the case where r is distributed as an exponential distribution with rate \\(\\lambda\\). The probability density function of the exponential distribution is given by\n\\[\nf(r) = \\lambda e^{-\\lambda r}\n\\]\nThe average recovery over time is then given by\n\\[\\begin{aligned}\nR(t) &= \\int_{0}^{1/t} r \\cdot t \\cdot \\lambda e^{-\\lambda r} \\, dr + \\int_{1/t}^{\\infty} 1 \\cdot \\lambda e^{-\\lambda r} \\, dr \\\\\n&= \\left[ -\\frac{t e^{\\lambda  (-r)} (\\lambda  r+1)}{\\lambda } \\right]_{0}^{1/t} + \\left[ -e^{-\\lambda r} \\right]_{1/t}^{\\infty} \\\\\n&= \\frac{t-e^{-\\lambda / t} (\\lambda +t)}{\\lambda } + e^{-\\lambda / t} \\\\\n&= \\frac{t}{\\lambda} (1 - e^{-\\lambda/t})\n\\end{aligned}\\]\n\n\nGamma distribution\nThe probability density function of the gamma distribution is given by\n\\[\nf(r) = \\frac{\\lambda^{\\alpha} r^{\\alpha - 1} e^{-\\lambda r}}{\\Gamma(\\alpha)}\n\\]\nwhere \\(\\alpha\\) is the shape parameter and \\(\\lambda\\) is the rate parameter. The average recovery over time is then given by\n\\[\n\\begin{aligned}\nR(t) &= \\int_{0}^{1/t} r \\cdot t \\cdot \\frac{\\lambda^{\\alpha} r^{\\alpha - 1} e^{-\\lambda r}}{\\Gamma(\\alpha)} \\, dr + \\int_{1/t}^{\\infty} 1 \\cdot \\frac{\\lambda^{\\alpha} r^{\\alpha - 1} e^{-\\lambda r}}{\\Gamma(\\alpha)} \\, dr \\\\\n&= \\frac{\\lambda^{\\alpha}}{\\Gamma(\\alpha)} \\left( t \\int_{0}^{1/t} r^{\\alpha} e^{-\\lambda r} \\, dr + \\int_{1/t}^{\\infty} r^{\\alpha - 1} e^{-\\lambda r} \\, dr \\right) \\\\\n\\end{aligned}\n\\]\nwhich after some transformations can be written as\n\\[\n\\begin{aligned}\nR(t) &= \\frac{\\Gamma \\left(\\alpha ,\\frac{\\lambda }{t}\\right)}{\\Gamma (\\alpha )}+\\frac{\\alpha t}{\\lambda}\\frac{\\gamma \\left(\\alpha +1,\\frac{\\lambda }{t}\\right)}{\\Gamma (\\alpha +1)} \\\\\n&= P(\\alpha, \\lambda / t) + \\frac{\\alpha t}{\\lambda} Q(\\alpha + 1, \\lambda / t)\n\\end{aligned}\n\\]\nwhere \\(\\Gamma(a, z)\\) is the upper incomplete gamma function, \\(\\gamma(a, z)\\) is the lower incomplete gamma function; and \\(P(a, z)\\) and \\(Q(a, z)\\) are the regularized upper and lower incomplete gamma functions, respectively. P and Q are respectively the complimentary cdf and the cdf of the gamma distribution. They can be computed using the pgamma function in R with the lower.tail = FALSE argument for the upper incomplete gamma function and the pgamma function with the lower.tail = TRUE argument for the lower incomplete gamma function.",
    "crumbs": [
      "Notebooks",
      "Model 3: Non-linear recovery",
      "Linear recovery as a random variable"
    ]
  },
  {
    "objectID": "docs/notebooks/linear-recovery-random-variable.html#compare-with-exponential-recovery",
    "href": "docs/notebooks/linear-recovery-random-variable.html#compare-with-exponential-recovery",
    "title": "Linear recovery as a random variable",
    "section": "Compare with exponential recovery",
    "text": "Compare with exponential recovery\nThe equations derived above under the assumption of an exponential or gamma distribution of r are obviously not the same as the exponential recovery equation. But is the exponential recovery equation a good approximation of the average recovery over time when r is a random variable following an exponential or gamma distribution?\nIn this part, I compare the recovery given by the exponential recovery equation with the recovery given by the marginalized linear recovery with r following an exponential distribution or gamma distribution. I will use the analytical solutions derived above to fit the parameters of the exponential and gamma distributions to the exponential recovery equation.\nLet’s define our functions:\n\nexp_recovery &lt;- function(r, t) {\n  1 - exp(-r * t)\n}\n\nmarginal_recovery_exponential &lt;- function(lambda, t) {\n  t/lambda * (1 - exp(-lambda/t))\n}\n\nmarginal_recovery_gamma &lt;- function(alpha, lambda, t) {\n  P &lt;- pgamma(lambda / t, shape = alpha, lower.tail = FALSE) \n  R &lt;- pgamma(lambda / t, shape = alpha + 1)\n  P + alpha * t / lambda * R\n}\n\n\nMarginal recovery from an exponential distribution\nSet the true recovery rate to 0.3 and fit the rate parameter of the exponential distribution to the exponential recovery equation.\n\n\nCode\nobjective_fn &lt;- function(log_lambda, t, target) {\n  pred &lt;- marginal_recovery_exponential(exp(log_lambda), t)\n  sqrt(mean((pred - target)^2))\n}\n\nset.seed(2)\ntrue_r &lt;- 0.3\nt &lt;- seq(0, 60, length.out = 1000)\nfit &lt;- optim(1, objective_fn, t = t, target = exp_recovery(true_r, t))\nlambda &lt;- exp(fit$par)\n\ncurve(\n  marginal_recovery_exponential(lambda, t),\n  from = 0, to = 20, n = 1001,\n  col = \"blue\", ylim = c(0, 1), xname = \"t\", ylab = \"R(t)\"\n)\ncurve(\n  exp_recovery(true_r, t),\n  col = \"red\", add = TRUE, xname = \"t\"\n)\nlegend(\"bottomright\", legend = c(\"Marginal linear recovery\\n(rate is an exponential random variable)\", \"Exponential recovery\"), col = c(\"blue\", \"red\"), lty = 1)\n\n\n\n\n\n\n\n\n\nThe overall shape is similar, but the approximation is not great.\n\n\nMarginal recovery from a gamma distribution\nThe approximation when the rate follows a gamma distribution is much better::\n\n\nCode\nobjective_fn &lt;- function(pars, t, target) {\n  pred &lt;- marginal_recovery_gamma(exp(pars[1]), exp(pars[2]), t)\n  sqrt(mean((pred - target)^2))\n}\n\nset.seed(2)\ntrue_r &lt;- 0.30\nt &lt;- seq(0, 200, length.out = 1000)\nfit &lt;- optim(c(0, 0), objective_fn, t = t, target = exp_recovery(true_r, t))\npars &lt;- exp(fit$par)\n\ncurve(\n  marginal_recovery_gamma(pars[1], pars[2], t),\n  from = 0, to = 1 / true_r * 5, n = 1001,\n  col = \"blue\", ylim = c(0, 1), xname = \"t\", ylab = \"R(t)\"\n)\ncurve(\n  exp_recovery(true_r, t),\n  col = \"red\", add = TRUE, xname = \"t\"\n)\nlegend(\"bottomright\", legend = c(\"Marginal linear recovery\\n(rate is a gamma random variable)\", \"Exponential recovery\"), col = c(\"blue\", \"red\"), lty = 1)\n\n\n\n\n\n\n\n\n\nwith an RMSE of 0.005. The fit is basically the same for all values of r.\n\n\n\n\n\n\nSummary\n\n\n\nThe exponential recovery equation is a good approximation of the average recovery over time when the rate of recovery is a random variable following a gamma distribution. The approximation is not as good when the rate of recovery is a random variable following an exponential distribution.",
    "crumbs": [
      "Notebooks",
      "Model 3: Non-linear recovery",
      "Linear recovery as a random variable"
    ]
  },
  {
    "objectID": "docs/notebooks/linear-recovery-random-variable.html#conversion-of-parameters",
    "href": "docs/notebooks/linear-recovery-random-variable.html#conversion-of-parameters",
    "title": "Linear recovery as a random variable",
    "section": "Conversion of parameters",
    "text": "Conversion of parameters\n\n\n\n\n\n\nOptional\n\n\n\nThe section below is not directly relevant to our project, but I found the results quite fascinating. Feel free to skip it if you are not interested in the relationship between the parameters of the gamma distribution and the true recovery rate.\n\n\nWhen the rate of recovery is a random variable following a gamma distribution, this is approximated well by the exponential recovery equation. The gamma distribution has two parameters: the shape parameter \\(\\alpha\\) and the rate parameter \\(\\lambda\\), while the exponential recovery equation has only one parameter: the rate of recovery r. How do the parameters of the gamma distribution relate to the recovery rate in exponential recovery equation?\nI will simulate the recovery over time for different values of the true recovery rate r in the range [0.1, 5]. Then I will fit to this the marginal recovery predicted by a linear recovery process where the rate follows a gamma distribution. I will then explore the relationship between the parameters of the gamma distribution and the true recovery rate.\n\n\nCode\nt &lt;- seq(0, 200, length.out = 10000)\nget_pars &lt;- function(true_r, t) {\n  objective_fn &lt;- function(pars, t, target) {\n    pred &lt;- marginal_recovery_gamma(exp(pars[1]), exp(pars[2]), t)\n    sqrt(mean((pred - target)^2))\n  }\n\n  fit &lt;- optim(c(0, 0), objective_fn, t = t, target = exp_recovery(true_r, t))\n\n  pars &lt;- exp(fit$par)\n  pars\n}\n\ntrue_r &lt;- seq(0.1, 5, length.out = 100)\npars &lt;- run_or_load(sapply(true_r, get_pars, t = t), \"output/gamma_recovery_pars.rds\")\n\npar(mfrow = c(1, 2))\nplot(true_r, pars[1, ], type = \"l\", col = \"blue\", xlab = \"Exponential recovery rate (r)\", ylab = \"alpha\")\nplot(true_r, pars[2, ], type = \"l\", col = \"red\", xlab = \"Exponential recovery rate (r)\", ylab = \"lambda\")\n\n\n\n\n\n\n\n\n\nI did not expect this. The shape parameter \\(\\alpha\\) is fixed at 2.7937 and only the rate \\(\\lambda\\) changes. Turns out that lambda is linearly related to 1/r (red line is the “data” and blue line is the fitted line from a linear regression):\n\n\nCode\npar(mfrow = c(1, 1), lwd = 3)\nplot(1 / true_r, pars[2, ], type = \"l\", col = \"red\", xlab = \"1 / r\", ylab = \"lambda\")\nlines(pars[2, ] ~ I(1 / true_r), col = \"blue\", lty = 4)\n\n\n\n\n\n\n\n\n\nwith the following relationship:\n\\[\n\\lambda \\approxeq 3.831 \\,r^{-1}\n\\]",
    "crumbs": [
      "Notebooks",
      "Model 3: Non-linear recovery",
      "Linear recovery as a random variable"
    ]
  },
  {
    "objectID": "docs/notebooks/linear-recovery-random-variable.html#summary-1",
    "href": "docs/notebooks/linear-recovery-random-variable.html#summary-1",
    "title": "Linear recovery as a random variable",
    "section": "Summary",
    "text": "Summary\nAn exponential recovery processes with rate \\(r\\) can be approximated well by a linear recovery process where the rate of recovery is a random variable which follows a gamma distribution with shape parameter \\(\\alpha =\\) 2.794 and rate parameter \\(\\lambda = 3.831 \\,r^{-1}\\). That is, modeling resource recovery with the following two equations is almost equivalent:\n\\[\nR(t) = 1 - e^{-r t}\n\\]\nand\n\\[\nR(t) = min(1, r_{linear}t) \\\\\n\\]\n\\[\nr_{linear} \\sim \\text{Gamma}(2.794, 3.831 \\cdot r^{-1})\n\\]",
    "crumbs": [
      "Notebooks",
      "Model 3: Non-linear recovery",
      "Linear recovery as a random variable"
    ]
  },
  {
    "objectID": "docs/notebooks/model_v2.html",
    "href": "docs/notebooks/model_v2.html",
    "title": "Main results",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(targets)\nlibrary(GGally)\nlibrary(kableExtra)\n# load \"R/*\" scripts and saved R objects from the targets pi\ntar_source()\ntar_load(c(exp1_data_agg_enc, exp2_data_agg_enc, fits3))\n\n\nHere I employ the same model as in Model 1 with one difference. In the previous model, the encoding time was not included. It assumed that during encoding of the triplets (0.9 seconds), nothing happened, and recovery of resources only started after the encoding was completed. In Popov & Reder (2020), we assumed that depletion happens instantaneously, and recovery continuous thoughout the encoding and inter-stimulus interval.\nSo here we just add the 0.9 seconds of encoding time to the ISI. For example, this is coded in the ISI column:\n\n\nCode\nexp1_data_agg_enc |&gt;\n  kbl() |&gt;\n  kable_styling()\n\n\n\n\n\nchunk\ngap\nitemtype\nn_total\nn_correct\np_correct\nISI\nitem_in_ltm\n\n\n\n\nknown\n500\nSP1-3\n1855\n1715\n0.9245283\n1.4\nTRUE\n\n\nknown\n500\nSP4-6\n1858\n1368\n0.7362756\n1.4\nFALSE\n\n\nknown\n500\nSP7-9\n1851\n943\n0.5094543\n1.4\nFALSE\n\n\nknown\n3000\nSP1-3\n1859\n1721\n0.9257665\n3.9\nTRUE\n\n\nknown\n3000\nSP4-6\n1858\n1443\n0.7766416\n1.4\nFALSE\n\n\nknown\n3000\nSP7-9\n1856\n1032\n0.5560345\n1.4\nFALSE\n\n\nrandom\n500\nSP1-3\n1853\n1495\n0.8067998\n1.4\nFALSE\n\n\nrandom\n500\nSP4-6\n1856\n1177\n0.6341595\n1.4\nFALSE\n\n\nrandom\n500\nSP7-9\n1855\n727\n0.3919137\n1.4\nFALSE\n\n\nrandom\n3000\nSP1-3\n1856\n1553\n0.8367457\n3.9\nFALSE\n\n\nrandom\n3000\nSP4-6\n1858\n1287\n0.6926803\n1.4\nFALSE\n\n\nrandom\n3000\nSP7-9\n1856\n838\n0.4515086\n1.4\nFALSE\n\n\n\n\n\n\n\n\n\nCode\n# calculate deviance and predictions\nfits3 &lt;- fits3 |&gt;\n  mutate(\n    deviance = pmap_dbl(\n      list(fit, data, exclude_sp1),\n      ~ overall_deviance(params = `..1`$par, data = `..2`, exclude_sp1 = `..3`)\n    ),\n    pred = map2(fit, data, ~ predict(.x, .y, group_by = c(\"chunk\", \"gap\")))\n  )\n\n\n(just like in Model 1, I fit the data by either excluding the first serial position or not, and including priors on the gain and rate parameters or not).",
    "crumbs": [
      "Notebooks",
      "Model 2: Include encoding time",
      "Main results"
    ]
  },
  {
    "objectID": "docs/notebooks/model_v2.html#overview",
    "href": "docs/notebooks/model_v2.html#overview",
    "title": "Main results",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(targets)\nlibrary(GGally)\nlibrary(kableExtra)\n# load \"R/*\" scripts and saved R objects from the targets pi\ntar_source()\ntar_load(c(exp1_data_agg_enc, exp2_data_agg_enc, fits3))\n\n\nHere I employ the same model as in Model 1 with one difference. In the previous model, the encoding time was not included. It assumed that during encoding of the triplets (0.9 seconds), nothing happened, and recovery of resources only started after the encoding was completed. In Popov & Reder (2020), we assumed that depletion happens instantaneously, and recovery continuous thoughout the encoding and inter-stimulus interval.\nSo here we just add the 0.9 seconds of encoding time to the ISI. For example, this is coded in the ISI column:\n\n\nCode\nexp1_data_agg_enc |&gt;\n  kbl() |&gt;\n  kable_styling()\n\n\n\n\n\nchunk\ngap\nitemtype\nn_total\nn_correct\np_correct\nISI\nitem_in_ltm\n\n\n\n\nknown\n500\nSP1-3\n1855\n1715\n0.9245283\n1.4\nTRUE\n\n\nknown\n500\nSP4-6\n1858\n1368\n0.7362756\n1.4\nFALSE\n\n\nknown\n500\nSP7-9\n1851\n943\n0.5094543\n1.4\nFALSE\n\n\nknown\n3000\nSP1-3\n1859\n1721\n0.9257665\n3.9\nTRUE\n\n\nknown\n3000\nSP4-6\n1858\n1443\n0.7766416\n1.4\nFALSE\n\n\nknown\n3000\nSP7-9\n1856\n1032\n0.5560345\n1.4\nFALSE\n\n\nrandom\n500\nSP1-3\n1853\n1495\n0.8067998\n1.4\nFALSE\n\n\nrandom\n500\nSP4-6\n1856\n1177\n0.6341595\n1.4\nFALSE\n\n\nrandom\n500\nSP7-9\n1855\n727\n0.3919137\n1.4\nFALSE\n\n\nrandom\n3000\nSP1-3\n1856\n1553\n0.8367457\n3.9\nFALSE\n\n\nrandom\n3000\nSP4-6\n1858\n1287\n0.6926803\n1.4\nFALSE\n\n\nrandom\n3000\nSP7-9\n1856\n838\n0.4515086\n1.4\nFALSE\n\n\n\n\n\n\n\n\n\nCode\n# calculate deviance and predictions\nfits3 &lt;- fits3 |&gt;\n  mutate(\n    deviance = pmap_dbl(\n      list(fit, data, exclude_sp1),\n      ~ overall_deviance(params = `..1`$par, data = `..2`, exclude_sp1 = `..3`)\n    ),\n    pred = map2(fit, data, ~ predict(.x, .y, group_by = c(\"chunk\", \"gap\")))\n  )\n\n\n(just like in Model 1, I fit the data by either excluding the first serial position or not, and including priors on the gain and rate parameters or not).",
    "crumbs": [
      "Notebooks",
      "Model 2: Include encoding time",
      "Main results"
    ]
  },
  {
    "objectID": "docs/notebooks/model_v2.html#overall-best-parameters-by-experiment-and-scenarious",
    "href": "docs/notebooks/model_v2.html#overall-best-parameters-by-experiment-and-scenarious",
    "title": "Main results",
    "section": "Overall best parameters by experiment and scenarious",
    "text": "Overall best parameters by experiment and scenarious\nThese are the best fitting parameters for each experiment, prior scenario, and whether the first serial position was excluded or not:\n\n\nCode\nfinal3 &lt;- fits3 |&gt;\n  filter(convergence == 0) |&gt;\n  group_by(exp, priors_scenario, exclude_sp1) |&gt;\n  arrange(deviance) |&gt;\n  slice(1) |&gt;\n  arrange(desc(exclude_sp1), exp, priors_scenario) |&gt;\n  mutate(\n    deviance = round(deviance, 1),\n    priors_scenario = case_when(\n      priors_scenario == \"none\" ~ \"None\",\n      priors_scenario == \"gain\" ~ \"Gain ~ N(25, 0.1)\",\n      priors_scenario == \"rate\" ~ \"Rate ~ N(0.1, 0.01)\"\n    )\n  )\n\nfinal3 |&gt;\n  select(exp, priors_scenario, exclude_sp1, prop:gain, deviance) |&gt;\n  mutate_all(round, 3) |&gt;\n  kbl() |&gt;\n  kable_styling()\n\n\n\n\n\nexp\npriors_scenario\nexclude_sp1\nprop\nprop_ltm\nrate\ntau\ngain\ndeviance\n\n\n\n\n1\nGain ~ N(25, 0.1)\nTRUE\n0.231\n0.617\n0.018\n0.162\n25.001\n33.1\n\n\n1\nNone\nTRUE\n0.110\n0.606\n0.009\n0.094\n99.996\n31.9\n\n\n1\nRate ~ N(0.1, 0.01)\nTRUE\n0.435\n0.646\n0.043\n0.222\n8.364\n42.4\n\n\n2\nGain ~ N(25, 0.1)\nTRUE\n0.211\n0.834\n0.013\n0.153\n25.000\n40.7\n\n\n2\nNone\nTRUE\n0.106\n0.826\n0.006\n0.091\n91.036\n40.5\n\n\n2\nRate ~ N(0.1, 0.01)\nTRUE\n0.473\n0.855\n0.030\n0.212\n6.286\n46.5\n\n\n1\nGain ~ N(25, 0.1)\nFALSE\n0.239\n0.660\n0.016\n0.163\n24.998\n145.4\n\n\n1\nNone\nFALSE\n0.302\n0.658\n0.020\n0.184\n16.186\n144.9\n\n\n1\nRate ~ N(0.1, 0.01)\nFALSE\n0.472\n0.673\n0.044\n0.220\n7.604\n152.2\n\n\n2\nGain ~ N(25, 0.1)\nFALSE\n0.222\n0.879\n0.011\n0.155\n24.998\n113.3\n\n\n2\nNone\nFALSE\n0.379\n0.868\n0.019\n0.198\n9.421\n109.7\n\n\n2\nRate ~ N(0.1, 0.01)\nFALSE\n0.497\n0.865\n0.030\n0.209\n5.950\n113.4\n\n\n\n\n\n\n\nfor comparison, here are the results of Model 1\n\n\n\nModel 1 Results\n\n\nDoesn’t make much of a difference. Do not pursue further.",
    "crumbs": [
      "Notebooks",
      "Model 2: Include encoding time",
      "Main results"
    ]
  },
  {
    "objectID": "docs/notebooks/subject_data.html",
    "href": "docs/notebooks/subject_data.html",
    "title": "Subject-level data",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(targets)\ntar_source()\ntar_load(c(exp1_data, exp2_data))\n\n\nAggregate data at the subject level\n\n\nCode\ndata_agg_subj &lt;- bind_rows(\n  mutate(exp1_data, exp = \"Exp 1\"),\n  mutate(exp2_data, exp = \"Exp 2\")\n) |&gt;\n  group_by(id, exp) |&gt;\n  nest() |&gt;\n  mutate(data = map(data, aggregate_data)) |&gt;\n  unnest(data)\n\n\nPlot data for each subject\n\n\nCode\ndata_agg_subj |&gt;\n  mutate(gap = as.factor(gap)) |&gt;\n  ggplot(aes(gap, p_correct, color = chunk, group = interaction(chunk, id))) +\n  geom_point() +\n  geom_line() +\n  scale_color_discrete(\"1st chunk LTM?\") +\n  facet_grid(id ~ itemtype) +\n  theme_pub()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Notebooks",
      "Data",
      "Subject-level data"
    ]
  },
  {
    "objectID": "docs/notebooks/par_identifiability.html",
    "href": "docs/notebooks/par_identifiability.html",
    "title": "Parameter identifiability",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(targets)\nlibrary(GGally)\nlibrary(kableExtra)\ntar_source()\n\n\nIn the initial modeling notebook, I discovered some problems with parameter identifiability. Here I will explore this issue further.\nInitially I ran the model described in the May 13, 2024 draft with 100 different starting parameters. Here are 5 sets of very different best-fitting parameters that produce nearly identical fits as measured by the negative log-likelihood (deviance):\n\n\nCode\nfits &lt;- readRDS(\"output/five_parsets_exp2.rds\")\nfits$set &lt;- paste0(\"set\", 1:5)\n\nfits[, 1:6] |&gt;\n  as.data.frame() |&gt;\n  `rownames&lt;-`(fits$set) |&gt;\n  kbl() %&gt;%\n  kable_styling()\n\n\n\n \n  \n      \n    prop \n    prop_ltm \n    rate \n    tau \n    gain \n    deviance \n  \n \n\n  \n    set1 \n    0.098 \n    0.815 \n    0.006 \n    0.084 \n    96.954 \n    40.377 \n  \n  \n    set2 \n    0.132 \n    0.818 \n    0.009 \n    0.108 \n    54.078 \n    40.429 \n  \n  \n    set3 \n    0.155 \n    0.820 \n    0.010 \n    0.123 \n    40.135 \n    40.491 \n  \n  \n    set4 \n    0.173 \n    0.822 \n    0.011 \n    0.133 \n    32.738 \n    40.556 \n  \n  \n    set5 \n    0.215 \n    0.826 \n    0.013 \n    0.154 \n    21.967 \n    40.772 \n  \n\n\n\n\n\nWe can see that they all produce nearly identical predictions (the lines are the model predictions, the points are the data):\n\n\nCode\nfits |&gt;\n  mutate(pred = map2(fit, data, \\(x, y) predict(x, y, group_by = c(\"chunk\", \"gap\")))) |&gt;\n  unnest(c(data, pred)) |&gt;\n  mutate(gap = as.factor(gap)) |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = set, group = interaction(chunk, set))) +\n  scale_color_discrete(\"First chunk LTM?\") +\n  scale_linetype_discrete(\"Parameter set\") +\n  facet_grid(~itemtype) +\n  theme_pub()\n\n\n\n\n\n\n\n\n\nThe plot below shows the strong nearly linear trade-offs between the parameters.\n\n\nCode\nfits |&gt;\n  select(prop, rate, tau, gain) |&gt;\n  ggpairs(\n    diag = list(continuous = \"blankDiag\"),\n    upper = list(continuous = \"points\")\n  ) +\n  theme_pub() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Parameter identifiability"
    ]
  },
  {
    "objectID": "docs/notebooks/par_identifiability.html#overview",
    "href": "docs/notebooks/par_identifiability.html#overview",
    "title": "Parameter identifiability",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(targets)\nlibrary(GGally)\nlibrary(kableExtra)\ntar_source()\n\n\nIn the initial modeling notebook, I discovered some problems with parameter identifiability. Here I will explore this issue further.\nInitially I ran the model described in the May 13, 2024 draft with 100 different starting parameters. Here are 5 sets of very different best-fitting parameters that produce nearly identical fits as measured by the negative log-likelihood (deviance):\n\n\nCode\nfits &lt;- readRDS(\"output/five_parsets_exp2.rds\")\nfits$set &lt;- paste0(\"set\", 1:5)\n\nfits[, 1:6] |&gt;\n  as.data.frame() |&gt;\n  `rownames&lt;-`(fits$set) |&gt;\n  kbl() %&gt;%\n  kable_styling()\n\n\n\n \n  \n      \n    prop \n    prop_ltm \n    rate \n    tau \n    gain \n    deviance \n  \n \n\n  \n    set1 \n    0.098 \n    0.815 \n    0.006 \n    0.084 \n    96.954 \n    40.377 \n  \n  \n    set2 \n    0.132 \n    0.818 \n    0.009 \n    0.108 \n    54.078 \n    40.429 \n  \n  \n    set3 \n    0.155 \n    0.820 \n    0.010 \n    0.123 \n    40.135 \n    40.491 \n  \n  \n    set4 \n    0.173 \n    0.822 \n    0.011 \n    0.133 \n    32.738 \n    40.556 \n  \n  \n    set5 \n    0.215 \n    0.826 \n    0.013 \n    0.154 \n    21.967 \n    40.772 \n  \n\n\n\n\n\nWe can see that they all produce nearly identical predictions (the lines are the model predictions, the points are the data):\n\n\nCode\nfits |&gt;\n  mutate(pred = map2(fit, data, \\(x, y) predict(x, y, group_by = c(\"chunk\", \"gap\")))) |&gt;\n  unnest(c(data, pred)) |&gt;\n  mutate(gap = as.factor(gap)) |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = set, group = interaction(chunk, set))) +\n  scale_color_discrete(\"First chunk LTM?\") +\n  scale_linetype_discrete(\"Parameter set\") +\n  facet_grid(~itemtype) +\n  theme_pub()\n\n\n\n\n\n\n\n\n\nThe plot below shows the strong nearly linear trade-offs between the parameters.\n\n\nCode\nfits |&gt;\n  select(prop, rate, tau, gain) |&gt;\n  ggpairs(\n    diag = list(continuous = \"blankDiag\"),\n    upper = list(continuous = \"points\")\n  ) +\n  theme_pub() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Parameter identifiability"
    ]
  },
  {
    "objectID": "docs/notebooks/par_identifiability.html#fix-rate-to-different-values-and-explore-the-parameter-space",
    "href": "docs/notebooks/par_identifiability.html#fix-rate-to-different-values-and-explore-the-parameter-space",
    "title": "Parameter identifiability",
    "section": "Fix rate to different values and explore the parameter space",
    "text": "Fix rate to different values and explore the parameter space\nI ran a simulation in which I fix1 the rate parameter to values in the range 0.005 to 0.2. I want to see if the identical fits extend to higher rates.\nWe first extract the best-fitting parameters for each rate from the 100 random start fits.\n\n\nCode\ntar_load(fits2)\nfits2 &lt;- fits2 |&gt;\n  mutate(\n    deviance = pmap_dbl(list(fit, data, exclude_sp1), function(x, y, z) {\n      overall_deviance(x$par, data = y, exclude_sp1 = z)\n    }),\n    rate = round(rate, 3)\n  )\n\n\nbest &lt;- fits2 |&gt;\n  group_by(rate, exp, exclude_sp1) |&gt;\n  filter(deviance == min(deviance)) |&gt;\n  slice(1) |&gt;\n  select(rate, prop, prop_ltm, tau, gain, deviance, fit, data) |&gt;\n  mutate(pred = map2(fit, data, \\(x, y) predict(x, y, group_by = c(\"chunk\", \"gap\")))) |&gt;\n  ungroup()\n\n\nAdding missing grouping variables: `exp`, `exclude_sp1`\n\n\nsubset for exp2 and exclude_sp1 = TRUE\n\n\nCode\nbest_subset &lt;- filter(best, exp == 2, exclude_sp1)\n\n\nFrom the deviance it’s already clear that the fits get wose once we get above 0.020 rate. Here are the best-fitting parameters for each rate:\n\n\nCode\nbest_subset |&gt;\n  select(rate, prop, prop_ltm, tau, gain, deviance) |&gt;\n  mutate_all(round, 3) |&gt;\n  kbl() %&gt;%\n  kable_styling()\n\n\n\n \n  \n    rate \n    prop \n    prop_ltm \n    tau \n    gain \n    deviance \n  \n \n\n  \n    0.005 \n    0.098 \n    0.812 \n    0.084 \n    100.000 \n    42.689 \n  \n  \n    0.010 \n    0.155 \n    0.819 \n    0.121 \n    41.585 \n    40.518 \n  \n  \n    0.015 \n    0.235 \n    0.826 \n    0.159 \n    19.374 \n    40.871 \n  \n  \n    0.020 \n    0.316 \n    0.835 \n    0.183 \n    11.537 \n    41.687 \n  \n  \n    0.025 \n    0.389 \n    0.841 \n    0.194 \n    8.143 \n    43.185 \n  \n  \n    0.030 \n    0.451 \n    0.844 \n    0.197 \n    6.359 \n    45.610 \n  \n  \n    0.035 \n    0.496 \n    0.842 \n    0.197 \n    5.394 \n    49.069 \n  \n  \n    0.040 \n    0.528 \n    0.834 \n    0.196 \n    4.765 \n    53.434 \n  \n  \n    0.045 \n    0.324 \n    0.465 \n    0.210 \n    8.718 \n    57.907 \n  \n  \n    0.050 \n    0.355 \n    0.459 \n    0.220 \n    7.359 \n    58.648 \n  \n  \n    0.055 \n    0.384 \n    0.455 \n    0.228 \n    6.362 \n    59.535 \n  \n  \n    0.060 \n    0.412 \n    0.448 \n    0.234 \n    5.559 \n    60.563 \n  \n  \n    0.065 \n    0.439 \n    0.442 \n    0.239 \n    4.955 \n    61.729 \n  \n  \n    0.070 \n    0.466 \n    0.438 \n    0.244 \n    4.430 \n    63.027 \n  \n  \n    0.075 \n    0.271 \n    0.452 \n    0.198 \n    12.494 \n    63.309 \n  \n  \n    0.080 \n    0.272 \n    0.457 \n    0.199 \n    12.582 \n    63.309 \n  \n  \n    0.085 \n    0.271 \n    0.464 \n    0.200 \n    12.799 \n    63.309 \n  \n  \n    0.090 \n    0.271 \n    0.470 \n    0.200 \n    12.950 \n    63.309 \n  \n  \n    0.095 \n    0.271 \n    0.476 \n    0.201 \n    13.072 \n    63.309 \n  \n  \n    0.100 \n    0.271 \n    0.481 \n    0.202 \n    13.233 \n    63.309 \n  \n\n\n\n\n\nA few observations:\n\nwith recovery rates higher than 0.07 the fits do no change much\nwhen the rate changes from 0.04 to 0.045, we switch to a different region of the parameter space. Similarly from 0.065 to 0.07\nlooks like we have four ranges of rates with qualitative shifts:\n\n0.005 to 0.02: the fits are very similar and prop_ltm is ~ 0.8. The other parameters change proportionally to the rate\n0.025 to 0.04: fits get progressively worse. the other parameters still changes similarly to the first group, but tau is not stable. Seems like tau can no longer compensate, leading to worsening fits\n0.045 to 0.07: prop_ltm drops to ~0.45 and stays there. The other parameters again change proportionally to the rate, but they are now in a different region of the parameter space. Fits gradually worsen as we increase the rates\n0.07 to 0.2: prop is now stable at ~0.255. The other parameters change but with very small increments. Fits are identical\n\n\nHere’s a pairs plot with colors indicating the rate group. The plot indicates that the parameter space does not have a smooth gradient, but rather jumps between regions when rate is fixed to different values.\n\n\nCode\nbest_subset &lt;- best_subset |&gt;\n  mutate(par_groups = case_when(\n    rate &lt;= 0.02 ~ \"0.005-0.02\",\n    rate &lt;= 0.04 ~ \"0.025-0.04\",\n    rate &lt;= 0.07 ~ \"0.045-0.07\",\n    TRUE ~ \"0.07-0.2\"\n  ))\n\nbest_subset |&gt;\n  select(prop, prop_ltm, rate, tau, gain, par_groups) |&gt;\n  ggpairs(\n    aes(color = par_groups),\n    diag = list(continuous = \"blankDiag\"),\n    upper = list(continuous = \"points\")\n  ) +\n  theme_pub() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nLet’s see how this affects the predictions for the different rate groups. It would be too much to plot all the predictions, so I will first plot the predictions for the first and last parameters for each rate group.\n\n\nCode\nbest_subset |&gt;\n  group_by(par_groups) |&gt;\n  slice(c(1, n())) |&gt;\n  unnest(c(data, pred)) |&gt;\n  mutate(\n    gap = as.factor(gap),\n    rate = as.factor(round(rate, 3))\n  ) |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = par_groups, group = interaction(chunk, rate))) +\n  scale_color_discrete(\"First chunk LTM?\") +\n  scale_linetype_discrete(\"Rate\") +\n  facet_grid(rate ~ itemtype) +\n  theme_pub()\n\n\n\n\n\n\n\n\n\nA few observations:\n\nRates above 0.045 lead to the same fit, because they cause full recovery from first item to the second with 6 s ISI. We can see that from he plots because the prediction for SP1-3 and sp4-6 (6000 ISI) are the same for all rates above 0.045, meaning no serial position effect from the first to the second chunk. This is completely implausible, so we can rule out rates above 0.045.\n\n\nIncluding SP1-3 in the fits\nI will now include SP1-3 in the fits and see if this changes the parameter space.\n\n\nCode\nbest_subset &lt;- filter(best, exp == 2, exclude_sp1 == FALSE)\n\n\nHere are the best-fitting parameters for each rate:\n\n\nCode\nbest_subset |&gt;\n  select(rate, prop, prop_ltm, tau, gain, deviance) |&gt;\n  mutate_all(round, 3) |&gt;\n  kbl() %&gt;%\n  kable_styling()\n\n\n\n \n  \n    rate \n    prop \n    prop_ltm \n    tau \n    gain \n    deviance \n  \n \n\n  \n    0.005 \n    0.109 \n    0.875 \n    0.091 \n    92.939 \n    119.499 \n  \n  \n    0.010 \n    0.216 \n    0.869 \n    0.149 \n    24.963 \n    114.013 \n  \n  \n    0.015 \n    0.303 \n    0.867 \n    0.176 \n    13.406 \n    110.780 \n  \n  \n    0.020 \n    0.374 \n    0.863 \n    0.189 \n    9.213 \n    109.681 \n  \n  \n    0.025 \n    0.432 \n    0.860 \n    0.193 \n    7.153 \n    110.370 \n  \n  \n    0.030 \n    0.481 \n    0.859 \n    0.193 \n    5.961 \n    112.559 \n  \n  \n    0.035 \n    0.527 \n    0.857 \n    0.191 \n    5.106 \n    116.055 \n  \n  \n    0.040 \n    0.567 \n    0.853 \n    0.187 \n    4.514 \n    120.700 \n  \n  \n    0.045 \n    0.602 \n    0.853 \n    0.183 \n    4.074 \n    126.391 \n  \n  \n    0.050 \n    0.640 \n    0.852 \n    0.177 \n    3.679 \n    133.024 \n  \n  \n    0.055 \n    0.667 \n    0.853 \n    0.172 \n    3.429 \n    140.578 \n  \n  \n    0.060 \n    0.697 \n    0.852 \n    0.166 \n    3.181 \n    148.930 \n  \n  \n    0.065 \n    0.723 \n    0.852 \n    0.161 \n    2.992 \n    158.062 \n  \n  \n    0.070 \n    0.749 \n    0.851 \n    0.155 \n    2.811 \n    167.905 \n  \n  \n    0.075 \n    0.773 \n    0.853 \n    0.148 \n    2.660 \n    178.428 \n  \n  \n    0.080 \n    0.793 \n    0.854 \n    0.143 \n    2.537 \n    189.595 \n  \n  \n    0.085 \n    0.816 \n    0.852 \n    0.136 \n    2.405 \n    201.389 \n  \n  \n    0.090 \n    0.836 \n    0.854 \n    0.130 \n    2.300 \n    213.730 \n  \n  \n    0.095 \n    0.855 \n    0.855 \n    0.124 \n    2.203 \n    226.585 \n  \n  \n    0.100 \n    0.873 \n    0.855 \n    0.118 \n    2.115 \n    239.930 \n  \n\n\n\n\n\nThis now looks much more stable. Until 0.130 we don’t have switchest in the parameter region. Still, rates 0.005-0.03 are very similar.\nHere are the plots for rates 0.005-0.10:\n\n\nCode\npred_data &lt;- best_subset |&gt;\n  unnest(c(data, pred)) |&gt;\n  mutate(\n    gap = as.factor(gap),\n    rate = round(rate, 3)\n  )\n\nmyplot &lt;- function(data) {\n  ggplot(data, aes(x = gap, y = p_correct, color = chunk)) +\n    geom_point() +\n    geom_line(aes(y = pred, group = chunk)) +\n    scale_color_discrete(\"First chunk LTM?\") +\n    facet_grid(rate ~ itemtype) +\n    theme_pub()\n}\n\n\n\nRate [0.005, 0.025]Rate [0.03, 0.05]Rate [0.055, 0.75]Rate [0.08, 0.10]\n\n\n\n\nCode\nmyplot(filter(pred_data, rate &lt; 0.026))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmyplot(filter(pred_data, rate &lt; 0.051 & rate &gt; 0.029))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmyplot(filter(pred_data, rate &lt; 0.076 & rate &gt; 0.054))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmyplot(filter(pred_data, rate &gt; 0.079, rate &lt; 0.101))",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Parameter identifiability"
    ]
  },
  {
    "objectID": "docs/notebooks/par_identifiability.html#what-about-experiment-1",
    "href": "docs/notebooks/par_identifiability.html#what-about-experiment-1",
    "title": "Parameter identifiability",
    "section": "What about experiment 1?",
    "text": "What about experiment 1?\nI will now repeat the same analysis for experiment 1.\n\n\nCode\nbest_subset &lt;- filter(best, exp == 1, exclude_sp1)\n\nbest_subset |&gt;\n  select(rate, prop, prop_ltm, tau, gain, deviance) |&gt;\n  mutate_all(round, 3) |&gt;\n  kbl() %&gt;%\n  kable_styling()\n\n\n\n \n  \n    rate \n    prop \n    prop_ltm \n    tau \n    gain \n    deviance \n  \n \n\n  \n    0.005 \n    0.106 \n    0.573 \n    0.089 \n    100.000 \n    36.273 \n  \n  \n    0.010 \n    0.115 \n    0.576 \n    0.096 \n    85.610 \n    31.959 \n  \n  \n    0.015 \n    0.170 \n    0.579 \n    0.129 \n    40.879 \n    32.380 \n  \n  \n    0.020 \n    0.223 \n    0.587 \n    0.154 \n    24.841 \n    33.079 \n  \n  \n    0.025 \n    0.270 \n    0.593 \n    0.172 \n    17.465 \n    34.078 \n  \n  \n    0.030 \n    0.314 \n    0.600 \n    0.184 \n    13.397 \n    35.384 \n  \n  \n    0.035 \n    0.351 \n    0.602 \n    0.192 \n    11.022 \n    37.002 \n  \n  \n    0.040 \n    0.384 \n    0.605 \n    0.197 \n    9.416 \n    38.918 \n  \n  \n    0.045 \n    0.413 \n    0.607 \n    0.200 \n    8.305 \n    41.117 \n  \n  \n    0.050 \n    0.439 \n    0.608 \n    0.202 \n    7.448 \n    43.575 \n  \n  \n    0.055 \n    0.457 \n    0.603 \n    0.204 \n    6.888 \n    46.274 \n  \n  \n    0.060 \n    0.481 \n    0.603 \n    0.205 \n    6.310 \n    49.177 \n  \n  \n    0.065 \n    0.497 \n    0.598 \n    0.206 \n    5.889 \n    52.249 \n  \n  \n    0.070 \n    0.510 \n    0.594 \n    0.207 \n    5.570 \n    55.470 \n  \n  \n    0.075 \n    0.405 \n    0.374 \n    0.221 \n    7.375 \n    56.971 \n  \n  \n    0.080 \n    0.423 \n    0.369 \n    0.225 \n    6.802 \n    58.529 \n  \n  \n    0.085 \n    0.441 \n    0.367 \n    0.228 \n    6.311 \n    60.169 \n  \n  \n    0.090 \n    0.457 \n    0.364 \n    0.230 \n    5.910 \n    61.889 \n  \n  \n    0.095 \n    0.474 \n    0.360 \n    0.232 \n    5.534 \n    63.682 \n  \n  \n    0.100 \n    0.490 \n    0.357 \n    0.234 \n    5.222 \n    65.541 \n  \n\n\n\n\n\nsimilar switch at 0.075 to a different region of the parameter space.\nHere are the plots for rates 0.005-0.10:\n\n\nCode\npred_data &lt;- best_subset |&gt;\n  unnest(c(data, pred)) |&gt;\n  mutate(\n    gap = as.factor(gap),\n    rate = round(rate, 3)\n  )\n\n\n\nRate [0.005, 0.025]Rate [0.03, 0.05]Rate [0.055, 0.75]Rate [0.08, 0.10]\n\n\n\n\nCode\nmyplot(filter(pred_data, rate &lt; 0.026))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmyplot(filter(pred_data, rate &lt; 0.051 & rate &gt; 0.029))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmyplot(filter(pred_data, rate &lt; 0.076 & rate &gt; 0.054))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmyplot(filter(pred_data, rate &gt; 0.079, rate &lt; 0.101))\n\n\n\n\n\n\n\n\n\n\n\n\nso even a rate of 0.07 does not necessarily predict a big interaction",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Parameter identifiability"
    ]
  },
  {
    "objectID": "docs/notebooks/par_identifiability.html#summary",
    "href": "docs/notebooks/par_identifiability.html#summary",
    "title": "Parameter identifiability",
    "section": "Summary",
    "text": "Summary\n\nThe parameter space is not smooth, but rather jumps between regions when rate is fixed to different values\nThere are ridges in the parameter space where the fits are very similar",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Parameter identifiability"
    ]
  },
  {
    "objectID": "docs/notebooks/par_identifiability.html#footnotes",
    "href": "docs/notebooks/par_identifiability.html#footnotes",
    "title": "Parameter identifiability",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ntechnically, I put a really narrow prior Normal(value, 0.0001) on the rate parameter rather than fixing it just because I can do it in the existing code.↩︎",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Parameter identifiability"
    ]
  },
  {
    "objectID": "docs/dev-notebooks/2024-05-16-meeting.html",
    "href": "docs/dev-notebooks/2024-05-16-meeting.html",
    "title": "2024-05-16 Meeting Notes",
    "section": "",
    "text": "Parameter identifiability and trade-offs (more details linked here)\n\nThe parameter space geometry is complicated\nBest-fitting parameters differ from the paper reported values\nI started from 100 different values\nPossible overfitting?\n\nArgument is about best-fitting parameters. But question is more about “reasonable parameters” (related to overfitting)\n\nhow to determine what range of parameter values are consistent with the data?\n\nWhat is “too slow?”. Comparison with estimates from Popov & Reder (2020) & Ma et al (In press)\n\n(!) just model it trial by trial allowing resource to recover however much they can in the time between trials\n\nCaputring the primacy effect is to “blame”. Model with multiple contributions to primacy?\n\nIn experiment 2, after 6 seconds gap there is no difference in SP4-6 between known and random chunks, which the model can only account for width full recovery.\n\nShould we model the first chunk?\n\nIf so, how?\n\nHow much modeling is enough\n\nI need to be convinced that most reasonable versions of the model require very slow rates\n\nConclusions\n\nI finally understand the argument for freeing capacity. But now the argument seems one-sided. We do formal modeling to reject the resource account, and based on that we conclude that the other capacity account is more plausible without doing any modeling for it?",
    "crumbs": [
      "Development notes",
      "2024-05-16 Meeting Notes"
    ]
  },
  {
    "objectID": "docs/dev-notebooks/2024-05-16-meeting.html#to-discuss",
    "href": "docs/dev-notebooks/2024-05-16-meeting.html#to-discuss",
    "title": "2024-05-16 Meeting Notes",
    "section": "",
    "text": "Parameter identifiability and trade-offs (more details linked here)\n\nThe parameter space geometry is complicated\nBest-fitting parameters differ from the paper reported values\nI started from 100 different values\nPossible overfitting?\n\nArgument is about best-fitting parameters. But question is more about “reasonable parameters” (related to overfitting)\n\nhow to determine what range of parameter values are consistent with the data?\n\nWhat is “too slow?”. Comparison with estimates from Popov & Reder (2020) & Ma et al (In press)\n\n(!) just model it trial by trial allowing resource to recover however much they can in the time between trials\n\nCaputring the primacy effect is to “blame”. Model with multiple contributions to primacy?\n\nIn experiment 2, after 6 seconds gap there is no difference in SP4-6 between known and random chunks, which the model can only account for width full recovery.\n\nShould we model the first chunk?\n\nIf so, how?\n\nHow much modeling is enough\n\nI need to be convinced that most reasonable versions of the model require very slow rates\n\nConclusions\n\nI finally understand the argument for freeing capacity. But now the argument seems one-sided. We do formal modeling to reject the resource account, and based on that we conclude that the other capacity account is more plausible without doing any modeling for it?",
    "crumbs": [
      "Development notes",
      "2024-05-16 Meeting Notes"
    ]
  },
  {
    "objectID": "docs/dev-notebooks/2024-05-16-meeting.html#models-ive-tried",
    "href": "docs/dev-notebooks/2024-05-16-meeting.html#models-ive-tried",
    "title": "2024-05-16 Meeting Notes",
    "section": "Models I’ve tried",
    "text": "Models I’ve tried\n\nModel 1a: same as in the paper (with many different starting values)\nModel 1b: same as 1a but also accounting for the first chunk\nModel 1c: putting prior on parameters. Particularly on the rate parameter over a grid\nModel 2: including the encoding time for the recovery\nModel 3: strength = sqrt(resource_cost)\n\nConclusions so far: - Despite the complicated geometry and parameter trade-offs, recovery rates do need to be slow with this model group",
    "crumbs": [
      "Development notes",
      "2024-05-16 Meeting Notes"
    ]
  },
  {
    "objectID": "docs/dev-notebooks/2024-05-16-meeting.html#modeling-left-to-do",
    "href": "docs/dev-notebooks/2024-05-16-meeting.html#modeling-left-to-do",
    "title": "2024-05-16 Meeting Notes",
    "section": "Modeling left to do",
    "text": "Modeling left to do\n\nExperiment 3\nApply model from Ma et al (In press) just replacing LF and HF with random and known chunks\n\nanalytical likelihood, should be fast, maybe even bayesian\ncan be applied on a trial-by-trial basis\n(!) just model it trial by trial allowing resource to recover however much they can in the time between trials",
    "crumbs": [
      "Development notes",
      "2024-05-16 Meeting Notes"
    ]
  },
  {
    "objectID": "docs/dev-notebooks/2024-05-16-meeting.html#next-steps",
    "href": "docs/dev-notebooks/2024-05-16-meeting.html#next-steps",
    "title": "2024-05-16 Meeting Notes",
    "section": "Next steps",
    "text": "Next steps\n\nBootstrap participants and trials to and fit the model too get distribution\n\nif possible bayesian\n\nFit the Ma et al (In press) model to the trail level data with actual inter-trail intervals\nWhat does the model predict - interaction for introduction\nFraming - we are testing this model as a potential new comprehensive explanation. Do not embrace alternative conclusion just because (if) this models fails to pass",
    "crumbs": [
      "Development notes",
      "2024-05-16 Meeting Notes"
    ]
  },
  {
    "objectID": "docs/notes.html",
    "href": "docs/notes.html",
    "title": "Notes",
    "section": "",
    "text": "Cleaning up the notebooks and targets pipeline.",
    "crumbs": [
      "Development notes",
      "Notes"
    ]
  },
  {
    "objectID": "docs/notes.html#next-steps",
    "href": "docs/notes.html#next-steps",
    "title": "Notes",
    "section": "Next steps",
    "text": "Next steps\n\nKlaus suggests to fit the data with an extra parameter to absorb parts of the primacy effect. Maybe that will allow us to get a better estimate of the recovery rate without assuming that the primacy effect is entirely due to resource depletion. I’m worried that this will make the model too complex and we will not be able to estimate the parameters reliably. But I will try it. Update: actually works.\nI still want to fit the model I developed for Ma, Popov & Zhang (In press), in which I solved the likelihood function for the model with linear recovery and was able to fit it trial by trial. Perhaps slow recovery is not a problem if the depletion and recovery balance each other out over the course of the experiment (see 2024-05-16 Meeting Notes)\nGiven that the non-linear recovery model predicts an interaction more reliably, can I also develop a trial-by-trial likelihood function for this model?",
    "crumbs": [
      "Development notes",
      "Notes"
    ]
  },
  {
    "objectID": "docs/notes.html#non-linear-recovery",
    "href": "docs/notes.html#non-linear-recovery",
    "title": "Notes",
    "section": "Non-linear recovery",
    "text": "Non-linear recovery\nI added simulations with a non-linear recovery model. We still have the issue that parameter estimates are mostly driven by primacy, but there are a few novel results.\nThe non-linear recovery model predicts an interaction more reliably, not just when resources become completely depleted in one condition. You can play around in an interactive app here to explore the model predictions: here\nThe non-linear model estimates for the three experiments are here\nParameter estimates are very similar for the three experiments. I’m not sure why, but navigating the parameter space was much easier for this model, and these parameter estimates were reached from many more starting points relative to the linear recovery model\nSince the recovery is non-linear, we cannot calculate as easily how long it takes to recover all resources (that is in fact Infinite time!), we can ask how long it would take to recover a certain proportion of resources from baseline, e.g. 50%. Results are described in the link, but here’s the gist: based on the rates from the 3 experiments, it would take 5-10 seconds to recovery 50% of the resources.\nI did the bootstrapping simulations we discussed with this new model. Results are here: here\nThe bootstrap recovery rates indicate that we have 95% probability that the best fitting model is one in which we can recover 50% of the resources within 2.8-10 seconds\nIn summary, the non-linear recovery model makes a stronger prediction for an interaction between the two effects, and tells us that recovery rates are not necessarily super slow.\nIs 2.8-10 seconds for 50% recovery fast enough for working memory? The same rates tell us that we would need between 1.2 and 4.4 seconds for 25% recovery. As we discussed last time, it might be the case that within a long experiment resources never fully recover, but the depletion and recovery rates stabilize. I still need to run that simulation. But at least to me these do not seem absurd estimates and we also still have the issue that this assumes that the primacy effect is 100% due to resource depletion.",
    "crumbs": [
      "Development notes",
      "Notes"
    ]
  },
  {
    "objectID": "docs/dev-notebooks/new_model_eq.html",
    "href": "docs/dev-notebooks/new_model_eq.html",
    "title": "Extra primacy parameter",
    "section": "",
    "text": "TODO: Clean-up this notebook\nCode\nlibrary(tidyverse)\nlibrary(targets)\ntar_source()\ntar_load(c(exp3_data_agg, exp3_data))\nDefine new model likelihood\nOne example fit:\nCode\nset.seed(25)\nfit &lt;- estimate_model(c(start_fun(), prim_prop = rbeta(1, 5, 2)), exp3_data_agg, version = 2, exclude_sp1 = T)\nkableExtra::kable(optimfit_to_df(fit))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprop\nprop_ltm\nrate\ngain\ntau\nprim_prop\ndeviance\nconvergence\n\n\n\n\n0.129707\n0.5184667\n0.1597853\n42.70181\n0.089936\n0.9305835\n760.8202\n0\n\n\n\n\n\nCode\nexp3_data_agg$pred &lt;- predict(fit, exp3_data_agg, group_by = c(\"chunk\", \"gap\"))\n\nexp3_data_agg |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point(alpha = 0.5) +\n  stat_smooth(method = \"lm\", se = FALSE, linewidth = 0.5) +\n  geom_line(aes(y = pred), linetype = \"dashed\", linewidth = 1.1) +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\nRun the estimation 100 times with different starting values:\nCode\nset.seed(18)\nfits &lt;- run_or_load(\n  {\n    res &lt;- replicate(\n      n = 100,\n      expr = estimate_model(\n        c(start_fun(), prim_prop = rbeta(1, 5, 2)),\n        exp3_data_agg,\n        version = 2, exclude_sp1 = T\n      ),\n      simplify = FALSE\n    )\n    res &lt;- do.call(rbind, lapply(res, optimfit_to_df))\n    arrange(res, deviance)\n  },\n  file = \"output/exp3_fits_100_prop_primacy.rds\"\n)\nRefine the fits with a second pass starting from the best fit:\nCode\nfits_refined &lt;- run_or_load(\n  {\n    res &lt;- apply(fits, 1, function(x) {\n      fit &lt;- estimate_model(x, exp3_data_agg, version = 2, exclude_sp1 = T)\n      optimfit_to_df(fit)\n    })\n    do.call(rbind, res)\n  },\n  file = \"output/exp3_fits_100_prop_primacy_refined.rds\"\n)\nfits_refined$deviance &lt;- 2 * fits_refined$deviance\nPrint fits sorted by deviance:\nCode\nkableExtra::kable(arrange(fits_refined, deviance))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprop\nprop_ltm\nrate\ngain\ntau\nprim_prop\ndeviance\nconvergence\n\n\n\n\n0.0830018\n0.5354440\n0.1521677\n100.00000\n0.0664134\n0.9592183\n1520.717\n0\n\n\n0.0830345\n0.5354821\n0.1522194\n99.92881\n0.0664355\n0.9592133\n1520.717\n0\n\n\n0.0827561\n0.5303713\n0.1540288\n99.99304\n0.0661689\n0.9585271\n1520.720\n0\n\n\n0.0833021\n0.5326726\n0.1534809\n98.99042\n0.0665430\n0.9585757\n1520.723\n0\n\n\n0.0836369\n0.5354833\n0.1524269\n98.53729\n0.0668022\n0.9588923\n1520.724\n0\n\n\n0.0838078\n0.5349846\n0.1527897\n98.13969\n0.0669035\n0.9587414\n1520.726\n0\n\n\n0.0839589\n0.5352356\n0.1523192\n97.77404\n0.0669837\n0.9586211\n1520.728\n0\n\n\n0.0840183\n0.5344278\n0.1528843\n97.58774\n0.0670162\n0.9585227\n1520.729\n0\n\n\n0.0840788\n0.5337896\n0.1528134\n97.48065\n0.0670620\n0.9585533\n1520.729\n0\n\n\n0.0843323\n0.5352119\n0.1524255\n96.99083\n0.0672207\n0.9584937\n1520.731\n0\n\n\n0.0844863\n0.5384292\n0.1511989\n96.96959\n0.0673687\n0.9588990\n1520.732\n0\n\n\n0.0841521\n0.5333316\n0.1531871\n97.06339\n0.0670524\n0.9581341\n1520.733\n0\n\n\n0.0850234\n0.5387903\n0.1513496\n95.74477\n0.0676811\n0.9585429\n1520.738\n0\n\n\n0.0854952\n0.5374167\n0.1511926\n94.65133\n0.0679400\n0.9581097\n1520.744\n0\n\n\n0.0853425\n0.5370373\n0.1515787\n94.77365\n0.0678094\n0.9579259\n1520.744\n0\n\n\n0.0854451\n0.5350333\n0.1525429\n94.50061\n0.0678727\n0.9577814\n1520.745\n0\n\n\n0.0857269\n0.5368538\n0.1515001\n94.18233\n0.0680980\n0.9581038\n1520.746\n0\n\n\n0.0872424\n0.5390619\n0.1510209\n91.40748\n0.0690687\n0.9577849\n1520.763\n0\n\n\n0.0878679\n0.5354037\n0.1524971\n89.87321\n0.0693818\n0.9569372\n1520.771\n0\n\n\n0.0880266\n0.5353644\n0.1539354\n89.55011\n0.0694815\n0.9567853\n1520.778\n0\n\n\n0.0884297\n0.5360268\n0.1513224\n88.72862\n0.0696856\n0.9565830\n1520.779\n0\n\n\n0.0878443\n0.5251040\n0.1565616\n89.02264\n0.0691813\n0.9553297\n1520.786\n0\n\n\n0.0892337\n0.5399017\n0.1496270\n87.49761\n0.0702117\n0.9566977\n1520.788\n0\n\n\n0.0896874\n0.5467911\n0.1466649\n86.80853\n0.0704719\n0.9567592\n1520.802\n0\n\n\n0.0915173\n0.5403781\n0.1506760\n83.44070\n0.0715516\n0.9554777\n1520.814\n0\n\n\n0.0915863\n0.5467830\n0.1481750\n83.97125\n0.0717513\n0.9567280\n1520.815\n0\n\n\n0.0907884\n0.5516159\n0.1462604\n85.63544\n0.0713531\n0.9578251\n1520.816\n0\n\n\n0.0904884\n0.5308324\n0.1531446\n84.52523\n0.0708115\n0.9548965\n1520.819\n0\n\n\n0.0921162\n0.5379638\n0.1511176\n82.15251\n0.0718255\n0.9546897\n1520.824\n0\n\n\n0.0934299\n0.5461201\n0.1483120\n80.77434\n0.0727898\n0.9556430\n1520.837\n0\n\n\n0.0936599\n0.5373178\n0.1509119\n79.91900\n0.0728257\n0.9546394\n1520.847\n0\n\n\n0.0934048\n0.5373752\n0.1522977\n79.64859\n0.0724489\n0.9532476\n1520.856\n0\n\n\n0.0954108\n0.5338564\n0.1530252\n76.62310\n0.0736285\n0.9523789\n1520.874\n0\n\n\n0.0964956\n0.5497604\n0.1470154\n76.30653\n0.0746150\n0.9547932\n1520.879\n0\n\n\n0.0917041\n0.5584003\n0.1470930\n83.85527\n0.0717872\n0.9566873\n1520.882\n0\n\n\n0.0973273\n0.5302367\n0.1539601\n73.54802\n0.0746042\n0.9507225\n1520.911\n0\n\n\n0.0987585\n0.5460551\n0.1491697\n72.64034\n0.0757628\n0.9528200\n1520.913\n0\n\n\n0.0987959\n0.5381743\n0.1510308\n71.89543\n0.0755197\n0.9509666\n1520.921\n0\n\n\n0.1007429\n0.5473097\n0.1478690\n70.29082\n0.0769701\n0.9526443\n1520.941\n0\n\n\n0.1031380\n0.5365476\n0.1515734\n66.21486\n0.0778125\n0.9484016\n1520.993\n0\n\n\n0.1041868\n0.5425277\n0.1497028\n65.23047\n0.0784592\n0.9486410\n1521.005\n0\n\n\n0.1041107\n0.5585138\n0.1426450\n66.85880\n0.0790617\n0.9531637\n1521.005\n0\n\n\n0.1052323\n0.5522870\n0.1466097\n65.03940\n0.0794905\n0.9512479\n1521.012\n0\n\n\n0.1032888\n0.5270420\n0.1553102\n65.66822\n0.0777932\n0.9473241\n1521.014\n0\n\n\n0.1050829\n0.5429193\n0.1493026\n64.45731\n0.0790327\n0.9487062\n1521.016\n0\n\n\n0.1055747\n0.5453514\n0.1495319\n64.38901\n0.0795725\n0.9500933\n1521.017\n0\n\n\n0.1057176\n0.5363009\n0.1519290\n63.21499\n0.0791567\n0.9469873\n1521.037\n0\n\n\n0.1056761\n0.5328284\n0.1528260\n63.19574\n0.0791423\n0.9469534\n1521.040\n0\n\n\n0.1095031\n0.5597043\n0.1427385\n60.18882\n0.0814623\n0.9482292\n1521.102\n0\n\n\n0.1038245\n0.5178414\n0.1606086\n64.11113\n0.0776493\n0.9440227\n1521.108\n0\n\n\n0.1112165\n0.5405131\n0.1513658\n57.83141\n0.0821252\n0.9451479\n1521.121\n0\n\n\n0.1139487\n0.5665163\n0.1408282\n56.79748\n0.0842963\n0.9495305\n1521.163\n0\n\n\n0.1150464\n0.5620415\n0.1424970\n55.49423\n0.0846745\n0.9478959\n1521.174\n0\n\n\n0.1104956\n0.5844357\n0.1330230\n61.23827\n0.0830523\n0.9550584\n1521.176\n0\n\n\n0.1150277\n0.5707915\n0.1394673\n56.13136\n0.0850230\n0.9502024\n1521.195\n0\n\n\n0.1168787\n0.5563987\n0.1475113\n53.57138\n0.0854354\n0.9456710\n1521.224\n0\n\n\n0.1170302\n0.5467398\n0.1477255\n52.79237\n0.0850381\n0.9430341\n1521.225\n0\n\n\n0.1190151\n0.5745878\n0.1375233\n52.76130\n0.0870007\n0.9487474\n1521.265\n0\n\n\n0.1180843\n0.5374202\n0.1543369\n51.51058\n0.0853142\n0.9404624\n1521.282\n0\n\n\n0.1206378\n0.5369294\n0.1526183\n49.75313\n0.0867212\n0.9404126\n1521.311\n0\n\n\n0.1222813\n0.5450775\n0.1484505\n48.58462\n0.0874118\n0.9398335\n1521.338\n0\n\n\n0.1219608\n0.5950076\n0.1293583\n51.47250\n0.0890850\n0.9520846\n1521.387\n0\n\n\n0.1258652\n0.5543236\n0.1449665\n46.47660\n0.0893696\n0.9402240\n1521.393\n0\n\n\n0.1261358\n0.5512622\n0.1469249\n46.15517\n0.0893986\n0.9393450\n1521.405\n0\n\n\n0.1229203\n0.6004439\n0.1274795\n51.09406\n0.0898015\n0.9532568\n1521.437\n0\n\n\n0.1253455\n0.5347543\n0.1524909\n45.93500\n0.0884188\n0.9358299\n1521.453\n0\n\n\n0.1096435\n0.6245279\n0.1157842\n65.14104\n0.0838056\n0.9644263\n1521.465\n0\n\n\n0.1291902\n0.5698808\n0.1404231\n44.52248\n0.0909739\n0.9401136\n1521.507\n0\n\n\n0.1316542\n0.5861278\n0.1329112\n44.07969\n0.0930474\n0.9448045\n1521.528\n0\n\n\n0.1320240\n0.5487744\n0.1460666\n42.47430\n0.0920422\n0.9368930\n1521.552\n0\n\n\n0.1407647\n0.5726243\n0.1378135\n38.67907\n0.0966723\n0.9386590\n1521.727\n0\n\n\n0.1410802\n0.5921934\n0.1299198\n39.08275\n0.0972915\n0.9421512\n1521.746\n0\n\n\n0.1416289\n0.5584553\n0.1449145\n37.55499\n0.0961602\n0.9331385\n1521.772\n0\n\n\n0.1442493\n0.5817889\n0.1357689\n37.16069\n0.0981666\n0.9379234\n1521.805\n0\n\n\n0.1485820\n0.5506684\n0.1476578\n34.44050\n0.0989464\n0.9297136\n1521.974\n0\n\n\n0.1335032\n0.4896621\n0.1771724\n40.02027\n0.0912584\n0.9248692\n1522.027\n0\n\n\n0.1508329\n0.5659537\n0.1397505\n33.56548\n0.0994088\n0.9283791\n1522.043\n0\n\n\n0.1464991\n0.6365233\n0.1119736\n38.25589\n0.1015555\n0.9525763\n1522.066\n0\n\n\n0.1570901\n0.5720074\n0.1369226\n31.39721\n0.1020279\n0.9277262\n1522.192\n0\n\n\n0.1578781\n0.5796311\n0.1354878\n31.28607\n0.1025839\n0.9289186\n1522.195\n0\n\n\n0.1557528\n0.5623220\n0.1426612\n31.36372\n0.1005411\n0.9230903\n1522.254\n0\n\n\n0.1615952\n0.5661500\n0.1440328\n29.91051\n0.1040553\n0.9264082\n1522.330\n0\n\n\n0.1625162\n0.5859924\n0.1360934\n29.80458\n0.1043471\n0.9275039\n1522.355\n0\n\n\n0.1643793\n0.6202930\n0.1195451\n30.34787\n0.1072577\n0.9396434\n1522.388\n0\n\n\n0.1628606\n0.6659198\n0.1026569\n32.71971\n0.1100000\n0.9576486\n1522.606\n0\n\n\n0.1673065\n0.5570623\n0.1476521\n27.42337\n0.1035436\n0.9131497\n1522.795\n0\n\n\n0.1494665\n0.7064391\n0.0870326\n40.84013\n0.1073822\n0.9772426\n1522.822\n0\n\n\n0.1757971\n0.5737036\n0.1355153\n25.60590\n0.1074592\n0.9172798\n1522.859\n0\n\n\n0.1237810\n0.7275594\n0.0803559\n60.88937\n0.0958362\n0.9892582\n1522.916\n0\n\n\n0.1478485\n0.7136234\n0.0842819\n42.33303\n0.1072880\n0.9809562\n1522.918\n0\n\n\n0.1508772\n0.7128888\n0.0843160\n40.69104\n0.1086132\n0.9801147\n1522.946\n0\n\n\n0.1641892\n0.5166694\n0.1581206\n27.52562\n0.1009010\n0.9058324\n1522.977\n0\n\n\n0.1454702\n0.7272316\n0.0801023\n44.59122\n0.1069657\n0.9859114\n1523.054\n0\n\n\n0.1560018\n0.7256169\n0.0812223\n39.04160\n0.1118570\n0.9840110\n1523.160\n0\n\n\n0.1008613\n0.7540632\n0.0701410\n95.94044\n0.0832805\n0.9999999\n1523.350\n0\n\n\n0.1500814\n0.7543605\n0.0712736\n44.53389\n0.1115246\n0.9976890\n1523.548\n0\n\n\n0.2012512\n0.6942274\n0.0925222\n22.99875\n0.1245127\n0.9570987\n1523.700\n0\n\n\n0.1954065\n0.5381045\n0.1690204\n21.11082\n0.1135602\n0.9050614\n1524.333\n0\n\n\n0.2222245\n0.7091319\n0.0886436\n19.79598\n0.1324736\n0.9621327\n1524.418\n0\n\n\n0.2420613\n0.7426193\n0.0755768\n17.80195\n0.1411300\n0.9779758\n1525.069\n0\nNot as much variance as I thought there would be. Let’s limit to the fits within 2 deviance units of the best fit:\nHere is the distribution of parameter values that are within 2 deviance units of the best fit:\nCode\nbest_fits &lt;- filter(fits_refined, deviance &lt; min(deviance) + 2)\n\nbest_fits |&gt;\n  select(prop:prim_prop) |&gt;\n  pivot_longer(cols = everything(), names_to = \"parameter\", values_to = \"value\") |&gt;\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 30) +\n  facet_wrap(~parameter, scales = \"free\") +\n  theme_pub()\nHere is the plot of the best fitting model:\nCode\nbest_fit &lt;- estimate_model(unlist(best_fits[1, 1:6]), exp3_data_agg, version = 2, exclude_sp1 = T)\nexp3_data_agg$pred &lt;- predict(best_fit, exp3_data_agg, group_by = c(\"chunk\", \"gap\"))\n\nexp3_data_agg |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point(alpha = 0.5) +\n  stat_smooth(method = \"lm\", se = FALSE, linewidth = 0.5) +\n  geom_line(aes(y = pred), linetype = \"dashed\", linewidth = 1.1) +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n\n\n`geom_smooth()` using formula = 'y ~ x'\nand the parameter estimates:\nCode\nkableExtra::kable(t(as.data.frame(best_fit$par)))\n\n\n\n\n\n\nprop\nprop_ltm\nrate\ngain\ntau\nprim_prop\n\n\n\n\nx\n0.0827601\n0.5305627\n0.1540501\n99.99801\n0.0661733\n0.958538\nwith this rate, the resource recovery over time looks like this:\nCode\nresources &lt;- function(R, rate, time, r_max = 1) {\n  (r_max - R) * (1 - exp(-rate * time))\n}\n\ntime &lt;- seq(0, 20, 0.1)\ntibble(time = time, resources = resources(0, best_fits[1, ]$rate, time)) |&gt;\n  ggplot(aes(x = time, y = resources)) +\n  geom_line() +\n  theme_pub()\nwith 50% of the resource recovered after 4.5 seconds\nand 25% of the resource recovered after 1.87 seconds",
    "crumbs": [
      "Development notes",
      "Extra primacy parameter"
    ]
  },
  {
    "objectID": "docs/dev-notebooks/new_model_eq.html#simulate-full-trial-sequences",
    "href": "docs/dev-notebooks/new_model_eq.html#simulate-full-trial-sequences",
    "title": "Extra primacy parameter",
    "section": "Simulate full trial sequences",
    "text": "Simulate full trial sequences\n\nWith recovery during entire recall period\nCalculate the total recall period duration on each trial:\n\n\nCode\ntotal_rts &lt;- exp3_data |&gt;\n  group_by(id, trial) |&gt;\n  summarize(total_recall_duration = sum(rt) + 9 * 0.2 + 1)\n\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups` argument.\n\n\nCode\nhist(total_rts$total_recall_duration, breaks = 30, col = \"grey\", border = \"white\", xlab = \"Total recall period duration (s.)\", main = \"\")\n\n\n\n\n\n\n\n\n\nCreate the trial structure for the simulation:\n\n\nCode\nexp3_trial_str &lt;- run_or_load({\n  exp3_data |&gt;\n  group_by(id, trial) |&gt;\n  do({\n    aggregate_data(.)\n  })\n}, file = \"output/exp3_trial_structure.rds\")\n\nexp3_trial_str &lt;- left_join(exp3_trial_str, total_rts) |&gt; \n  mutate(ISI = ifelse(itemtype == \"SP7-9\", total_recall_duration, ISI),\n         ser_pos = case_when(\n          itemtype == \"SP1-3\" ~ 1,\n          itemtype == \"SP4-6\" ~ 2,\n          itemtype == \"SP7-9\" ~ 3\n         ))\n\n\nRun the model without resetting the resource at the start of each trial:\n\n\nCode\nserial_recall_full &lt;- function(\n    setsize, ISI = rep(0.5, setsize), item_in_ltm = rep(TRUE, setsize), ser_pos = 1:setsize, \n    prop = 0.2, prop_ltm = 0.5, tau = 0.15, gain = 25, rate = 0.1, prim_prop = 1,\n    r_max = 1, lambda = 1, growth = \"linear\") {\n  R &lt;- r_max\n  p_recall &lt;- vector(\"numeric\", length = setsize)\n  prop_ltm &lt;- ifelse(item_in_ltm, prop_ltm, 1)\n\n  for (item in 1:setsize) {\n    # strength of the item and recall probability\n    strength &lt;- (prop * R)^lambda * prim_prop^(ser_pos[item] - 1)\n    p_recall[item] &lt;- 1 / (1 + exp(-(strength - tau) * gain))\n\n    # amount of resources consumed by the item\n    r_cost &lt;- strength^(1 / lambda) * prop_ltm[item]\n    R &lt;- R - r_cost\n\n    # recover resources\n    R &lt;- switch(growth,\n      \"linear\" = min(r_max, R + rate * ISI[item]),\n      \"asy\" = R + (r_max - R) * (1 - exp(-rate * ISI[item]))\n    )\n  }\n\n  p_recall\n}\n\nsubj1 &lt;- exp3_trial_str |&gt;\n  filter(id == 44125)\n\n\nHere it is for one example subject:\n\n\n\nCode\nsubj1$pred &lt;- serial_recall_full(\n  setsize = nrow(subj1),\n  ISI = subj1$ISI,\n  item_in_ltm = subj1$item_in_ltm,\n  ser_pos = subj1$ser_pos,\n  prop = best_fit$par[\"prop\"],\n  prop_ltm = best_fit$par[\"prop_ltm\"],\n  tau = best_fit$par[\"tau\"],\n  gain = best_fit$par[\"gain\"],\n  rate = best_fit$par[\"rate\"],\n  prim_prop = best_fit$par[\"prim_prop\"],\n  growth = \"asy\"\n)\n\nsubj1 |&gt;\n  ungroup() |&gt;\n  mutate(absolute_position = 1:n()) |&gt;\n  ggplot(aes(x = absolute_position, y = pred)) +\n  geom_point() +\n  geom_line() +\n  theme_pub()\n\n\n\n\n\n\n\n\n\n\nSame, but collapsed over serial position (one value per trial):\n\n\nCode\nsubj1 |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = trial, y = pred)) +\n  stat_summary(geom = \"point\") +\n  geom_smooth() +\n  theme_pub()\n\n\n\n\n\n\n\n\n\nNow simulate the full dataset:\n\n\nCode\nsim_no_reset_full &lt;- function(data, best_fit) {\n  data |&gt;\n    group_by(id) |&gt;\n    mutate(pred = serial_recall_full(\n      setsize = n(),\n      ISI = ISI,\n      item_in_ltm = item_in_ltm,\n      ser_pos = ser_pos,\n      prop = best_fit$par[\"prop\"],\n      prop_ltm = best_fit$par[\"prop_ltm\"],\n      tau = best_fit$par[\"tau\"],\n      gain = best_fit$par[\"gain\"],\n      rate = best_fit$par[\"rate\"],\n      prim_prop = best_fit$par[\"prim_prop\"],\n      growth = \"asy\"\n    ))\n}\n\nfull_sim &lt;- sim_no_reset_full(exp3_trial_str, best_fit)\n\nggplot(full_sim, aes(x = trial, y = pred)) +\n  stat_summary() +\n  geom_smooth() +\n  theme_pub()\n\n\n\n\n\n\n\n\n\nYes, the model predicts worsening performance over trials, but the effect is miniscule.\nPlot the original predictions recomputed with the full trial-by-trial model:\n\n\nCode\nfull_sim |&gt;\n  group_by(chunk, gap, itemtype) |&gt;\n  summarize(pred = mean(pred),\n            p_correct = mean(p_correct)) |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point(alpha = 0.5) +\n  stat_smooth(method = \"lm\", se = FALSE, linewidth = 0.5) +\n  geom_line(aes(y = pred), linetype = \"dashed\", linewidth = 1.1) +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n\n\n\n\n\n\n\n\n\n\n\nNo recovery during recall, except during empty screens\n\n\nCode\nexp3_trial_str &lt;- run_or_load(\n  {\n    exp3_data |&gt;\n      group_by(id, trial) |&gt;\n      do({\n        aggregate_data(.)\n      })\n  },\n  file = \"output/exp3_trial_structure.rds\"\n)\n\n\nexp3_trial_str &lt;- exp3_trial_str |&gt;\n  mutate(\n    ISI = ifelse(itemtype == \"SP7-9\", 9 * 0.2 + 1, ISI),\n    ser_pos = case_when(\n      itemtype == \"SP1-3\" ~ 1,\n      itemtype == \"SP4-6\" ~ 2,\n      itemtype == \"SP7-9\" ~ 3\n    )\n  )\n\nfull_sim_v2 &lt;- sim_no_reset_full(exp3_trial_str, best_fit)\n\nggplot(full_sim_v2, aes(x = trial, y = pred)) +\n  stat_summary() +\n  theme_pub()\n\n\nNo summary function supplied, defaulting to `mean_se()`\n\n\n\n\n\n\n\n\n\nCode\nfilter(full_sim_v2, id == 44125) |&gt;\n  mutate(absolute_position = 1:n()) |&gt;\n  ggplot(aes(x = absolute_position, y = pred)) +\n  geom_point() +\n  geom_line() +\n  theme_pub()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfull_sim_v2 |&gt;\n  group_by(chunk, gap, itemtype) |&gt;\n  summarize(pred = mean(pred),\n            p_correct = mean(p_correct)) |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point(alpha = 0.5) +\n  stat_smooth(method = \"lm\", se = FALSE, linewidth = 0.5) +\n  geom_line(aes(y = pred), linetype = \"dashed\", linewidth = 1.1) +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)",
    "crumbs": [
      "Development notes",
      "Extra primacy parameter"
    ]
  },
  {
    "objectID": "docs/notebooks/model3_basic_fits.html",
    "href": "docs/notebooks/model3_basic_fits.html",
    "title": "Basic fits",
    "section": "",
    "text": "Fit the non-linear resource recovery model to the three experiments. Dashed lines are model predictions. Solid dots are the observed data and solid lines are linear regression lines.\n\n\nCode\nlibrary(tidyverse)\nlibrary(targets)\nlibrary(GGally)\nlibrary(kableExtra)\n# load \"R/*\" scripts and saved R objects from the targets pi\ntar_source()\ntar_load(c(exp1_data, exp2_data, exp1_data_agg, exp2_data_agg, exp3_data_agg))\nset.seed(213)",
    "crumbs": [
      "Notebooks",
      "Model 3: Non-linear recovery",
      "Basic fits"
    ]
  },
  {
    "objectID": "docs/notebooks/model3_basic_fits.html#overview",
    "href": "docs/notebooks/model3_basic_fits.html#overview",
    "title": "Basic fits",
    "section": "",
    "text": "Fit the non-linear resource recovery model to the three experiments. Dashed lines are model predictions. Solid dots are the observed data and solid lines are linear regression lines.\n\n\nCode\nlibrary(tidyverse)\nlibrary(targets)\nlibrary(GGally)\nlibrary(kableExtra)\n# load \"R/*\" scripts and saved R objects from the targets pi\ntar_source()\ntar_load(c(exp1_data, exp2_data, exp1_data_agg, exp2_data_agg, exp3_data_agg))\nset.seed(213)",
    "crumbs": [
      "Notebooks",
      "Model 3: Non-linear recovery",
      "Basic fits"
    ]
  },
  {
    "objectID": "docs/notebooks/model3_basic_fits.html#experiment-1",
    "href": "docs/notebooks/model3_basic_fits.html#experiment-1",
    "title": "Basic fits",
    "section": "Experiment 1",
    "text": "Experiment 1\n\n\nCode\nstart &lt;- c(prop = 0.15, prop_ltm = 0.5, rate = 0.25, gain = 30, tau = 0.12)\nest1 &lt;- estimate_model(start, data = exp1_data_agg, exclude_sp1 = TRUE, growth = \"asy\")\nexp1_data_agg$pred &lt;- predict(est1, exp1_data_agg, group_by = c(\"chunk\", \"gap\"), growth = \"asy\")\nexp1_data_agg |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)",
    "crumbs": [
      "Notebooks",
      "Model 3: Non-linear recovery",
      "Basic fits"
    ]
  },
  {
    "objectID": "docs/notebooks/model3_basic_fits.html#experiment-2",
    "href": "docs/notebooks/model3_basic_fits.html#experiment-2",
    "title": "Basic fits",
    "section": "Experiment 2",
    "text": "Experiment 2\n\n\nCode\nest2 &lt;- estimate_model(est1$par, data = exp2_data_agg, exclude_sp1 = TRUE, growth = \"asy\")\nexp2_data_agg$pred &lt;- predict(est2, exp2_data_agg, group_by = c(\"chunk\", \"gap\"), growth = \"asy\")\nexp2_data_agg |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)",
    "crumbs": [
      "Notebooks",
      "Model 3: Non-linear recovery",
      "Basic fits"
    ]
  },
  {
    "objectID": "docs/notebooks/model3_basic_fits.html#experiment-3",
    "href": "docs/notebooks/model3_basic_fits.html#experiment-3",
    "title": "Basic fits",
    "section": "Experiment 3",
    "text": "Experiment 3\n\n\nCode\nest3 &lt;- estimate_model(est1$par, data = exp3_data_agg, exclude_sp1 = TRUE, growth = \"asy\")\nexp3_data_agg$pred &lt;- predict(est3, exp3_data_agg, group_by = c(\"chunk\", \"gap\"), growth = \"asy\")\nexp3_data_agg |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point(alpha = 0.5) +\n  stat_smooth(method = \"lm\", se = FALSE, linewidth = 0.5) +\n  geom_line(aes(y = pred), linetype = \"dashed\", linewidth = 1.1) +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nparameter estimates for the three experiments:\n\n\nCode\nkable(round(bind_rows(est1$par, est2$par, est3$par), 3)) %&gt;%\n  kable_styling()\n\n\n\n\n\nprop\nprop_ltm\nrate\ngain\ntau\n\n\n\n\n0.112\n0.511\n0.121\n95.703\n0.095\n\n\n0.103\n0.726\n0.107\n93.932\n0.089\n\n\n0.106\n0.754\n0.070\n87.741\n0.086",
    "crumbs": [
      "Notebooks",
      "Model 3: Non-linear recovery",
      "Basic fits"
    ]
  },
  {
    "objectID": "docs/notebooks/model3_basic_fits.html#summary",
    "href": "docs/notebooks/model3_basic_fits.html#summary",
    "title": "Basic fits",
    "section": "Summary",
    "text": "Summary\nWith the non-linear recovery model, we get rate estiates of 0.121, 0.107, 0.070 in the three experiments. Unlike the linear recovery rate, we cannot interpret these simply. But we can ask, given the resource recovery equation and these estimates, how long will it take to recover a proportion X of the resource from 0? This is equivalent to the equation\n\\[\n1-e^{-rate \\times t} = X\n\\]\nsolving for t gives\n\\[\nt = -\\frac{log(1-X)}{rate}\n\\]\nwhich we can plot:\n\n\nCode\nrdata &lt;- expand.grid(\n  X = seq(0.1, 0.9, 0.1),\n  rates = round(c(est1$par[\"rate\"], est2$par[\"rate\"], est3$par[\"rate\"]), 3)\n)\n\nrdata$time &lt;- -log(1 - rdata$X) / rdata$rates\n\nrdata |&gt;\n  ggplot(aes(x = X, y = time, color = factor(rates))) +\n  geom_line() +\n  scale_color_discrete(\"Rate\") +\n  labs(x = \"Proportion recovered\", y = \"Time to recover\")\n\n\n\n\n\n\n\n\n\nWe see that it takes:\n\n2.50-4 seconds to recover 25% of the resource from 0 based on the estimates from the three experiments\n5-10 seconds to recover 50% of the resource from 0\n12-20 seconds to recover 75% of the resource from 0.",
    "crumbs": [
      "Notebooks",
      "Model 3: Non-linear recovery",
      "Basic fits"
    ]
  },
  {
    "objectID": "docs/notebooks/non_linear_recovery.html",
    "href": "docs/notebooks/non_linear_recovery.html",
    "title": "Explore model predictions",
    "section": "",
    "text": "In Exploring model predictions I used a shiny app to understand how the basic model with linear resource recovery works, and I found that it does not predict an interaction between chunking and free time proactive benefits. I suspect that a model with a non-linear recovery will be different, because for a fixed time interval it will recover more resource the lower the resource level is. Let’s explore this. Resources are recovered according to the following equation:\n\\[\nR(t) = R_{\\text{current}} + (R_{\\text{max}} - R_{\\text{current}})(1 - e^{-\\text{rate} \\times t})\n\\]\nwhere \\(R_{\\text{max}}\\) is the maximum resource level, \\(R_{\\text{current}}\\) is the current resource level, \\(\\text{rate}\\) is the recovery rate, and \\(t\\) is the time elapsed.",
    "crumbs": [
      "Notebooks",
      "Model 3: Non-linear recovery",
      "Explore model predictions"
    ]
  },
  {
    "objectID": "docs/notebooks/non_linear_recovery.html#non-linear-resource-recovery-model",
    "href": "docs/notebooks/non_linear_recovery.html#non-linear-resource-recovery-model",
    "title": "Explore model predictions",
    "section": "",
    "text": "In Exploring model predictions I used a shiny app to understand how the basic model with linear resource recovery works, and I found that it does not predict an interaction between chunking and free time proactive benefits. I suspect that a model with a non-linear recovery will be different, because for a fixed time interval it will recover more resource the lower the resource level is. Let’s explore this. Resources are recovered according to the following equation:\n\\[\nR(t) = R_{\\text{current}} + (R_{\\text{max}} - R_{\\text{current}})(1 - e^{-\\text{rate} \\times t})\n\\]\nwhere \\(R_{\\text{max}}\\) is the maximum resource level, \\(R_{\\text{current}}\\) is the current resource level, \\(\\text{rate}\\) is the recovery rate, and \\(t\\) is the time elapsed.",
    "crumbs": [
      "Notebooks",
      "Model 3: Non-linear recovery",
      "Explore model predictions"
    ]
  },
  {
    "objectID": "docs/notebooks/non_linear_recovery.html#just-the-resource-recovery",
    "href": "docs/notebooks/non_linear_recovery.html#just-the-resource-recovery",
    "title": "Explore model predictions",
    "section": "Just the resource recovery",
    "text": "Just the resource recovery\nPlot the amount of resource recovered for a fixed time interval as a function of the current resource level.\nHere’s a fixed plot for rate = 0.5:\n\n\nCode\nresources &lt;- function(R, rate, time, r_max = 1) {\n  (r_max - R) * (1 - exp(-rate * time))\n}\npars &lt;- expand.grid(R = seq(0, 0.9, by = 0.1), time = seq(0, 6, by = 0.1))\npars$resources &lt;- resources(pars$R, 0.5, pars$time)\n\n# ggplot with rate as frame and then convert via ggplotly to an interactive plot\nlibrary(ggplot2)\nggplot(pars, aes(time, resources + R, color = R, group = R)) +\n  geom_line() +\n  theme_test(base_size = 14) +\n  labs(\n    title = \"Resource recovery\",\n    x = \"Elapse time for recovery\",\n    y = \"Available resources\",\n    color = \"Initial resource level\"\n  )\n\n\n\n\n\n\n\n\n\n\nYes, now we can clearly see above that within the same time interval, the lower the resource level, the more resources are recovered. Let’s see if this produces the interaction we expected.",
    "crumbs": [
      "Notebooks",
      "Model 3: Non-linear recovery",
      "Explore model predictions"
    ]
  },
  {
    "objectID": "docs/notebooks/non_linear_recovery.html#model-predictions",
    "href": "docs/notebooks/non_linear_recovery.html#model-predictions",
    "title": "Explore model predictions",
    "section": "Model predictions",
    "text": "Model predictions\n\n#| standalone: true\n#| viewerHeight: 700\nlibrary(shiny)\nlibrary(shinylive)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# define functions\npred_prob &lt;- function(\n    setsize, ISI = rep(0.5, setsize), item_in_ltm = rep(TRUE, setsize),\n    prop = 0.2, prop_ltm = 0.5, tau = 0.15, gain = 25, rate = 0.1,\n    r_max = 1, lambda = 1, growth = \"linear\", type = \"response\") {\n  R &lt;- r_max\n  strengths &lt;- vector(\"numeric\", length = setsize)\n  p_recall &lt;- vector(\"numeric\", length = setsize)\n  prop_ltm &lt;- ifelse(item_in_ltm, prop_ltm, 1)\n\n  for (item in 1:setsize) {\n    # strength of the item and recall probability\n    strengths[item] &lt;- (prop * R)^lambda\n\n    # amount of resources consumed by the item\n    r_cost &lt;- strengths[item]^(1 / lambda) * prop_ltm[item]\n    R &lt;- R - r_cost\n\n    # recover resources\n    R &lt;- switch(growth,\n      \"linear\" = min(r_max, R + rate * ISI[item]),\n      \"asy\" = R + (r_max - R) * (1 - exp(-rate * ISI[item]))\n    )\n  }\n\n  if (type == \"response\") {\n    1 / (1 + exp(-(strengths - tau) * gain))\n  } else {\n    strengths\n  }\n}\n\npredict_resmodel &lt;- function(object, data, group_by, type = \"response\", ...) {\n  if (missing(group_by)) {\n    pred &lt;- pred_prob(\n      setsize = nrow(data),\n      ISI = data$ISI,\n      item_in_ltm = data$item_in_ltm,\n      prop = object[\"prop\"],\n      prop_ltm = object[\"prop_ltm\"],\n      tau = object[\"tau\"],\n      gain = object[\"gain\"],\n      rate = object[\"rate\"],\n      type = type,\n      ...\n    )\n    return(pred)\n  }\n\n  by &lt;- do.call(paste, c(data[, group_by], sep = \"_\"))\n  out &lt;- lapply(split(data, by), function(x) {\n    x$pred_tmp_col295 &lt;- predict_resmodel(object, x, type = type, ...)\n    x\n  })\n  out &lt;- do.call(rbind, out)\n  out &lt;- suppressMessages(dplyr::left_join(data, out))\n  out$pred_tmp_col295\n}\n\ndata &lt;- expand.grid(\n  chunk = c(\"known\", \"random\"),\n  gap = seq(500, 6000, by = 225),\n  itemtype = c(\"SP1-3\", \"SP4-6\", \"SP7-9\")\n)\n\ndata$ISI &lt;- ifelse(data$itemtype == \"SP1-3\", data$gap / 1000, 0.5)\ndata$item_in_ltm &lt;- ifelse(data$itemtype == \"SP1-3\", data$chunk == \"known\", FALSE)\nshinyApp(\n  ui = fluidPage(\n    titlePanel(\"Interactive Plot\"),\n    sidebarLayout(\n      sidebarPanel(\n        sliderInput(\"prop\", \"prop:\", min = 0, max = 1, value = 0.15),\n        sliderInput(\"prop_ltm\", \"prop_ltm:\", min = 0, max = 1, value = 0.5),\n        sliderInput(\"rate\", \"rate:\", min = 0, max = 1, value = 0.25),\n        sliderInput(\"gain\", \"gain:\", min = 1, max = 100, value = 33),\n        sliderInput(\"tau\", \"tau:\", min = 0, max = 1, value = 0.12),\n      ),\n      mainPanel(\n        plotOutput(\"distPlot\")\n      )\n    )\n  ),\n  server = function(input, output) {\n    output$distPlot &lt;- renderPlot({\n      par &lt;- c(prop = input$prop, prop_ltm = input$prop_ltm, rate = input$rate, gain = input$gain, tau = input$tau)\n      data |&gt;\n        mutate(\n          Probability = predict_resmodel(par, data = data, group_by = c(\"gap\", \"chunk\"), growth = \"asy\", lambda = 1),\n          Strength = predict_resmodel(par, data = data, group_by = c(\"gap\", \"chunk\"), type = \"strength\", growth = \"asy\", lambda = 1)\n        ) |&gt;\n        pivot_longer(c(Probability, Strength), names_to = \"type\", values_to = \"value\") |&gt;\n        ggplot(aes(gap, value, color = chunk, group = chunk)) +\n        geom_line() +\n        scale_color_discrete(\"1st chunk LTM?\") +\n        facet_grid(type ~ itemtype, scales = \"free\") +\n        theme_classic(base_size = 14)\n    })\n  }\n)\n\nWith the non-linear resource recovery, we do predict an interaction between chunking and free time proactive benefits. In contrast to the linear recovery model, this interaction occurs for a large range of parameter values and does not require that resources are depleted completely in one condition in order for the interaction to occur.",
    "crumbs": [
      "Notebooks",
      "Model 3: Non-linear recovery",
      "Explore model predictions"
    ]
  },
  {
    "objectID": "docs/notebooks/EDA.html",
    "href": "docs/notebooks/EDA.html",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(targets)\nlibrary(ggdist)\ntar_source()\ntar_load(c(exp1_data, exp2_data, exp1_data_agg, exp2_data_agg, exp3_data, exp3_data_agg))\n\ndata_agg &lt;- bind_rows(\n  mutate(exp1_data_agg, exp = \"Exp 1\"),\n  mutate(exp2_data_agg, exp = \"Exp 2\")\n)\n\n\nHere I have a bunch of plots to help me understand the data for the later modeling.",
    "crumbs": [
      "Notebooks",
      "Data",
      "Exploratory data analysis"
    ]
  },
  {
    "objectID": "docs/notebooks/EDA.html#overview",
    "href": "docs/notebooks/EDA.html#overview",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(targets)\nlibrary(ggdist)\ntar_source()\ntar_load(c(exp1_data, exp2_data, exp1_data_agg, exp2_data_agg, exp3_data, exp3_data_agg))\n\ndata_agg &lt;- bind_rows(\n  mutate(exp1_data_agg, exp = \"Exp 1\"),\n  mutate(exp2_data_agg, exp = \"Exp 2\")\n)\n\n\nHere I have a bunch of plots to help me understand the data for the later modeling.",
    "crumbs": [
      "Notebooks",
      "Data",
      "Exploratory data analysis"
    ]
  },
  {
    "objectID": "docs/notebooks/EDA.html#overall-performance",
    "href": "docs/notebooks/EDA.html#overall-performance",
    "title": "Exploratory data analysis",
    "section": "Overall performance",
    "text": "Overall performance\n\n\nCode\ndata_agg |&gt;\n  mutate(gap = as.factor(gap)) |&gt;\n  ggplot(aes(gap, p_correct, color = chunk, group = chunk)) +\n  geom_point() +\n  geom_line() +\n  scale_color_discrete(\"1st chunk LTM?\") +\n  facet_grid(exp ~ itemtype) +\n  theme_pub()\n\n\n\n\n\n\n\n\n\nTogether to compare shared levels:\n\n\nCode\ndata_agg |&gt;\n  ggplot(aes(gap, p_correct, color = chunk, linetype = exp)) +\n  geom_point() +\n  geom_line() +\n  scale_color_discrete(\"1st chunk LTM?\") +\n  scale_x_continuous(breaks = c(500, 3000, 6000)) +\n  coord_cartesian(xlim = c(300, 6200)) +\n  facet_wrap(~itemtype) +\n  theme_pub()\n\n\n\n\n\n\n\n\n\nOverall performance is a bit lower in experiment 2, but not by much. Still, it might be an issue if trying to model the two experiments together. The effect size of the chunk is also smaller.",
    "crumbs": [
      "Notebooks",
      "Data",
      "Exploratory data analysis"
    ]
  },
  {
    "objectID": "docs/notebooks/EDA.html#individual-performance",
    "href": "docs/notebooks/EDA.html#individual-performance",
    "title": "Exploratory data analysis",
    "section": "Individual performance",
    "text": "Individual performance\n\n\nCode\ndata_agg_subj &lt;- bind_rows(\n  mutate(exp1_data, exp = \"Exp 1\"),\n  mutate(exp2_data, exp = \"Exp 2\")\n) |&gt;\n  group_by(id, exp) |&gt;\n  nest() |&gt;\n  mutate(data = map(data, aggregate_data)) |&gt;\n  unnest(data)\n\n\nprint overall accuracy for each subject:\n\n\nCode\ndata_agg_subj |&gt;\n  group_by(exp, id) |&gt;\n  summarize(p_correct = mean(p_correct)) |&gt;\n  ggplot(aes(exp, p_correct)) +\n  geom_dotsinterval(side = \"both\") +\n  theme_pub()",
    "crumbs": [
      "Notebooks",
      "Data",
      "Exploratory data analysis"
    ]
  },
  {
    "objectID": "docs/notebooks/EDA.html#experiment-3",
    "href": "docs/notebooks/EDA.html#experiment-3",
    "title": "Exploratory data analysis",
    "section": "Experiment 3",
    "text": "Experiment 3\n\n\nCode\nexp3_data_agg |&gt;\n  ggplot(aes(gap, p_correct, color = chunk, group = chunk)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  scale_color_discrete(\"1st chunk LTM?\") +\n  facet_wrap(~itemtype) +\n  theme_pub()\n\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Notebooks",
      "Data",
      "Exploratory data analysis"
    ]
  },
  {
    "objectID": "docs/notebooks/EDA.html#recall-times",
    "href": "docs/notebooks/EDA.html#recall-times",
    "title": "Exploratory data analysis",
    "section": "Recall times",
    "text": "Recall times\nThis is the distribution of total recall period duration for experiment 3:\n\n\nCode\ntotal_rts &lt;- exp3_data |&gt;\n  group_by(id, trial) |&gt;\n  summarize(total_recall_duration = sum(rt) + 9 * 0.2) |&gt;\n  pluck(\"total_recall_duration\")\n\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups` argument.\n\n\nCode\nhist(total_rts, breaks = 30, col = \"grey\", border = \"white\", xlab = \"Total recall period duration (s.)\", main = \"\")",
    "crumbs": [
      "Notebooks",
      "Data",
      "Exploratory data analysis"
    ]
  },
  {
    "objectID": "docs/notebooks/exp3_model1.html",
    "href": "docs/notebooks/exp3_model1.html",
    "title": "Experiment 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(targets)\ntar_source()\ntar_load(c(exp3_data_agg))\ntar_load(fits1)\n\nfits1_e3 &lt;- fits1 |&gt;\n  filter(exp == 3) |&gt;\n  mutate(\n    deviance = pmap_dbl(\n      list(fit, data, exclude_sp1),\n      ~ overall_deviance(params = `..1`$par, data = `..2`, exclude_sp1 = `..3`)\n    ),\n    pred = map2(fit, data, ~ predict(.x, .y, group_by = c(\"chunk\", \"gap\")))\n  )",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Experiment 3"
    ]
  },
  {
    "objectID": "docs/notebooks/exp3_model1.html#basic-fit",
    "href": "docs/notebooks/exp3_model1.html#basic-fit",
    "title": "Experiment 3",
    "section": "Basic fit",
    "text": "Basic fit\nFit starting from the parameters reported in the draft for Experiment 1:\n\n\nCode\nest &lt;- run_or_load(\n  estimate_model(\n    start = paper_params(),\n    data = exp3_data_agg,\n    two_step = TRUE,\n    exclude_sp1 = TRUE,\n    simplify = TRUE,\n    prior = list(\n      rate = list(mean = 0.05, sd = 0.01)\n    )\n  ),\n  file = \"output/exp3_basic_fit.rds\"\n)\n\nest\n\n\n# A tibble: 1 × 8\n   prop prop_ltm   rate   tau  gain deviance convergence fit       \n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt; &lt;list&gt;    \n1 0.296    0.819 0.0143 0.153  12.9     769.           0 &lt;srl_rcl_&gt;\n\n\n\n\nCode\nexp3_data_agg$pred &lt;- predict(est$fit[[1]], data = exp3_data_agg, group_by = c(\"chunk\", \"gap\"))\n\nexp3_data_agg |&gt;\n  ggplot(aes(gap, p_correct, color = chunk, group = chunk)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  geom_line(aes(y = pred), color = \"black\") +\n  scale_color_discrete(\"1st chunk LTM?\") +\n  facet_wrap(~itemtype) +\n  theme_pub()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThese are the best fitting parameters when running with 100 different starting points:\n\n\nCode\nfits1_e3 |&gt;\n  filter(exclude_sp1 == TRUE, priors_scenario == \"none\") |&gt;\n  arrange(deviance) |&gt;\n  select(prop:deviance) |&gt;\n  head(15) |&gt;\n  kableExtra::kable()\n\n\n\n\n\nprop\nprop_ltm\nrate\ntau\ngain\ndeviance\n\n\n\n\n0.1176852\n0.8038622\n0.0056074\n0.0928974\n69.325675\n764.8021\n\n\n0.1171605\n0.8039528\n0.0055792\n0.0925837\n69.905273\n764.8022\n\n\n0.1158791\n0.8034533\n0.0055264\n0.0918260\n71.369326\n764.8022\n\n\n0.1171228\n0.8034459\n0.0055762\n0.0925632\n69.947438\n764.8023\n\n\n0.1223134\n0.8039849\n0.0058120\n0.0955887\n64.418454\n764.8032\n\n\n0.1184461\n0.8040876\n0.0056514\n0.0933540\n68.472023\n764.8049\n\n\n0.1169663\n0.8037114\n0.0055752\n0.0924832\n70.135163\n764.8066\n\n\n0.1255808\n0.8051079\n0.0060152\n0.0974679\n61.297036\n764.8067\n\n\n0.1357685\n0.8044079\n0.0064193\n0.1030411\n52.924650\n764.8160\n\n\n0.1169577\n0.8037673\n0.0055755\n0.0924814\n70.102525\n764.8164\n\n\n0.1516215\n0.8060898\n0.0071830\n0.1111224\n43.004528\n764.8409\n\n\n0.5709167\n0.4530002\n0.2213147\n0.2424797\n3.249835\n848.5274\n\n\n0.5710025\n0.4530985\n0.2213565\n0.2424506\n3.248783\n848.5274\n\n\n0.5711107\n0.4530694\n0.2213929\n0.2424626\n3.247816\n848.5274\n\n\n0.5706589\n0.4531750\n0.2211694\n0.2424859\n3.252406\n848.5275\n\n\n\n\n\nThe fits require very slow rate again. Here’s the plot of the best fitting parameters:\n\n\nCode\nfits1_e3 |&gt;\n  filter(exp == 3, exclude_sp1 == TRUE, priors_scenario == \"none\") |&gt;\n  arrange(deviance) |&gt; \n  slice(1) |&gt;\n  select(data, pred) |&gt;\n  unnest(cols = everything()) |&gt;\n  ggplot(aes(gap, p_correct, color = chunk, group = chunk)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  geom_line(aes(y = pred), color = \"black\") +\n  scale_color_discrete(\"1st chunk LTM?\") +\n  facet_wrap(~itemtype) +\n  theme_pub()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nTrying priors of the gain parameter\nLike for Experiment 1, restrict the gain to be ~ 25\n\n\nCode\nfits1_e3 |&gt;\n  filter(priors_scenario == \"gain\", exclude_sp1 == TRUE, convergence == 0) |&gt;\n  select(prop:convergence) |&gt;\n  arrange(deviance) |&gt;\n  mutate_all(round, 3) |&gt;\n  print(n = 10)\n\n\n# A tibble: 100 × 7\n    prop prop_ltm  rate   tau  gain deviance convergence\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n 1 0.204    0.811  0.01 0.132  25.0     765.           0\n 2 0.204    0.81   0.01 0.132  25.0     765.           0\n 3 0.204    0.811  0.01 0.132  25       765.           0\n 4 0.204    0.811  0.01 0.132  25.0     765.           0\n 5 0.204    0.811  0.01 0.132  25       765.           0\n 6 0.204    0.81   0.01 0.132  25       765.           0\n 7 0.204    0.811  0.01 0.132  25.0     765.           0\n 8 0.204    0.811  0.01 0.132  25       765.           0\n 9 0.204    0.81   0.01 0.132  25.0     765.           0\n10 0.204    0.81   0.01 0.132  25.0     765.           0\n# ℹ 90 more rows\n\n\nplot of the best fitting parameters:\n\n\nCode\nfits1_e3 |&gt;\n  filter(priors_scenario == \"gain\", exclude_sp1 == TRUE, convergence == 0) |&gt;\n  arrange(deviance) |&gt; \n  slice(1) |&gt;\n  select(data, pred) |&gt;\n  unnest(cols = everything()) |&gt;\n  ggplot(aes(gap, p_correct, color = chunk, group = chunk)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  geom_line(aes(y = pred), color = \"black\") +\n  scale_color_discrete(\"1st chunk LTM?\") +\n  facet_wrap(~itemtype) +\n  theme_pub()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nTrying priors of the rate parameter\nLike for Experiment 1, rate parameter prior ~ Normal(0.1, 0.01)\n\n\nCode\nfits1_e3 |&gt;\n  filter(priors_scenario == \"rate\", exclude_sp1 == TRUE, convergence == 0) |&gt;\n  select(prop:convergence) |&gt;\n  arrange(deviance) |&gt;\n  mutate_all(round, 3) |&gt;\n  print(n = 10)\n\n\n# A tibble: 98 × 7\n    prop prop_ltm  rate   tau  gain deviance convergence\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n 1 0.404    0.831 0.021 0.153  7.74     770.           0\n 2 0.407    0.832 0.021 0.152  7.65     770.           0\n 3 0.408    0.832 0.021 0.152  7.63     770.           0\n 4 0.408    0.832 0.021 0.152  7.64     770.           0\n 5 0.408    0.832 0.021 0.152  7.62     770.           0\n 6 0.408    0.832 0.021 0.152  7.62     770.           0\n 7 0.408    0.832 0.021 0.152  7.62     770.           0\n 8 0.408    0.832 0.021 0.152  7.62     770.           0\n 9 0.408    0.832 0.021 0.152  7.62     770.           0\n10 0.409    0.832 0.021 0.152  7.61     770.           0\n# ℹ 88 more rows\n\n\nplot of the best fitting parameters:\n\n\nCode\nfits1_e3 |&gt;\n  filter(priors_scenario == \"rate\", exclude_sp1 == TRUE, convergence == 0) |&gt;\n  arrange(deviance) |&gt; \n  slice(1) |&gt;\n  select(data, pred) |&gt;\n  unnest(cols = everything()) |&gt;\n  ggplot(aes(gap, p_correct, color = chunk, group = chunk)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  geom_line(aes(y = pred), color = \"black\") +\n  scale_color_discrete(\"1st chunk LTM?\") +\n  facet_wrap(~itemtype) +\n  theme_pub()\n\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Experiment 3"
    ]
  },
  {
    "objectID": "docs/notebooks/exp3_model1.html#summary",
    "href": "docs/notebooks/exp3_model1.html#summary",
    "title": "Experiment 3",
    "section": "Summary",
    "text": "Summary\nThe linear recovery model cannot fit the interaction present in the data. As discussed elsewhere, this is because in order to capture the primacy effect and the global proactive benefit of free time, it needs low depletion and low recovery rates.",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Experiment 3"
    ]
  },
  {
    "objectID": "docs/notebooks/sensitivity-to-tau.html",
    "href": "docs/notebooks/sensitivity-to-tau.html",
    "title": "Sensitivity to tau",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(targets)\ntar_source()\ntar_load(exp1_data_agg)\n\n\nIn the current draft (May 12th), Eda modelled the data by ignoring the first chunk when calculating the likelihood.\nHere are the predictions using the parameters reported in the paper:\n\n\nCode\nstart &lt;- paper_params()\n\nexp1_data_agg$pred &lt;- predict(start, data = exp1_data_agg, group_by = c(\"chunk\", \"gap\"))\n\nexp1_data_agg |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  facet_wrap(~itemtype)\n\n\n\n\n\n\n\n\n\nStrangely, there is a mismatch between these results and what is reported in the paper.\nThis is because the predictions are extremely sensitive to small changes in tau:\n\n\nCode\nparams &lt;- list(start, start, start)\nparams[[2]][[\"tau\"]] &lt;- 0.15\nparams[[3]][[\"tau\"]] &lt;- 0.13\n\n\noverall_deviance(params[[1]], exp1_data_agg, by = c(\"chunk\", \"gap\"), exclude_sp1 = TRUE)\n\n\n[1] 116.5524\n\n\nCode\noverall_deviance(params[[2]], exp1_data_agg, by = c(\"chunk\", \"gap\"), exclude_sp1 = TRUE)\n\n\n[1] 38.11554\n\n\nCode\noverall_deviance(params[[3]], exp1_data_agg, by = c(\"chunk\", \"gap\"), exclude_sp1 = TRUE)\n\n\n[1] 395.7678\n\n\nCode\nexp1_data_agg$pred2 &lt;- predict(params[[2]], data = exp1_data_agg, group_by = c(\"chunk\", \"gap\"))\nexp1_data_agg$pred3 &lt;- predict(params[[3]], data = exp1_data_agg, group_by = c(\"chunk\", \"gap\"))\n\nexp1_data_agg |&gt;\n  pivot_longer(cols = starts_with(\"pred\"), names_to = \"tau\", values_to = \"pred\") |&gt;\n  mutate(tau = case_when(\n    tau == \"pred\" ~ \"0.14\",\n    tau == \"pred2\" ~ \"0.15\",\n    tau == \"pred3\" ~ \"0.13\"\n  )) |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  facet_grid(tau ~ itemtype)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Sensitivity to tau"
    ]
  },
  {
    "objectID": "docs/notebooks/model3_bootstrap.html",
    "href": "docs/notebooks/model3_bootstrap.html",
    "title": "Bootstrapping data and fits for parameter uncertainty estimation",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(targets)\nlibrary(GGally)\nlibrary(kableExtra)\n\n# load \"R/*\" scripts and saved R objects from the targets pipeline\ntar_source()\ntar_load(c(exp1_data, exp2_data, exp1_data_agg, exp2_data_agg, exp3_data_agg))\nset.seed(213)\n\n# make sure that if the model fails to converge, it doesn't stop the whole process\nsafe_boot_est &lt;- purrr::safely(boot_est)",
    "crumbs": [
      "Notebooks",
      "Model 3: Non-linear recovery",
      "Bootstrapping data and fits for parameter uncertainty estimation"
    ]
  },
  {
    "objectID": "docs/notebooks/model3_bootstrap.html#overview",
    "href": "docs/notebooks/model3_bootstrap.html#overview",
    "title": "Bootstrapping data and fits for parameter uncertainty estimation",
    "section": "Overview",
    "text": "Overview\nTo get an estimate of the uncertainty in the parameter estimates, we can use bootstrapping. This involves resampling the data with replacement and fitting the model to each resampled dataset. We do this:\n\nResample the IDs of participants in the dataset with replacement.\nFor each subject, calculate the proportion of correct responses in each condtion and resample the observed counts from a binomial distribution with the probability of success equal to the proportion of correct responses.\nEstimate the model parameters from the resampled data.\nRepeat steps 1-3 1000 times to get a distribution of parameter estimates.",
    "crumbs": [
      "Notebooks",
      "Model 3: Non-linear recovery",
      "Bootstrapping data and fits for parameter uncertainty estimation"
    ]
  },
  {
    "objectID": "docs/notebooks/model3_bootstrap.html#experiment-1",
    "href": "docs/notebooks/model3_bootstrap.html#experiment-1",
    "title": "Bootstrapping data and fits for parameter uncertainty estimation",
    "section": "Experiment 1",
    "text": "Experiment 1\nFor now I’ve done it just for experiment 1. Here are the results:\n\n\nCode\nres_asy &lt;- run_or_load(\n  do.call(rbind, replicate(1000, safe_boot_est(exp1_data)$result)),\n  \"output/res_boot1000_asy.rds\")\n\nplot_bootstrap_results(res_asy)\n\n\n\n\n\n\n\n\n\nand here are the 95% highest density intervals for the rate parameters\n\n\nCode\nround(HDInterval::hdi(res_asy$rate), 3)\n\n\nlower upper \n0.056 0.213 \nattr(,\"credMass\")\n[1] 0.95\n\n\nfrom these bootstrap estimates, we can calculate how long it would take for 50% of the resources to recover from 0 (also see here).\n\n\nCode\nt_est &lt;- -log(0.5) / res_asy$rate\n\nround(HDInterval::hdi(t_est), 3)\n\n\n lower  upper \n 2.799 10.676 \nattr(,\"credMass\")\n[1] 0.95\n\n\nIt would take on average \\(6.1883827\\) seconds for 50% of the resources to recover from 0, with a 95% HDI of \\(2.7987571, 10.67613\\).",
    "crumbs": [
      "Notebooks",
      "Model 3: Non-linear recovery",
      "Bootstrapping data and fits for parameter uncertainty estimation"
    ]
  },
  {
    "objectID": "docs/notebooks/modelling_edas_approach.html",
    "href": "docs/notebooks/modelling_edas_approach.html",
    "title": "Main results",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(targets)\nlibrary(GGally)\nlibrary(kableExtra)\n# load \"R/*\" scripts and saved R objects from the targets pi\ntar_source()\ntar_load(c(exp1_data, exp2_data, exp1_data_agg, exp2_data_agg, fits1))",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Main results"
    ]
  },
  {
    "objectID": "docs/notebooks/modelling_edas_approach.html#overview",
    "href": "docs/notebooks/modelling_edas_approach.html#overview",
    "title": "Main results",
    "section": "Overview",
    "text": "Overview\nHere I apply the model described in the May 13th draft of the paper to the data. I will first ignore the first chunk in the optimization, then include it. I will also try different priors on the parameters to understand the paramater space. Final results from different choices summarized at the end.",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Main results"
    ]
  },
  {
    "objectID": "docs/notebooks/modelling_edas_approach.html#ignoring-first-chunk-in-the-optimiziation",
    "href": "docs/notebooks/modelling_edas_approach.html#ignoring-first-chunk-in-the-optimiziation",
    "title": "Main results",
    "section": "Ignoring first chunk in the optimiziation",
    "text": "Ignoring first chunk in the optimiziation\n\nBasic estimation of Exp1\nLet’s apply the modeling approach reported in the paper. We ignore the first chunk (SP1-3) while evaluating the likelihood. Eda did this because the model as implemented predicts the same performance for known and random chunks.\n\n\nCode\ntar_load(exp1_data_agg)\nstart &lt;- paper_params()\n(est &lt;- estimate_model(start, data = exp1_data_agg, exclude_sp1 = TRUE))\n\n\n$start\n    prop prop_ltm      tau     gain     rate \n    0.21     0.55     0.14    25.00     0.02 \n\n$par\n        prop     prop_ltm          tau         gain         rate \n 0.108898668  0.575632670  0.091783835 95.188660503  0.009362919 \n\n$convergence\n[1] 0\n\n$counts\nfunction gradient \n    2080       NA \n\n$value\n[1] 31.92645\n\n\nCode\nexp1_data_agg$pred &lt;- predict(est, exp1_data_agg, group_by = c(\"chunk\", \"gap\"))\nexp1_data_agg |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n\n\n\n\n\n\n\n\n\nI get startlingly different paramemter estimates. Much lower prop and rate and tau, higher gain.\n\n\nTrying different starting values\n\n\nCode\n# load the fits of the first simulation, calculate the deviance(s) and predictions\ntar_load(fits1)\nfits1 &lt;- fits1 |&gt;\n  mutate(\n    deviance = pmap_dbl(\n      list(fit, data, exclude_sp1),\n      ~ overall_deviance(params = `..1`$par, data = `..2`, exclude_sp1 = `..3`)\n    ),\n    pred = map2(fit, data, ~ predict(.x, .y, group_by = c(\"chunk\", \"gap\")))\n  )\n\n\nI’ve run this with many different starting values. We tend to end up in different regions of the parameter space (the top result close to the paper’s estimates):\n\n\nCode\nfits1 |&gt;\n  filter(priors_scenario == \"none\", exclude_sp1 == TRUE, exp == 1, deviance &lt;= 50, convergence == 0) |&gt;\n  select(prop:convergence) |&gt;\n  arrange(gain) |&gt;\n  mutate_all(round, 3) |&gt;\n  print(n = 100)\n\n\n# A tibble: 55 × 7\n    prop prop_ltm  rate   tau  gain deviance convergence\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n 1 0.176    0.582 0.015 0.133  38.2     32.4           0\n 2 0.175    0.578 0.015 0.132  38.8     32.4           0\n 3 0.171    0.582 0.015 0.13   40.3     32.4           0\n 4 0.165    0.581 0.014 0.127  43.1     32.3           0\n 5 0.137    0.578 0.012 0.11   61.8     32.1           0\n 6 0.126    0.577 0.011 0.103  71.8     32.0           0\n 7 0.125    0.577 0.011 0.103  72.6     32.0           0\n 8 0.123    0.577 0.011 0.101  75.2     32.0           0\n 9 0.118    0.576 0.01  0.098  81.8     32.0           0\n10 0.118    0.577 0.01  0.098  82.0     32.0           0\n11 0.117    0.576 0.01  0.097  83.3     32.0           0\n12 0.106    0.575 0.009 0.09   99.4     31.9           0\n13 0.106    0.575 0.009 0.09   99.6     31.9           0\n14 0.106    0.576 0.009 0.09   99.7     31.9           0\n15 0.106    0.575 0.009 0.09   99.8     31.9           0\n16 0.106    0.576 0.009 0.09   99.8     31.9           0\n17 0.106    0.576 0.009 0.09   99.9     31.9           0\n18 0.106    0.575 0.009 0.09   99.9     31.9           0\n19 0.106    0.576 0.009 0.09   99.9     31.9           0\n20 0.106    0.575 0.009 0.09   99.9     31.9           0\n21 0.106    0.575 0.009 0.09   99.9     31.9           0\n22 0.106    0.575 0.009 0.09  100.      31.9           0\n23 0.106    0.576 0.009 0.09  100.      31.9           0\n24 0.106    0.576 0.009 0.09  100.      31.9           0\n25 0.106    0.576 0.009 0.09  100.      31.9           0\n26 0.106    0.576 0.009 0.09  100.      31.9           0\n27 0.106    0.576 0.009 0.09  100.      31.9           0\n28 0.106    0.577 0.009 0.09  100.      31.9           0\n29 0.106    0.575 0.009 0.09  100.      31.9           0\n30 0.106    0.576 0.009 0.09  100.      31.9           0\n31 0.106    0.574 0.009 0.09  100.      31.9           0\n32 0.106    0.575 0.009 0.09  100.      31.9           0\n33 0.106    0.576 0.009 0.09  100.      31.9           0\n34 0.106    0.575 0.009 0.09  100.      31.9           0\n35 0.106    0.575 0.009 0.09  100.      31.9           0\n36 0.106    0.576 0.009 0.09  100.      31.9           0\n37 0.106    0.574 0.009 0.09  100.      31.9           0\n38 0.106    0.576 0.009 0.09  100.      31.9           0\n39 0.106    0.576 0.009 0.09  100.      31.9           0\n40 0.106    0.575 0.009 0.09  100.      31.9           0\n41 0.106    0.576 0.009 0.09  100.      31.9           0\n42 0.106    0.575 0.009 0.09  100.      31.9           0\n43 0.106    0.575 0.009 0.09  100.      31.9           0\n44 0.106    0.574 0.009 0.09  100.      31.9           0\n45 0.106    0.576 0.009 0.09  100.      31.9           0\n46 0.106    0.576 0.009 0.09  100.      31.9           0\n47 0.106    0.575 0.009 0.09  100       31.9           0\n48 0.106    0.575 0.009 0.09  100       31.9           0\n49 0.106    0.575 0.009 0.09  100       31.9           0\n50 0.106    0.575 0.009 0.09  100       31.9           0\n51 0.106    0.576 0.009 0.09  100       31.9           0\n52 0.106    0.576 0.009 0.09  100       31.9           0\n53 0.106    0.576 0.009 0.09  100       31.9           0\n54 0.106    0.575 0.009 0.09  100       31.9           0\n55 0.106    0.571 0.009 0.09  100       31.9           0\n\n\n\n\nTrying priors of the gain parameter\nOne way to deal with that is to put a prior on the gain parameter to keep it near 25. I know priors are usually a bayesian thing, but they work with ML optimization just as well. On the next set of simulations, I used a Normal(25, 0.1) prior on the gain parameter (could have also fixed it to this value, but this gives me mroe control).\n\n\nCode\nfits1 |&gt;\n  filter(priors_scenario == \"gain\", exclude_sp1 == TRUE, exp == 1, deviance &lt;= 50, convergence == 0) |&gt;\n  select(prop:convergence) |&gt;\n  arrange(gain) |&gt;\n  mutate_all(round, 3) |&gt;\n  print(n = 100)\n\n\n# A tibble: 23 × 7\n    prop prop_ltm  rate   tau  gain deviance convergence\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n 1 0.222    0.587 0.019 0.154  25.0     33.0           0\n 2 0.222    0.588 0.018 0.154  25.0     33.0           0\n 3 0.222    0.588 0.019 0.154  25       33.0           0\n 4 0.222    0.588 0.019 0.154  25       33.0           0\n 5 0.222    0.588 0.019 0.154  25       33.0           0\n 6 0.222    0.588 0.019 0.154  25       33.0           0\n 7 0.222    0.588 0.019 0.154  25       33.0           0\n 8 0.222    0.588 0.019 0.154  25       33.0           0\n 9 0.222    0.588 0.019 0.154  25.0     33.0           0\n10 0.222    0.588 0.019 0.154  25.0     33.0           0\n11 0.222    0.588 0.019 0.154  25.0     33.0           0\n12 0.222    0.588 0.019 0.154  25.0     33.0           0\n13 0.222    0.588 0.019 0.154  25.0     33.0           0\n14 0.208    0.398 0.031 0.154  25.0     47.7           0\n15 0.222    0.587 0.019 0.154  25.0     33.0           0\n16 0.222    0.587 0.019 0.154  25.0     33.0           0\n17 0.222    0.587 0.019 0.154  25.0     33.0           0\n18 0.222    0.588 0.019 0.154  25.0     33.0           0\n19 0.222    0.588 0.019 0.154  25.0     33.0           0\n20 0.222    0.588 0.019 0.154  25.0     33.0           0\n21 0.222    0.588 0.019 0.154  25.0     33.0           0\n22 0.222    0.588 0.019 0.154  25.0     33.0           0\n23 0.222    0.588 0.019 0.154  25.0     33.0           0\n\n\nSo we do get at least some parameters that are close to that reported in the paper. The predictions with those parameters:\n\n\nCode\nexp1_data_agg$pred &lt;- fits1 |&gt;\n  filter(priors_scenario == \"gain\", exclude_sp1 == TRUE, exp == 1, convergence == 0) |&gt;\n  arrange(deviance) |&gt;\n  pluck(\"pred\", 1)\n\nexp1_data_agg |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n\n\n\n\n\n\n\n\n\n\n\nTrying priors on the rate parameter\nMaybe there is a region with a higher rate that we have not explored? Let’s try a prior on the rate parameter, ~ Normal(0.1, 0.01).\n\n\nCode\nfits1 |&gt;\n  filter(priors_scenario == \"rate\", exclude_sp1 == TRUE, exp == 1, convergence == 0) |&gt;\n  select(prop:convergence) |&gt;\n  arrange(deviance) |&gt;\n  mutate_all(round, 3) |&gt;\n  print(n = 100)\n\n\n# A tibble: 100 × 7\n     prop prop_ltm  rate   tau  gain deviance convergence\n    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n  1 0.435    0.608 0.049 0.202  7.59     43.1           0\n  2 0.434    0.607 0.049 0.202  7.60     43.1           0\n  3 0.435    0.607 0.049 0.202  7.59     43.1           0\n  4 0.435    0.608 0.049 0.202  7.59     43.1           0\n  5 0.435    0.607 0.049 0.202  7.59     43.1           0\n  6 0.435    0.608 0.049 0.202  7.59     43.1           0\n  7 0.434    0.607 0.049 0.202  7.59     43.1           0\n  8 0.435    0.607 0.049 0.202  7.58     43.1           0\n  9 0.435    0.607 0.049 0.202  7.59     43.1           0\n 10 0.435    0.607 0.049 0.202  7.59     43.1           0\n 11 0.435    0.608 0.049 0.202  7.59     43.1           0\n 12 0.435    0.607 0.049 0.202  7.59     43.1           0\n 13 0.435    0.607 0.049 0.202  7.58     43.1           0\n 14 0.435    0.607 0.049 0.202  7.59     43.1           0\n 15 0.435    0.607 0.049 0.202  7.59     43.1           0\n 16 0.435    0.607 0.049 0.202  7.58     43.1           0\n 17 0.435    0.607 0.049 0.202  7.58     43.1           0\n 18 0.435    0.607 0.049 0.202  7.59     43.1           0\n 19 0.435    0.607 0.049 0.202  7.59     43.1           0\n 20 0.435    0.607 0.049 0.202  7.59     43.1           0\n 21 0.435    0.607 0.049 0.202  7.58     43.1           0\n 22 0.435    0.608 0.049 0.202  7.59     43.1           0\n 23 0.435    0.607 0.049 0.202  7.58     43.1           0\n 24 0.435    0.607 0.049 0.202  7.58     43.1           0\n 25 0.435    0.607 0.049 0.202  7.59     43.1           0\n 26 0.435    0.607 0.049 0.202  7.59     43.1           0\n 27 0.435    0.607 0.049 0.202  7.58     43.1           0\n 28 0.435    0.607 0.049 0.202  7.58     43.1           0\n 29 0.435    0.607 0.049 0.202  7.58     43.1           0\n 30 0.435    0.607 0.049 0.202  7.58     43.1           0\n 31 0.435    0.607 0.049 0.202  7.59     43.1           0\n 32 0.435    0.607 0.049 0.202  7.58     43.1           0\n 33 0.435    0.607 0.049 0.202  7.58     43.1           0\n 34 0.435    0.607 0.049 0.202  7.58     43.1           0\n 35 0.435    0.607 0.049 0.202  7.58     43.1           0\n 36 0.435    0.607 0.049 0.202  7.58     43.1           0\n 37 0.435    0.607 0.049 0.202  7.59     43.1           0\n 38 0.435    0.607 0.049 0.202  7.59     43.1           0\n 39 0.435    0.607 0.049 0.202  7.58     43.1           0\n 40 0.435    0.607 0.049 0.202  7.58     43.1           0\n 41 0.435    0.607 0.049 0.202  7.58     43.1           0\n 42 0.435    0.607 0.049 0.202  7.59     43.1           0\n 43 0.435    0.607 0.049 0.202  7.58     43.1           0\n 44 0.435    0.607 0.049 0.202  7.59     43.1           0\n 45 0.435    0.607 0.049 0.202  7.58     43.1           0\n 46 0.435    0.607 0.049 0.202  7.59     43.1           0\n 47 0.435    0.607 0.049 0.202  7.58     43.1           0\n 48 0.435    0.608 0.049 0.202  7.58     43.1           0\n 49 0.435    0.607 0.049 0.202  7.58     43.1           0\n 50 0.435    0.608 0.049 0.202  7.58     43.1           0\n 51 0.435    0.607 0.049 0.202  7.58     43.1           0\n 52 0.435    0.607 0.049 0.202  7.59     43.1           0\n 53 0.435    0.607 0.049 0.202  7.58     43.1           0\n 54 0.435    0.607 0.049 0.202  7.59     43.1           0\n 55 0.435    0.607 0.049 0.202  7.58     43.1           0\n 56 0.435    0.607 0.049 0.202  7.58     43.1           0\n 57 0.435    0.607 0.049 0.202  7.58     43.1           0\n 58 0.435    0.607 0.049 0.202  7.58     43.1           0\n 59 0.435    0.607 0.049 0.202  7.58     43.1           0\n 60 0.435    0.607 0.049 0.202  7.58     43.1           0\n 61 0.435    0.607 0.049 0.202  7.58     43.1           0\n 62 0.435    0.607 0.049 0.202  7.58     43.1           0\n 63 0.435    0.608 0.049 0.202  7.59     43.1           0\n 64 0.435    0.608 0.049 0.202  7.58     43.1           0\n 65 0.435    0.607 0.049 0.202  7.58     43.1           0\n 66 0.435    0.607 0.049 0.202  7.58     43.1           0\n 67 0.435    0.607 0.049 0.202  7.58     43.1           0\n 68 0.435    0.607 0.049 0.202  7.58     43.1           0\n 69 0.435    0.607 0.049 0.202  7.58     43.1           0\n 70 0.435    0.607 0.049 0.202  7.58     43.1           0\n 71 0.435    0.607 0.049 0.202  7.58     43.2           0\n 72 0.435    0.608 0.049 0.202  7.58     43.2           0\n 73 0.435    0.607 0.049 0.202  7.58     43.2           0\n 74 0.435    0.607 0.049 0.202  7.58     43.2           0\n 75 0.435    0.607 0.049 0.202  7.58     43.2           0\n 76 0.435    0.608 0.049 0.202  7.58     43.2           0\n 77 0.435    0.608 0.049 0.202  7.58     43.2           0\n 78 0.435    0.607 0.049 0.202  7.58     43.2           0\n 79 0.435    0.608 0.049 0.202  7.57     43.2           0\n 80 0.435    0.607 0.049 0.202  7.57     43.2           0\n 81 0.39     0.375 0.071 0.218  7.90     55.8           0\n 82 0.39     0.375 0.071 0.218  7.88     55.8           0\n 83 0.39     0.375 0.071 0.218  7.88     55.8           0\n 84 0.39     0.375 0.071 0.218  7.88     55.8           0\n 85 0.39     0.375 0.071 0.218  7.87     55.8           0\n 86 0.39     0.375 0.071 0.218  7.88     55.8           0\n 87 0.39     0.375 0.071 0.218  7.88     55.8           0\n 88 0.39     0.375 0.071 0.218  7.87     55.8           0\n 89 0.39     0.375 0.071 0.218  7.87     55.8           0\n 90 0.39     0.375 0.071 0.218  7.88     55.8           0\n 91 0.39     0.375 0.071 0.218  7.87     55.8           0\n 92 0.39     0.375 0.071 0.218  7.87     55.8           0\n 93 0.39     0.375 0.071 0.218  7.87     55.8           0\n 94 0.39     0.375 0.071 0.218  7.87     55.8           0\n 95 0.39     0.375 0.071 0.218  7.88     55.8           0\n 96 0.39     0.375 0.071 0.218  7.87     55.8           0\n 97 0.39     0.375 0.071 0.218  7.87     55.8           0\n 98 0.391    0.375 0.071 0.218  7.86     55.8           0\n 99 0.39     0.375 0.071 0.218  7.87     55.8           0\n100 0.386    0.017 0.081 0.226  7.39     75.0           0\n\n\nDeviance is quite much higher. Predictions?\n\n\nCode\nfit &lt;- fits1 |&gt;\n  filter(priors_scenario == \"rate\", exclude_sp1 == TRUE, exp == 1, convergence == 0) |&gt;\n  arrange(deviance) |&gt;\n  pluck(\"fit\", 1)\n\nfit$par\n\n\n      prop   prop_ltm       rate        tau       gain \n0.43456497 0.60758909 0.04906632 0.20157962 7.58996852 \n\n\nCode\nexp1_data_agg$pred &lt;- predict(fit, exp1_data_agg, group_by = c(\"chunk\", \"gap\"))\n\nexp1_data_agg |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n\n\n\n\n\n\n\n\n\nGeater mismatch. Let’s include error bars of the data:\n\n\nCode\nexp1_data |&gt;\n  group_by(id, chunk, gap, itemtype) |&gt;\n  summarise(\n    n_total = dplyr::n(),\n    n_correct = sum(cor),\n    p_correct = mean(cor)\n  ) |&gt;\n  ungroup() |&gt;\n  left_join(\n    select(exp1_data_agg, chunk, gap, itemtype, pred),\n    by = c(\"chunk\", \"gap\", \"itemtype\")\n  ) |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  stat_summary() +\n  stat_summary(aes(y = pred), linetype = \"dashed\", geom = \"line\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n\n\n`summarise()` has grouped output by 'id', 'chunk', 'gap'. You can override using the `.groups` argument.\nNo summary function supplied, defaulting to `mean_se()`\nNo summary function supplied, defaulting to `mean_se()`\nNo summary function supplied, defaulting to `mean_se()`\nNo summary function supplied, defaulting to `mean_se()`\nNo summary function supplied, defaulting to `mean_se()`\nNo summary function supplied, defaulting to `mean_se()`\n\n\n\n\n\n\n\n\n\nParaneters seem consistent with the data (see my notes).",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Main results"
    ]
  },
  {
    "objectID": "docs/notebooks/modelling_edas_approach.html#including-the-first-chunk-in-the-optimization",
    "href": "docs/notebooks/modelling_edas_approach.html#including-the-first-chunk-in-the-optimization",
    "title": "Main results",
    "section": "Including the first chunk in the optimization",
    "text": "Including the first chunk in the optimization\nThe reports above followed the approach in the current draft and excluded the first chunk from the calculation of the likelihood when optimizing the parameters. Let’s include it:\n\n\nCode\nstart &lt;- paper_params()\n(est &lt;- estimate_model(start, data = exp1_data_agg, exclude_sp1 = FALSE))\n\n\n$start\n    prop prop_ltm      tau     gain     rate \n    0.21     0.55     0.14    25.00     0.02 \n\n$par\n       prop    prop_ltm         tau        gain        rate \n 0.30248729  0.63594776  0.17698525 15.21043755  0.02143605 \n\n$convergence\n[1] 0\n\n$counts\nfunction gradient \n    1276       NA \n\n$value\n[1] 144.8785\n\n\nCode\nexp1_data_agg$pred &lt;- predict(est, exp1_data_agg, group_by = c(\"chunk\", \"gap\"))\nexp1_data_agg |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n\n\n\n\n\n\n\n\n\nIn this case I didn’t have to use many starting values - the result is reached from almost everywhere:\n\n\nCode\nfits1 |&gt;\n  filter(priors_scenario == \"none\", exclude_sp1 == FALSE, exp == 1, convergence == 0) |&gt;\n  arrange(deviance) |&gt;\n  select(prop:convergence) |&gt;\n  print(n = 100)\n\n\n# A tibble: 100 × 7\n     prop prop_ltm   rate    tau  gain deviance convergence\n    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt;\n  1 0.303    0.636 0.0214 0.177  15.2      145.           0\n  2 0.302    0.636 0.0214 0.177  15.2      145.           0\n  3 0.302    0.636 0.0214 0.177  15.2      145.           0\n  4 0.302    0.636 0.0214 0.177  15.2      145.           0\n  5 0.302    0.636 0.0214 0.177  15.2      145.           0\n  6 0.302    0.636 0.0214 0.177  15.2      145.           0\n  7 0.302    0.636 0.0214 0.177  15.2      145.           0\n  8 0.302    0.636 0.0214 0.177  15.2      145.           0\n  9 0.302    0.636 0.0214 0.177  15.2      145.           0\n 10 0.303    0.636 0.0214 0.177  15.2      145.           0\n 11 0.302    0.636 0.0214 0.177  15.2      145.           0\n 12 0.302    0.636 0.0214 0.177  15.2      145.           0\n 13 0.303    0.636 0.0214 0.177  15.2      145.           0\n 14 0.303    0.636 0.0214 0.177  15.2      145.           0\n 15 0.302    0.636 0.0214 0.177  15.2      145.           0\n 16 0.302    0.636 0.0214 0.177  15.2      145.           0\n 17 0.302    0.636 0.0214 0.177  15.2      145.           0\n 18 0.302    0.636 0.0214 0.177  15.2      145.           0\n 19 0.302    0.636 0.0214 0.177  15.2      145.           0\n 20 0.302    0.636 0.0214 0.177  15.2      145.           0\n 21 0.302    0.636 0.0214 0.177  15.2      145.           0\n 22 0.303    0.636 0.0215 0.177  15.2      145.           0\n 23 0.302    0.636 0.0214 0.177  15.2      145.           0\n 24 0.302    0.636 0.0214 0.177  15.2      145.           0\n 25 0.302    0.636 0.0214 0.177  15.2      145.           0\n 26 0.303    0.636 0.0214 0.177  15.2      145.           0\n 27 0.303    0.636 0.0215 0.177  15.2      145.           0\n 28 0.303    0.636 0.0214 0.177  15.2      145.           0\n 29 0.303    0.636 0.0215 0.177  15.2      145.           0\n 30 0.303    0.636 0.0215 0.177  15.2      145.           0\n 31 0.303    0.636 0.0214 0.177  15.2      145.           0\n 32 0.302    0.636 0.0214 0.177  15.2      145.           0\n 33 0.302    0.636 0.0214 0.177  15.2      145.           0\n 34 0.302    0.636 0.0214 0.177  15.2      145.           0\n 35 0.302    0.636 0.0214 0.177  15.2      145.           0\n 36 0.303    0.636 0.0214 0.177  15.2      145.           0\n 37 0.303    0.636 0.0214 0.177  15.2      145.           0\n 38 0.302    0.636 0.0214 0.177  15.2      145.           0\n 39 0.302    0.636 0.0214 0.177  15.2      145.           0\n 40 0.303    0.636 0.0214 0.177  15.2      145.           0\n 41 0.303    0.636 0.0214 0.177  15.2      145.           0\n 42 0.302    0.636 0.0214 0.177  15.2      145.           0\n 43 0.302    0.636 0.0214 0.177  15.2      145.           0\n 44 0.303    0.636 0.0215 0.177  15.2      145.           0\n 45 0.303    0.636 0.0214 0.177  15.2      145.           0\n 46 0.302    0.636 0.0214 0.177  15.2      145.           0\n 47 0.302    0.636 0.0214 0.177  15.2      145.           0\n 48 0.302    0.636 0.0214 0.177  15.2      145.           0\n 49 0.303    0.636 0.0214 0.177  15.2      145.           0\n 50 0.302    0.636 0.0214 0.177  15.2      145.           0\n 51 0.302    0.636 0.0214 0.177  15.3      145.           0\n 52 0.303    0.636 0.0215 0.177  15.2      145.           0\n 53 0.303    0.636 0.0215 0.177  15.2      145.           0\n 54 0.302    0.636 0.0214 0.177  15.2      145.           0\n 55 0.302    0.636 0.0214 0.177  15.2      145.           0\n 56 0.303    0.636 0.0214 0.177  15.2      145.           0\n 57 0.303    0.636 0.0215 0.177  15.1      145.           0\n 58 0.302    0.636 0.0214 0.177  15.2      145.           0\n 59 0.302    0.636 0.0214 0.177  15.2      145.           0\n 60 0.302    0.636 0.0214 0.177  15.2      145.           0\n 61 0.303    0.636 0.0214 0.177  15.2      145.           0\n 62 0.303    0.636 0.0215 0.177  15.2      145.           0\n 63 0.302    0.636 0.0214 0.177  15.2      145.           0\n 64 0.302    0.636 0.0214 0.177  15.2      145.           0\n 65 0.302    0.636 0.0214 0.177  15.2      145.           0\n 66 0.302    0.636 0.0214 0.177  15.2      145.           0\n 67 0.302    0.636 0.0214 0.177  15.2      145.           0\n 68 0.302    0.636 0.0214 0.177  15.2      145.           0\n 69 0.303    0.636 0.0214 0.177  15.2      145.           0\n 70 0.303    0.636 0.0214 0.177  15.2      145.           0\n 71 0.303    0.636 0.0214 0.177  15.2      145.           0\n 72 0.302    0.636 0.0214 0.177  15.2      145.           0\n 73 0.302    0.636 0.0214 0.177  15.2      145.           0\n 74 0.302    0.636 0.0214 0.177  15.2      145.           0\n 75 0.302    0.636 0.0214 0.177  15.2      145.           0\n 76 0.303    0.636 0.0214 0.177  15.2      145.           0\n 77 0.302    0.636 0.0214 0.177  15.3      145.           0\n 78 0.548    0.515 0.258  0.285   5.92     381.           0\n 79 0.548    0.534 0.291  0.295   6.16     381.           0\n 80 0.548    0.512 0.253  0.283   5.89     381.           0\n 81 0.548    0.535 0.294  0.296   6.18     381.           0\n 82 0.549    0.551 0.321  0.305   6.40     381.           0\n 83 0.549    0.491 0.217  0.272   5.64     381.           0\n 84 0.549    0.549 0.318  0.304   6.37     381.           0\n 85 0.549    0.569 0.353  0.315   6.67     381.           0\n 86 0.548    0.536 0.294  0.296   6.18     381.           0\n 87 0.548    0.512 0.254  0.283   5.89     381.           0\n 88 0.548    0.575 0.362  0.317   6.76     381.           0\n 89 0.548    0.492 0.220  0.273   5.66     381.           0\n 90 0.548    0.515 0.260  0.285   5.93     381.           0\n 91 0.549    0.554 0.327  0.306   6.44     381.           0\n 92 0.548    0.604 0.413  0.334   7.26     381.           0\n 93 0.548    0.517 0.263  0.286   5.95     381.           0\n 94 0.548    0.565 0.345  0.312   6.60     381.           0\n 95 0.548    0.540 0.303  0.299   6.25     381.           0\n 96 0.548    0.513 0.256  0.284   5.91     381.           0\n 97 0.548    0.560 0.337  0.309   6.53     381.           0\n 98 0.549    0.556 0.330  0.307   6.48     381.           0\n 99 0.548    0.556 0.330  0.308   6.48     381.           0\n100 0.106    0.473 0.275  0.0736 24.3     1663.           0\n\n\n\nWith prior on rate\n\n\nCode\nfits1 |&gt;\n  filter(priors_scenario == \"rate\", exclude_sp1 == FALSE, exp == 1, convergence == 0) |&gt;\n  mutate(deviance = map2_dbl(fit, exclude_sp1, function(x, y) {\n    overall_deviance(x$par, exp1_data_agg, exclude_sp1 = y)\n  })) |&gt;\n  select(prop:convergence) |&gt;\n  arrange(deviance) |&gt;\n  mutate_all(round, 3) |&gt;\n  print(n = 100)\n\n\n# A tibble: 98 × 7\n    prop prop_ltm  rate   tau  gain deviance convergence\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n 1 0.475    0.645 0.049 0.197  6.91     153.           0\n 2 0.475    0.645 0.049 0.197  6.91     153.           0\n 3 0.475    0.645 0.049 0.197  6.91     153.           0\n 4 0.475    0.645 0.049 0.197  6.91     153.           0\n 5 0.475    0.645 0.049 0.197  6.91     153.           0\n 6 0.475    0.645 0.049 0.197  6.91     153.           0\n 7 0.475    0.644 0.049 0.197  6.90     153.           0\n 8 0.475    0.644 0.049 0.197  6.91     153.           0\n 9 0.475    0.644 0.049 0.197  6.91     153.           0\n10 0.475    0.645 0.049 0.197  6.90     153.           0\n11 0.475    0.644 0.049 0.197  6.91     153.           0\n12 0.475    0.645 0.049 0.197  6.91     153.           0\n13 0.475    0.645 0.049 0.197  6.91     153.           0\n14 0.475    0.644 0.049 0.197  6.91     153.           0\n15 0.475    0.645 0.049 0.197  6.91     153.           0\n16 0.475    0.645 0.049 0.197  6.91     153.           0\n17 0.475    0.644 0.049 0.197  6.91     153.           0\n18 0.475    0.645 0.049 0.197  6.91     153.           0\n19 0.475    0.645 0.049 0.197  6.91     153.           0\n20 0.475    0.645 0.049 0.197  6.91     153.           0\n21 0.475    0.645 0.049 0.197  6.90     153.           0\n22 0.475    0.644 0.049 0.197  6.91     153.           0\n23 0.475    0.645 0.049 0.197  6.91     153.           0\n24 0.475    0.645 0.049 0.197  6.90     153.           0\n25 0.475    0.645 0.049 0.197  6.90     153.           0\n26 0.475    0.645 0.049 0.197  6.90     153.           0\n27 0.475    0.645 0.049 0.197  6.91     153.           0\n28 0.475    0.645 0.049 0.197  6.90     153.           0\n29 0.475    0.645 0.049 0.197  6.91     153.           0\n30 0.475    0.645 0.049 0.197  6.91     153.           0\n31 0.475    0.644 0.049 0.197  6.91     153.           0\n32 0.475    0.645 0.049 0.197  6.91     153.           0\n33 0.475    0.645 0.049 0.197  6.91     153.           0\n34 0.475    0.645 0.049 0.197  6.91     153.           0\n35 0.475    0.645 0.049 0.197  6.90     153.           0\n36 0.475    0.644 0.049 0.197  6.91     153.           0\n37 0.475    0.645 0.049 0.197  6.91     153.           0\n38 0.475    0.645 0.049 0.197  6.90     153.           0\n39 0.475    0.645 0.049 0.197  6.91     153.           0\n40 0.475    0.644 0.049 0.197  6.90     153.           0\n41 0.475    0.645 0.049 0.197  6.91     153.           0\n42 0.475    0.645 0.049 0.197  6.90     153.           0\n43 0.475    0.644 0.049 0.197  6.91     153.           0\n44 0.475    0.645 0.049 0.197  6.90     153.           0\n45 0.475    0.645 0.049 0.197  6.90     153.           0\n46 0.475    0.645 0.049 0.197  6.90     153.           0\n47 0.475    0.644 0.049 0.197  6.91     153.           0\n48 0.475    0.645 0.049 0.197  6.91     153.           0\n49 0.475    0.645 0.049 0.197  6.90     153.           0\n50 0.475    0.645 0.049 0.197  6.90     153.           0\n51 0.475    0.645 0.049 0.197  6.91     153.           0\n52 0.475    0.645 0.049 0.197  6.91     153.           0\n53 0.475    0.644 0.049 0.197  6.91     153.           0\n54 0.475    0.645 0.049 0.197  6.90     153.           0\n55 0.475    0.645 0.049 0.197  6.91     153.           0\n56 0.475    0.644 0.049 0.197  6.91     153.           0\n57 0.475    0.645 0.049 0.197  6.90     153.           0\n58 0.475    0.644 0.049 0.197  6.91     153.           0\n59 0.475    0.644 0.049 0.197  6.90     153.           0\n60 0.475    0.645 0.049 0.197  6.91     153.           0\n61 0.475    0.645 0.049 0.197  6.90     153.           0\n62 0.475    0.645 0.049 0.197  6.90     153.           0\n63 0.475    0.645 0.049 0.197  6.90     153.           0\n64 0.475    0.645 0.049 0.197  6.90     153.           0\n65 0.475    0.645 0.049 0.197  6.90     153.           0\n66 0.475    0.645 0.049 0.197  6.91     153.           0\n67 0.475    0.645 0.049 0.197  6.90     153.           0\n68 0.475    0.645 0.049 0.197  6.91     153.           0\n69 0.475    0.644 0.049 0.197  6.90     153.           0\n70 0.475    0.645 0.049 0.197  6.90     153.           0\n71 0.475    0.645 0.049 0.197  6.90     153.           0\n72 0.476    0.644 0.049 0.197  6.89     153.           0\n73 0.475    0.645 0.049 0.197  6.91     153.           0\n74 0.475    0.645 0.049 0.197  6.90     153.           0\n75 0.475    0.645 0.049 0.197  6.90     153.           0\n76 0.475    0.645 0.049 0.197  6.90     153.           0\n77 0.475    0.644 0.049 0.197  6.90     153.           0\n78 0.475    0.644 0.049 0.197  6.90     153.           0\n79 0.475    0.644 0.049 0.197  6.91     153.           0\n80 0.475    0.645 0.049 0.197  6.90     153.           0\n81 0.475    0.645 0.049 0.197  6.90     153.           0\n82 0.475    0.644 0.049 0.197  6.90     153.           0\n83 0.475    0.645 0.049 0.197  6.91     153.           0\n84 0.475    0.645 0.049 0.197  6.90     153.           0\n85 0.475    0.645 0.049 0.197  6.91     153.           0\n86 0.475    0.645 0.049 0.197  6.90     153.           0\n87 0.475    0.645 0.049 0.197  6.90     153.           0\n88 0.476    0.644 0.049 0.197  6.90     153.           0\n89 0.475    0.645 0.049 0.197  6.90     153.           0\n90 0.475    0.644 0.049 0.197  6.90     153.           0\n91 0.475    0.645 0.049 0.197  6.90     153.           0\n92 0.476    0.644 0.049 0.197  6.9      153.           0\n93 0.475    0.645 0.049 0.197  6.91     153.           0\n94 0.476    0.645 0.049 0.197  6.90     153.           0\n95 0.475    0.645 0.049 0.197  6.90     153.           0\n96 0.476    0.644 0.049 0.197  6.90     153.           0\n97 0.476    0.645 0.049 0.197  6.89     153.           0\n98 0.476    0.645 0.049 0.197  6.90     153.           0\n\n\nCode\nfit &lt;- fits1 |&gt;\n  filter(priors_scenario == \"rate\", exclude_sp1 == FALSE, exp == 1, convergence == 0) |&gt;\n  arrange(deviance) |&gt;\n  pluck(\"fit\", 1)\n\nexp1_data_agg$pred &lt;- predict(fit, exp1_data_agg, group_by = c(\"chunk\", \"gap\"))\n\nexp1_data_agg |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Main results"
    ]
  },
  {
    "objectID": "docs/notebooks/modelling_edas_approach.html#repeat-for-expereiment-2",
    "href": "docs/notebooks/modelling_edas_approach.html#repeat-for-expereiment-2",
    "title": "Main results",
    "section": "Repeat for expereiment 2",
    "text": "Repeat for expereiment 2\nBasic estimation (ignoring first chunk):\n\n\nCode\ntar_load(exp2_data_agg)\nstart &lt;- paper_params(exp = 2)\n(est &lt;- estimate_model(start, data = exp2_data_agg, exclude_sp1 = TRUE))\n\n\n$start\n    prop prop_ltm      tau     gain     rate \n   0.170    0.400    0.135   25.000    0.025 \n\n$par\n       prop    prop_ltm         tau        gain        rate \n 0.16975119  0.48023531  0.13651974 29.75877754  0.02233212 \n\n$convergence\n[1] 0\n\n$counts\nfunction gradient \n     246       NA \n\n$value\n[1] 56.36045\n\n\nCode\nexp2_data_agg$pred &lt;- predict(est, exp2_data_agg, group_by = c(\"chunk\", \"gap\"))\nexp2_data_agg |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n\n\n\n\n\n\n\n\n\nagain parameter estimates are different from the paper.\nHere are from multiple starting values:\n\n\nCode\nfits &lt;- fits1 |&gt;\n  filter(priors_scenario == \"none\", exclude_sp1 == TRUE, exp == 2, convergence == 0) |&gt;\n  select(prop:convergence, fit, data) |&gt;\n  arrange(deviance) |&gt;\n  mutate_if(is.numeric, round, 3) |&gt;\n  print(n = 100)\n\n\n# A tibble: 98 × 9\n    prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt; &lt;list&gt;     &lt;list&gt;           \n 1 0.105    0.815 0.007 0.089  87.8     40.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 2 0.105    0.816 0.007 0.089  86.9     40.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 3 0.105    0.815 0.007 0.089  87.7     40.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 4 0.105    0.816 0.007 0.089  87.2     40.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 5 0.105    0.815 0.007 0.089  88.1     40.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 6 0.106    0.816 0.007 0.09   85.3     40.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 7 0.109    0.816 0.007 0.092  81.6     40.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 8 0.128    0.818 0.008 0.105  59.5     40.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 9 0.135    0.818 0.009 0.109  54.5     40.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n10 0.152    0.819 0.01  0.119  43.3     40.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n11 0.162    0.481 0.021 0.132  32.4     56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n12 0.162    0.481 0.021 0.132  32.4     56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n13 0.162    0.481 0.021 0.132  32.4     56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n14 0.162    0.481 0.021 0.132  32.4     56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n15 0.162    0.481 0.021 0.132  32.5     56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n16 0.162    0.481 0.021 0.132  32.6     56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n17 0.163    0.481 0.021 0.132  32.4     56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n18 0.163    0.48  0.021 0.132  32.1     56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n19 0.163    0.48  0.021 0.132  32.3     56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n20 0.163    0.48  0.021 0.132  32.3     56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n21 0.162    0.481 0.021 0.132  32.4     56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n22 0.271    0.505 0.12  0.205  13.9     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n23 0.271    0.526 0.138 0.208  14.5     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n24 0.271    0.656 0.249 0.225  20.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n25 0.271    0.564 0.171 0.213  15.8     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n26 0.271    0.487 0.105 0.203  13.4     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n27 0.271    0.521 0.134 0.207  14.3     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n28 0.271    0.795 0.367 0.244  33.5     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n29 0.271    0.628 0.225 0.222  18.4     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n30 0.271    0.721 0.304 0.234  24.6     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n31 0.271    0.787 0.361 0.243  32.2     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n32 0.271    0.755 0.333 0.238  28.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n33 0.271    0.544 0.154 0.21   15.1     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n34 0.271    0.531 0.143 0.209  14.6     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n35 0.271    0.723 0.306 0.234  24.7     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n36 0.271    0.645 0.239 0.224  19.3     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n37 0.271    0.591 0.193 0.217  16.8     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n38 0.271    0.591 0.194 0.217  16.8     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n39 0.271    0.654 0.247 0.225  19.9     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n40 0.271    0.645 0.24  0.224  19.4     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n41 0.271    0.571 0.176 0.214  16.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n42 0.271    0.759 0.336 0.239  28.5     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n43 0.271    0.635 0.231 0.223  18.8     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n44 0.271    0.73  0.312 0.235  25.4     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n45 0.271    0.718 0.302 0.234  24.4     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n46 0.271    0.593 0.195 0.217  16.8     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n47 0.271    0.622 0.219 0.221  18.1     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n48 0.271    0.707 0.293 0.232  23.4     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n49 0.271    0.495 0.111 0.204  13.6     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n50 0.271    0.643 0.238 0.224  19.2     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n51 0.271    0.718 0.302 0.234  24.3     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n52 0.271    0.648 0.242 0.224  19.5     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n53 0.271    0.72  0.304 0.234  24.5     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n54 0.271    0.666 0.257 0.227  20.5     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n55 0.271    0.601 0.202 0.218  17.2     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n56 0.271    0.45  0.073 0.198  12.5     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n57 0.271    0.561 0.168 0.213  15.6     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n58 0.271    0.527 0.139 0.208  14.5     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n59 0.271    0.532 0.143 0.209  14.7     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n60 0.271    0.802 0.373 0.245  34.7     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n61 0.271    0.627 0.224 0.221  18.4     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n62 0.271    0.619 0.218 0.22   18.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n63 0.271    0.62  0.218 0.221  18.1     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n64 0.271    0.575 0.18  0.215  16.2     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n65 0.271    0.645 0.24  0.224  19.3     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n66 0.271    0.507 0.121 0.205  13.9     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n67 0.271    0.635 0.23  0.222  18.8     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n68 0.271    0.724 0.307 0.235  24.9     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n69 0.271    0.711 0.296 0.233  23.8     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n70 0.271    0.485 0.104 0.203  13.4     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n71 0.271    0.64  0.236 0.223  19.1     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n72 0.271    0.841 0.406 0.25   43.2     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n73 0.271    0.684 0.273 0.229  21.8     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n74 0.271    0.539 0.149 0.21   14.9     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n75 0.27     0.812 0.381 0.246  36.7     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n76 0.271    0.57  0.175 0.214  16.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n77 0.271    0.852 0.416 0.251  46.3     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n78 0.271    0.425 0.052 0.195  11.9     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n79 0.271    0.561 0.168 0.213  15.6     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n80 0.271    0.48  0.099 0.202  13.2     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n81 0.271    0.656 0.249 0.225  20.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n82 0.271    0.743 0.323 0.237  26.6     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n83 0.271    0.678 0.268 0.228  21.3     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n84 0.271    0.592 0.194 0.217  16.8     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n85 0.271    0.624 0.222 0.221  18.2     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n86 0.271    0.742 0.322 0.237  26.6     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n87 0.271    0.671 0.262 0.227  20.9     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n88 0.271    0.508 0.122 0.206  13.9     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n89 0.271    0.537 0.147 0.21   14.8     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n90 0.271    0.572 0.177 0.214  16.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n91 0.271    0.541 0.151 0.21   15.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n92 0.271    0.667 0.258 0.227  20.6     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n93 0.271    0.833 0.4   0.249  41.2     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n94 0.348    0.266 0.289 0.272  10.7     84.2           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n95 0.348    0.279 0.291 0.272  10.8     84.2           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n96 0.584    0.725 0.218 1       0      623.            0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n97 0.616    0.715 0.273 1       0      623.            0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n98 0.612    0.541 0.248 1       0      623.            0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n\n\n\nProblem with parameter identifiability\nrows 12-16 illustrate the problem with parameter identifiability quite well. They have nearly identical deviance, but very different parameters.\n\n\nCode\n(fits &lt;- fits[c(12, 13, 14, 15, 16), ])\n\n\n# A tibble: 5 × 9\n   prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt; &lt;list&gt;     &lt;list&gt;           \n1 0.162    0.481 0.021 0.132  32.4     56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n2 0.162    0.481 0.021 0.132  32.4     56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n3 0.162    0.481 0.021 0.132  32.4     56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n4 0.162    0.481 0.021 0.132  32.5     56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n5 0.162    0.481 0.021 0.132  32.6     56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n\n\nCode\n# saveRDS(fits, \"output/five_parsets_exp2.rds\")\n\n\nPlot the predictions all 5 sets of parameters:\n\n\nCode\nfits |&gt;\n  mutate(pred = map2(fit, data, \\(x, y) predict(x, y, group_by = c(\"chunk\", \"gap\")))) |&gt;\n  unnest(c(data, pred)) |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = as.factor(round(prop, 3)))) +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_grid(~itemtype)\n\n\n\n\n\n\n\n\n\nThe way parameters change suggest that increasing prop can be compensated by increasing rate, taun and decreasing gain. Here’s a pair plot of these parameters\n\n\nCode\nfits |&gt;\n  select(prop, rate, tau, gain) |&gt;\n  ggpairs(diag = list(continuous = \"blankDiag\"))\n\n\nWarning in cor(x, y): the standard deviation is zero\nWarning in cor(x, y): the standard deviation is zero\nWarning in cor(x, y): the standard deviation is zero\nWarning in cor(x, y): the standard deviation is zero\nWarning in cor(x, y): the standard deviation is zero\nWarning in cor(x, y): the standard deviation is zero\n\n\n\n\n\n\n\n\n\nI’ll investigate this in a separate notebook.\n\n\nWith prior on gain\n\n\nCode\nfit &lt;- fits1 |&gt;\n  filter(priors_scenario == \"gain\", exclude_sp1 == TRUE, exp == 2, convergence == 0) |&gt;\n  mutate(deviance = map2_dbl(fit, exclude_sp1, function(x, y) {\n    overall_deviance(x$par, exp2_data_agg, exclude_sp1 = y)\n  })) |&gt;\n  select(prop:convergence, fit, data) |&gt;\n  arrange(deviance) |&gt;\n  mutate_if(is.numeric, round, 3) |&gt;\n  print(n = 100)\n\n\n# A tibble: 98 × 9\n    prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt; &lt;list&gt;     &lt;list&gt;           \n 1 0.205    0.824 0.013 0.146  25.0     40.7           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 2 0.205    0.824 0.013 0.146  25.0     40.7           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 3 0.205    0.824 0.013 0.146  25.0     40.7           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 4 0.205    0.824 0.013 0.146  25.0     40.7           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 5 0.204    0.824 0.013 0.146  25.0     40.7           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 6 0.205    0.824 0.013 0.146  25.0     40.7           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 7 0.186    0.479 0.025 0.146  25       56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 8 0.186    0.479 0.025 0.146  25       56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 9 0.186    0.479 0.025 0.146  25.0     56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n10 0.186    0.479 0.025 0.146  25.0     56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n11 0.186    0.479 0.025 0.146  25       56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n12 0.186    0.48  0.025 0.146  25.0     56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n13 0.271    0.726 0.309 0.235  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n14 0.271    0.726 0.308 0.235  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n15 0.271    0.725 0.308 0.234  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n16 0.271    0.726 0.308 0.235  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n17 0.272    0.727 0.31  0.235  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n18 0.272    0.727 0.31  0.235  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n19 0.272    0.726 0.31  0.235  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n20 0.271    0.725 0.307 0.234  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n21 0.271    0.725 0.308 0.234  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n22 0.271    0.726 0.309 0.235  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n23 0.27     0.724 0.306 0.234  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n24 0.271    0.725 0.307 0.234  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n25 0.27     0.724 0.305 0.233  25.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n26 0.27     0.724 0.305 0.233  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n27 0.269    0.722 0.303 0.233  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n28 0.27     0.723 0.304 0.233  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n29 0.272    0.727 0.309 0.235  25.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n30 0.269    0.721 0.302 0.232  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n31 0.272    0.727 0.31  0.235  25.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n32 0.269    0.72  0.302 0.232  25.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n33 0.276    0.735 0.323 0.24   25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n34 0.264    0.712 0.289 0.228  25.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n35 0.263    0.709 0.286 0.227  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n36 0.271    0.726 0.308 0.235  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n37 0.27     0.724 0.306 0.234  25.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n38 0.281    0.744 0.337 0.245  25.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n39 0.271    0.725 0.308 0.234  25.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n40 0.284    0.749 0.343 0.247  25.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n41 0.286    0.751 0.348 0.249  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n42 0.255    0.693 0.264 0.219  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n43 0.268    0.719 0.299 0.231  25.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n44 0.271    0.726 0.309 0.235  25.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n45 0.253    0.688 0.259 0.217  25.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n46 0.272    0.728 0.311 0.235  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n47 0.289    0.756 0.356 0.252  25.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n48 0.25     0.681 0.25  0.214  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n49 0.279    0.741 0.331 0.243  25.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n50 0.292    0.762 0.365 0.255  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n51 0.249    0.678 0.246 0.213  25.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n52 0.249    0.679 0.247 0.213  25.0     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n53 0.271    0.725 0.307 0.234  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n54 0.246    0.669 0.236 0.209  25.0     63.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n55 0.272    0.727 0.31  0.235  25.0     63.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n56 0.271    0.725 0.308 0.234  25.0     63.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n57 0.271    0.726 0.308 0.234  25       63.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n58 0.231    0.628 0.192 0.195  25.0     63.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n59 0.271    0.725 0.308 0.234  25       63.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n60 0.271    0.725 0.308 0.234  25       63.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n61 0.224    0.606 0.17  0.188  25       63.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n62 0.222    0.6   0.165 0.186  25       63.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n63 0.221    0.596 0.161 0.185  25       63.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n64 0.271    0.726 0.308 0.235  25       63.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n65 0.204    0.529 0.105 0.167  25.0     63.6           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n66 0.271    0.725 0.308 0.234  25.0     63.6           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n67 0.203    0.526 0.102 0.167  25       63.6           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n68 0.271    0.725 0.308 0.234  25       63.6           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n69 0.2      0.514 0.094 0.164  25       63.6           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n70 0.2      0.513 0.093 0.164  25.0     63.6           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n71 0.195    0.487 0.074 0.159  25.0     63.7           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n72 0.271    0.725 0.307 0.234  25       63.7           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n73 0.191    0.468 0.061 0.155  25       63.7           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n74 0.188    0.451 0.05  0.152  25.0     63.8           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n75 0.271    0.726 0.309 0.236  25.0     63.8           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n76 0.271    0.725 0.307 0.235  25.0     64.2           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n77 0.271    0.726 0.309 0.234  25       64.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n78 0.271    0.725 0.308 0.236  25.0     66.0           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n79 0.974    0.994 0.003 0.008  25.0     69.2           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n80 0.974    0.994 0.003 0.008  25.0     69.2           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n81 0.348    0.221 0.522 0.315  25       84.2           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n82 0.348    0.721 0.521 0.315  25       84.2           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n83 0.348    0.195 0.521 0.315  25       84.2           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n84 0.348    0.418 0.521 0.315  25.0     84.2           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n85 0.348    0.204 0.522 0.315  25       84.2           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n86 0.348    0.315 0.522 0.315  25.0     84.2           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n87 0.348    0.237 0.521 0.315  25       84.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n88 0.327    0.704 0.468 0.294  25       84.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n89 0.348    0.112 0.522 0.316  25       84.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n90 0.317    0.637 0.444 0.284  25       84.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n91 0.348    0.303 0.521 0.315  25.0     84.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n92 0.31     0.485 0.426 0.277  25       84.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n93 0.303    0.612 0.409 0.271  25.0     84.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n94 0.347    0.158 0.52  0.315  25.0     84.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n95 0.348    0.58  0.52  0.314  25.0     84.6           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n96 0.347    0.586 0.52  0.316  25.0     86.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n97 0.346    0.505 0.517 0.315  25       88.7           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n98 0.11     0.733 0.277 0.1    25      473.            0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n\n\nI see three sets of parameters that are close in deviance (relatively):\n\n\nCode\nfits &lt;- fit[c(1, 11, 13), ]\nfits\n\n\n# A tibble: 3 × 9\n   prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt; &lt;list&gt;     &lt;list&gt;           \n1 0.205    0.824 0.013 0.146  25.0     40.7           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n2 0.186    0.479 0.025 0.146  25       56.4           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n3 0.271    0.726 0.309 0.235  25       63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n\n\nplots\n\n\nCode\nfits |&gt;\n  mutate(pred = map2(fit, data, \\(x, y) predict(x, y, group_by = c(\"chunk\", \"gap\")))) |&gt;\n  unnest(c(data, pred)) |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = as.factor(round(prop, 3)))) +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_grid(~itemtype)\n\n\n\n\n\n\n\n\n\nthis case is particularly interesting. The bestfitting parameters produce almost no interaction. The other two sets of parameters produce a strong interaction, but misfit the overall data.\nFurther, the parameter set with rate 0.024 and 0.271 have quite similar fits despite very different parameter sets!\n\n\nwith prior on rate\n\n\nCode\nfit &lt;- fits1 |&gt;\n  filter(priors_scenario == \"rate\", exclude_sp1 == TRUE, exp == 2, convergence == 0) |&gt;\n  mutate(deviance = map2_dbl(fit, exclude_sp1, function(x, y) {\n    overall_deviance(x$par, exp2_data_agg, exclude_sp1 = y)\n  })) |&gt;\n  select(prop:convergence, fit, data) |&gt;\n  arrange(deviance) |&gt;\n  mutate_if(is.numeric, round, 3) |&gt;\n  print(n = 100)\n\n\n# A tibble: 99 × 9\n    prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt; &lt;list&gt;     &lt;list&gt;           \n 1 0.474    0.844 0.032 0.197  5.86     47.0           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 2 0.474    0.844 0.032 0.197  5.86     47.0           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 3 0.474    0.844 0.032 0.197  5.86     47.0           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 4 0.475    0.844 0.032 0.197  5.86     47.0           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 5 0.474    0.844 0.032 0.197  5.86     47.0           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 6 0.475    0.844 0.032 0.197  5.86     47.0           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 7 0.475    0.845 0.032 0.196  5.86     47.0           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 8 0.475    0.844 0.032 0.196  5.85     47.0           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n 9 0.474    0.844 0.032 0.197  5.86     47.0           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n10 0.475    0.844 0.032 0.197  5.86     47.0           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n11 0.475    0.844 0.032 0.197  5.86     47.0           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n12 0.475    0.844 0.032 0.196  5.86     47.0           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n13 0.475    0.844 0.032 0.197  5.86     47.0           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n14 0.475    0.844 0.032 0.197  5.86     47.0           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n15 0.475    0.845 0.032 0.196  5.85     47.0           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n16 0.475    0.844 0.032 0.196  5.85     47.0           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n17 0.475    0.844 0.032 0.197  5.85     47.0           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n18 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n19 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n20 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n21 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n22 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n23 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n24 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n25 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n26 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n27 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n28 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n29 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n30 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n31 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n32 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n33 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n34 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n35 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n36 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n37 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n38 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n39 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n40 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n41 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n42 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n43 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n44 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n45 0.271    0.482 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n46 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n47 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n48 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n49 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n50 0.271    0.481 0.1   0.202 13.3      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n51 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n52 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n53 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n54 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n55 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n56 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n57 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n58 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n59 0.271    0.482 0.1   0.202 13.3      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n60 0.272    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n61 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n62 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n63 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n64 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n65 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n66 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n67 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n68 0.27     0.481 0.1   0.202 13.3      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n69 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n70 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n71 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n72 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n73 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n74 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n75 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n76 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n77 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n78 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n79 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n80 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n81 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n82 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n83 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n84 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n85 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n86 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n87 0.271    0.481 0.1   0.202 13.2      63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n88 0.474    0.435 0.072 0.245  4.28     63.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n89 0.474    0.435 0.072 0.245  4.28     63.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n90 0.474    0.434 0.072 0.245  4.28     63.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n91 0.474    0.434 0.072 0.245  4.28     63.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n92 0.474    0.435 0.072 0.245  4.28     63.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n93 0.474    0.435 0.072 0.245  4.28     63.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n94 0.474    0.434 0.072 0.245  4.28     63.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n95 0.474    0.435 0.072 0.245  4.28     63.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n96 0.474    0.434 0.072 0.245  4.28     63.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n97 0.474    0.435 0.072 0.245  4.28     63.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n98 0.474    0.434 0.072 0.245  4.28     63.5           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n99 0.509    0.398 0.102 0.258  3.72     68.2           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n\n\nplots\n\n\nCode\nfits &lt;- fit[c(28, 43), ] # previous 83\nfits\n\n\n# A tibble: 2 × 9\n   prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt; &lt;list&gt;     &lt;list&gt;           \n1 0.271    0.481   0.1 0.202  13.2     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n2 0.271    0.481   0.1 0.202  13.2     63.3           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n\n\nCode\nfits |&gt;\n  mutate(pred = map2(fit, data, \\(x, y) predict(x, y, group_by = c(\"chunk\", \"gap\")))) |&gt;\n  unnest(c(data, pred)) |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = as.factor(round(prop, 3)))) +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_grid(~itemtype)\n\n\n\n\n\n\n\n\n\n\n\nIncluding the first chunk in the optimization\n\n\nCode\nstart &lt;- paper_params(exp = 2)\n(est &lt;- estimate_model(start, data = exp2_data_agg, exclude_sp1 = FALSE))\n\n\n$start\n    prop prop_ltm      tau     gain     rate \n   0.170    0.400    0.135   25.000    0.025 \n\n$par\n      prop   prop_ltm        tau       gain       rate \n 0.1618193  0.8723027  0.1235086 43.4849440  0.0080991 \n\n$convergence\n[1] 0\n\n$counts\nfunction gradient \n     412       NA \n\n$value\n[1] 116.1887\n\n\nCode\nexp2_data_agg$pred &lt;- predict(est, exp2_data_agg, group_by = c(\"chunk\", \"gap\"))\nexp2_data_agg |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line() +\n  geom_line(aes(y = pred), linetype = \"dashed\") +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_wrap(~itemtype)\n\n\n\n\n\n\n\n\n\nfrom multiple starting values:\n\n\nCode\nfits &lt;- fits1 |&gt;\n  filter(priors_scenario == \"none\", exclude_sp1 == FALSE, exp == 2, convergence == 0) |&gt;\n  select(prop:convergence, fit, data) |&gt;\n  arrange(deviance) |&gt;\n  mutate_if(is.numeric, round, 3)\nhead(fits)\n\n\n# A tibble: 6 × 9\n   prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt; &lt;list&gt;     &lt;list&gt;           \n1 0.38     0.862  0.02 0.189  8.98     110.           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n2 0.38     0.862  0.02 0.189  8.98     110.           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n3 0.38     0.862  0.02 0.189  8.98     110.           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n4 0.379    0.862  0.02 0.189  8.98     110.           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n5 0.38     0.862  0.02 0.189  8.98     110.           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n6 0.38     0.862  0.02 0.189  8.98     110.           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n\n\n\n\nWith prior on rate\n\n\nCode\nfit &lt;- fits1 |&gt;\n  filter(priors_scenario == \"rate\", exclude_sp1 == FALSE, exp == 2, convergence == 0) |&gt;\n  mutate(deviance = map2_dbl(fit, exclude_sp1, function(x, y) {\n    overall_deviance(x$par, exp2_data_agg, exclude_sp1 = y)\n  })) |&gt;\n  select(prop:convergence, fit, data) |&gt;\n  arrange(deviance) |&gt;\n  mutate_if(is.numeric, round, 3)\nhead(fit)\n\n\n# A tibble: 6 × 9\n   prop prop_ltm  rate   tau  gain deviance convergence fit        data             \n  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt; &lt;list&gt;     &lt;list&gt;           \n1 0.501    0.857 0.032 0.192  5.56     114.           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n2 0.501    0.857 0.032 0.192  5.55     114.           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n3 0.502    0.857 0.032 0.192  5.55     114.           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n4 0.501    0.858 0.032 0.192  5.56     114.           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n5 0.501    0.857 0.032 0.192  5.55     114.           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n6 0.501    0.858 0.032 0.192  5.55     114.           0 &lt;srl_rcl_&gt; &lt;tibble [12 × 8]&gt;\n\n\nplot predictions\n\n\nCode\nfit |&gt;\n  slice(1) |&gt;\n  mutate(pred = map2(fit, data, \\(x, y) predict(x, y, group_by = c(\"chunk\", \"gap\")))) |&gt;\n  unnest(c(data, pred)) |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = as.factor(round(prop, 3)))) +\n  scale_color_discrete(\"First chunk LTM?\") +\n  facet_grid(~itemtype)",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Main results"
    ]
  },
  {
    "objectID": "docs/notebooks/modelling_edas_approach.html#summary",
    "href": "docs/notebooks/modelling_edas_approach.html#summary",
    "title": "Main results",
    "section": "Summary",
    "text": "Summary\n\nThe parameters reported in the paper are not the best fitting\nWhen I start from 100 different starting values, I get better fitting parameters, but with an even lower rate\nI can reproduce the parameters from the paper if I fix the gain parameter to 25\n\n\nBest fitting parameters\nGiven the different modeling choices (ignoring the first chunk or not, priors on the parameters)\nTODO: make this into a function for getting the final parameters\n\n\nCode\nfinal &lt;- fits1 |&gt;\n  filter(convergence == 0) |&gt;\n  group_by(exp, priors_scenario, exclude_sp1) |&gt;\n  arrange(deviance) |&gt;\n  slice(1) |&gt;\n  arrange(desc(exclude_sp1), exp, priors_scenario) |&gt;\n  mutate(\n    deviance = round(deviance, 1),\n    priors_scenario = case_when(\n      priors_scenario == \"none\" ~ \"None\",\n      priors_scenario == \"gain\" ~ \"Gain ~ N(25, 0.1)\",\n      priors_scenario == \"rate\" ~ \"Rate ~ N(0.1, 0.01)\"\n    )\n  )\n\nfinal |&gt;\n  select(exp, priors_scenario, exclude_sp1, prop:gain, deviance) |&gt;\n  mutate_all(round, 3) |&gt;\n  kbl() |&gt;\n  kable_styling()\n\n\n\n\n\nexp\npriors_scenario\nexclude_sp1\nprop\nprop_ltm\nrate\ntau\ngain\ndeviance\n\n\n\n\n1\nGain ~ N(25, 0.1)\nTRUE\n0.222\n0.588\n0.019\n0.154\n25.002\n33.0\n\n\n1\nNone\nTRUE\n0.106\n0.575\n0.009\n0.090\n100.000\n31.9\n\n\n1\nRate ~ N(0.1, 0.01)\nTRUE\n0.435\n0.608\n0.049\n0.202\n7.590\n43.1\n\n\n2\nGain ~ N(25, 0.1)\nTRUE\n0.205\n0.824\n0.013\n0.146\n25.001\n40.7\n\n\n2\nNone\nTRUE\n0.105\n0.815\n0.007\n0.089\n87.787\n40.5\n\n\n2\nRate ~ N(0.1, 0.01)\nTRUE\n0.474\n0.844\n0.032\n0.197\n5.865\n47.0\n\n\n1\nGain ~ N(25, 0.1)\nFALSE\n0.231\n0.639\n0.016\n0.156\n24.999\n145.5\n\n\n1\nNone\nFALSE\n0.303\n0.636\n0.021\n0.177\n15.206\n144.9\n\n\n1\nRate ~ N(0.1, 0.01)\nFALSE\n0.475\n0.645\n0.049\n0.197\n6.907\n152.8\n\n\n2\nGain ~ N(25, 0.1)\nFALSE\n0.217\n0.874\n0.011\n0.150\n24.997\n113.5\n\n\n2\nNone\nFALSE\n0.380\n0.862\n0.020\n0.189\n8.983\n109.7\n\n\n2\nRate ~ N(0.1, 0.01)\nFALSE\n0.501\n0.857\n0.032\n0.192\n5.557\n113.8\n\n\n\n\n\n\n\n\nprop_ltm very different between the two experiments (overfitting…)\n\n\n\nPredictions\n(the two experiments are modeled separately)\n\n\nAll predictions\n\n\nCode\nfinal |&gt;\n  select(exp, rate, data, pred) |&gt;\n  mutate(exp = paste0(\"Exp \", exp)) |&gt;\n  unnest(c(data, pred)) |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = as.factor(round(rate, 3)))) +\n  scale_color_discrete(\"1st chunk LTM?\") +\n  scale_linetype_discrete(\"Rate\") +\n  facet_grid(exp ~ itemtype) +\n  theme_pub()\n\n\n\n\n\n\n\n\n\n\n\nExperiment 1\n\n\nCode\nfinal |&gt;\n  filter(exp == 1) |&gt;\n  arrange(rate) |&gt;\n  select(rate, data, pred) |&gt;\n  unnest(c(data, pred)) |&gt;\n  mutate(rate = as.character(round(rate, 4))) |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = )) +\n  scale_color_discrete(\"1st chunk LTM?\") +\n  scale_linetype_discrete(\"Rate\") +\n  facet_grid(rate ~ itemtype) +\n  theme_pub()\n\n\n\n\n\n\n\n\n\n\n\nExperiment 2\n\n\nCode\nfinal |&gt;\n  filter(exp == 2) |&gt;\n  arrange(rate) |&gt;\n  select(rate, data, pred) |&gt;\n  unnest(c(data, pred)) |&gt;\n  mutate(rate = as.character(round(rate, 4))) |&gt;\n  ggplot(aes(x = gap, y = p_correct, color = chunk)) +\n  geom_point() +\n  geom_line(aes(y = pred, linetype = )) +\n  scale_color_discrete(\"1st chunk LTM?\") +\n  scale_linetype_discrete(\"Rate\") +\n  facet_grid(rate ~ itemtype) +\n  theme_pub()",
    "crumbs": [
      "Notebooks",
      "Model 1: Original model",
      "Main results"
    ]
  },
  {
    "objectID": "docs/reference/plot_bootstrap_results.html",
    "href": "docs/reference/plot_bootstrap_results.html",
    "title": "Plot Bootstrap Results",
    "section": "",
    "text": "This function plots the bootstrap results for a given dataset.\n\n\n\nplot_bootstrap_results(data)\n\n\n\n\ndata: The dataset containing the bootstrap results.\n\n\n\n\nA ggplot object displaying the histogram of the bootstrap results for each parameter.\n\n\n\ntar_load(exp1_data)\ndata &lt;- replicate(10, boot_est(exp1_data), simplify = FALSE)\ndata &lt;- do.call(rbind, data)\nplot_bootstrap_results(data)",
    "crumbs": [
      "Function reference",
      "Plot Bootstrap Results"
    ]
  },
  {
    "objectID": "docs/reference/plot_bootstrap_results.html#description",
    "href": "docs/reference/plot_bootstrap_results.html#description",
    "title": "Plot Bootstrap Results",
    "section": "",
    "text": "This function plots the bootstrap results for a given dataset.",
    "crumbs": [
      "Function reference",
      "Plot Bootstrap Results"
    ]
  },
  {
    "objectID": "docs/reference/plot_bootstrap_results.html#usage",
    "href": "docs/reference/plot_bootstrap_results.html#usage",
    "title": "Plot Bootstrap Results",
    "section": "",
    "text": "plot_bootstrap_results(data)",
    "crumbs": [
      "Function reference",
      "Plot Bootstrap Results"
    ]
  },
  {
    "objectID": "docs/reference/plot_bootstrap_results.html#arguments",
    "href": "docs/reference/plot_bootstrap_results.html#arguments",
    "title": "Plot Bootstrap Results",
    "section": "",
    "text": "data: The dataset containing the bootstrap results.",
    "crumbs": [
      "Function reference",
      "Plot Bootstrap Results"
    ]
  },
  {
    "objectID": "docs/reference/plot_bootstrap_results.html#value",
    "href": "docs/reference/plot_bootstrap_results.html#value",
    "title": "Plot Bootstrap Results",
    "section": "",
    "text": "A ggplot object displaying the histogram of the bootstrap results for each parameter.",
    "crumbs": [
      "Function reference",
      "Plot Bootstrap Results"
    ]
  },
  {
    "objectID": "docs/reference/plot_bootstrap_results.html#examples",
    "href": "docs/reference/plot_bootstrap_results.html#examples",
    "title": "Plot Bootstrap Results",
    "section": "",
    "text": "tar_load(exp1_data)\ndata &lt;- replicate(10, boot_est(exp1_data), simplify = FALSE)\ndata &lt;- do.call(rbind, data)\nplot_bootstrap_results(data)",
    "crumbs": [
      "Function reference",
      "Plot Bootstrap Results"
    ]
  },
  {
    "objectID": "docs/reference/serial_recall.html",
    "href": "docs/reference/serial_recall.html",
    "title": "Serial Recall Model",
    "section": "",
    "text": "This function implements the model currently described in the draft on page 19. It gives the predicted recall probability for each item in a set of items.\n\n\n\nserial_recall(\n  setsize,\n  ISI = rep(0.5, setsize),\n  item_in_ltm = rep(TRUE, setsize),\n  prop = 0.2,\n  prop_ltm = 0.5,\n  tau = 0.15,\n  gain = 25,\n  rate = 0.1,\n  r_max = 1,\n  lambda = 1,\n  growth = \"linear\"\n)\n\n\n\n\nsetsize: The number of items in the set.\nISI: A numeric vector representing the inter-stimulus interval for each item.\nitem_in_ltm: A logical vector indicating whether each item is in LTM.\nprop: The proportion of resources allocated to each item.\nprop_ltm: Proportion by which the resources used by LTM items are multiplied.\ntau: The threshold for recall probability.\ngain: The gain parameter for the recall probability function.\nrate: The rate at which resources are recovered.\nr_max: The maximum amount of resources.\nlambda: The exponent converting resources to strength.\ngrowth: The growth function for resource recovery. Either ‘linear’ or ‘asy’.\n\n\n\n\nThe function uses a simulation approach. It loops over trials\n\n\n\nA numeric vector representing the recall probability for each item.\n\n\n\nserial_recall(setsize = 3, ISI = rep(0.5, 3))",
    "crumbs": [
      "Function reference",
      "Serial Recall Model"
    ]
  },
  {
    "objectID": "docs/reference/serial_recall.html#description",
    "href": "docs/reference/serial_recall.html#description",
    "title": "Serial Recall Model",
    "section": "",
    "text": "This function implements the model currently described in the draft on page 19. It gives the predicted recall probability for each item in a set of items.",
    "crumbs": [
      "Function reference",
      "Serial Recall Model"
    ]
  },
  {
    "objectID": "docs/reference/serial_recall.html#usage",
    "href": "docs/reference/serial_recall.html#usage",
    "title": "Serial Recall Model",
    "section": "",
    "text": "serial_recall(\n  setsize,\n  ISI = rep(0.5, setsize),\n  item_in_ltm = rep(TRUE, setsize),\n  prop = 0.2,\n  prop_ltm = 0.5,\n  tau = 0.15,\n  gain = 25,\n  rate = 0.1,\n  r_max = 1,\n  lambda = 1,\n  growth = \"linear\"\n)",
    "crumbs": [
      "Function reference",
      "Serial Recall Model"
    ]
  },
  {
    "objectID": "docs/reference/serial_recall.html#arguments",
    "href": "docs/reference/serial_recall.html#arguments",
    "title": "Serial Recall Model",
    "section": "",
    "text": "setsize: The number of items in the set.\nISI: A numeric vector representing the inter-stimulus interval for each item.\nitem_in_ltm: A logical vector indicating whether each item is in LTM.\nprop: The proportion of resources allocated to each item.\nprop_ltm: Proportion by which the resources used by LTM items are multiplied.\ntau: The threshold for recall probability.\ngain: The gain parameter for the recall probability function.\nrate: The rate at which resources are recovered.\nr_max: The maximum amount of resources.\nlambda: The exponent converting resources to strength.\ngrowth: The growth function for resource recovery. Either ‘linear’ or ‘asy’.",
    "crumbs": [
      "Function reference",
      "Serial Recall Model"
    ]
  },
  {
    "objectID": "docs/reference/serial_recall.html#details",
    "href": "docs/reference/serial_recall.html#details",
    "title": "Serial Recall Model",
    "section": "",
    "text": "The function uses a simulation approach. It loops over trials",
    "crumbs": [
      "Function reference",
      "Serial Recall Model"
    ]
  },
  {
    "objectID": "docs/reference/serial_recall.html#value",
    "href": "docs/reference/serial_recall.html#value",
    "title": "Serial Recall Model",
    "section": "",
    "text": "A numeric vector representing the recall probability for each item.",
    "crumbs": [
      "Function reference",
      "Serial Recall Model"
    ]
  },
  {
    "objectID": "docs/reference/serial_recall.html#examples",
    "href": "docs/reference/serial_recall.html#examples",
    "title": "Serial Recall Model",
    "section": "",
    "text": "serial_recall(setsize = 3, ISI = rep(0.5, 3))",
    "crumbs": [
      "Function reference",
      "Serial Recall Model"
    ]
  },
  {
    "objectID": "docs/reference/overall_deviance.html",
    "href": "docs/reference/overall_deviance.html",
    "title": "Calculate the overall deviance",
    "section": "",
    "text": "This function calculates the overall deviance for a given set of parameters and data. It splits the dataset by the given column and calculates the deviance for each subset. The overall deviance is the sum of the deviances for each subset.\n\n\n\noverall_deviance(params, data, by = c(\"chunk\", \"gap\"), ..., priors = list())\n\n\n\n\nparams: A vector of parameters.\ndata: A data frame containing the data.\nby: The column(s) to split the data by.\n\n\n\n\nThe overall deviance.",
    "crumbs": [
      "Function reference",
      "Calculate the overall deviance"
    ]
  },
  {
    "objectID": "docs/reference/overall_deviance.html#description",
    "href": "docs/reference/overall_deviance.html#description",
    "title": "Calculate the overall deviance",
    "section": "",
    "text": "This function calculates the overall deviance for a given set of parameters and data. It splits the dataset by the given column and calculates the deviance for each subset. The overall deviance is the sum of the deviances for each subset.",
    "crumbs": [
      "Function reference",
      "Calculate the overall deviance"
    ]
  },
  {
    "objectID": "docs/reference/overall_deviance.html#usage",
    "href": "docs/reference/overall_deviance.html#usage",
    "title": "Calculate the overall deviance",
    "section": "",
    "text": "overall_deviance(params, data, by = c(\"chunk\", \"gap\"), ..., priors = list())",
    "crumbs": [
      "Function reference",
      "Calculate the overall deviance"
    ]
  },
  {
    "objectID": "docs/reference/overall_deviance.html#arguments",
    "href": "docs/reference/overall_deviance.html#arguments",
    "title": "Calculate the overall deviance",
    "section": "",
    "text": "params: A vector of parameters.\ndata: A data frame containing the data.\nby: The column(s) to split the data by.",
    "crumbs": [
      "Function reference",
      "Calculate the overall deviance"
    ]
  },
  {
    "objectID": "docs/reference/overall_deviance.html#value",
    "href": "docs/reference/overall_deviance.html#value",
    "title": "Calculate the overall deviance",
    "section": "",
    "text": "The overall deviance.",
    "crumbs": [
      "Function reference",
      "Calculate the overall deviance"
    ]
  },
  {
    "objectID": "docs/reference/preprocess_data.html",
    "href": "docs/reference/preprocess_data.html",
    "title": "Preprocesses the data",
    "section": "",
    "text": "This function preprocesses the input data by converting the trial column to numeric, transforming the gap column values, and categorizing the itemtype based on serpos values.\n\n\n\npreprocess_data(data, longgap)\n\n\n\n\ndata: The input data frame\nlonggap: The value to use for the long gap in ms.\n\n\n\n\nThe preprocessed data frame",
    "crumbs": [
      "Function reference",
      "Preprocesses the data"
    ]
  },
  {
    "objectID": "docs/reference/preprocess_data.html#description",
    "href": "docs/reference/preprocess_data.html#description",
    "title": "Preprocesses the data",
    "section": "",
    "text": "This function preprocesses the input data by converting the trial column to numeric, transforming the gap column values, and categorizing the itemtype based on serpos values.",
    "crumbs": [
      "Function reference",
      "Preprocesses the data"
    ]
  },
  {
    "objectID": "docs/reference/preprocess_data.html#usage",
    "href": "docs/reference/preprocess_data.html#usage",
    "title": "Preprocesses the data",
    "section": "",
    "text": "preprocess_data(data, longgap)",
    "crumbs": [
      "Function reference",
      "Preprocesses the data"
    ]
  },
  {
    "objectID": "docs/reference/preprocess_data.html#arguments",
    "href": "docs/reference/preprocess_data.html#arguments",
    "title": "Preprocesses the data",
    "section": "",
    "text": "data: The input data frame\nlonggap: The value to use for the long gap in ms.",
    "crumbs": [
      "Function reference",
      "Preprocesses the data"
    ]
  },
  {
    "objectID": "docs/reference/preprocess_data.html#value",
    "href": "docs/reference/preprocess_data.html#value",
    "title": "Preprocesses the data",
    "section": "",
    "text": "The preprocessed data frame",
    "crumbs": [
      "Function reference",
      "Preprocesses the data"
    ]
  },
  {
    "objectID": "docs/reference/extract_object_from_rdata.html",
    "href": "docs/reference/extract_object_from_rdata.html",
    "title": "Get data object from a file",
    "section": "",
    "text": "This function safely loads the environment from an Rdata file and returns an object from it.\n\n\n\nextract_object_from_rdata(path, object_name = \"data_an\")\n\n\n\n\npath: The path to the file containing the Rdata file\nobject_name: The name of the object to extract from the Rdata file\n\n\n\n\nThe loaded data object.",
    "crumbs": [
      "Function reference",
      "Get data object from a file"
    ]
  },
  {
    "objectID": "docs/reference/extract_object_from_rdata.html#description",
    "href": "docs/reference/extract_object_from_rdata.html#description",
    "title": "Get data object from a file",
    "section": "",
    "text": "This function safely loads the environment from an Rdata file and returns an object from it.",
    "crumbs": [
      "Function reference",
      "Get data object from a file"
    ]
  },
  {
    "objectID": "docs/reference/extract_object_from_rdata.html#usage",
    "href": "docs/reference/extract_object_from_rdata.html#usage",
    "title": "Get data object from a file",
    "section": "",
    "text": "extract_object_from_rdata(path, object_name = \"data_an\")",
    "crumbs": [
      "Function reference",
      "Get data object from a file"
    ]
  },
  {
    "objectID": "docs/reference/extract_object_from_rdata.html#arguments",
    "href": "docs/reference/extract_object_from_rdata.html#arguments",
    "title": "Get data object from a file",
    "section": "",
    "text": "path: The path to the file containing the Rdata file\nobject_name: The name of the object to extract from the Rdata file",
    "crumbs": [
      "Function reference",
      "Get data object from a file"
    ]
  },
  {
    "objectID": "docs/reference/extract_object_from_rdata.html#value",
    "href": "docs/reference/extract_object_from_rdata.html#value",
    "title": "Get data object from a file",
    "section": "",
    "text": "The loaded data object.",
    "crumbs": [
      "Function reference",
      "Get data object from a file"
    ]
  },
  {
    "objectID": "docs/reference/plot_linear_rv_recovery.html",
    "href": "docs/reference/plot_linear_rv_recovery.html",
    "title": "Plot Linear RV Recovery",
    "section": "",
    "text": "This function plots the recovery of a linear random variable (RV) over time.\n\n\n\nplot_linear_rv_recovery(r, t, title = \"Uniform distribution\")\n\n\n\n\nr: A vector representing the recovery rates sampled from some distribution.\nt: A vector representing the time values.\ntitle: The title of the plot (default is “Uniform distribution”).\n\n\n\n\nAverage recovery is given as a red line, while individual trajectories are given as black lines.\n\n\n\nA ggplot object representing the plot of linear RV recovery over time.\n\n\n\nr &lt;- runif(1000, 0, 1)\nt &lt;- seq(0, 5, by = 0.01)\nplot_linear_rv_recovery(r, t, title = \"Uniform distribution\")",
    "crumbs": [
      "Function reference",
      "Plot Linear RV Recovery"
    ]
  },
  {
    "objectID": "docs/reference/plot_linear_rv_recovery.html#description",
    "href": "docs/reference/plot_linear_rv_recovery.html#description",
    "title": "Plot Linear RV Recovery",
    "section": "",
    "text": "This function plots the recovery of a linear random variable (RV) over time.",
    "crumbs": [
      "Function reference",
      "Plot Linear RV Recovery"
    ]
  },
  {
    "objectID": "docs/reference/plot_linear_rv_recovery.html#usage",
    "href": "docs/reference/plot_linear_rv_recovery.html#usage",
    "title": "Plot Linear RV Recovery",
    "section": "",
    "text": "plot_linear_rv_recovery(r, t, title = \"Uniform distribution\")",
    "crumbs": [
      "Function reference",
      "Plot Linear RV Recovery"
    ]
  },
  {
    "objectID": "docs/reference/plot_linear_rv_recovery.html#arguments",
    "href": "docs/reference/plot_linear_rv_recovery.html#arguments",
    "title": "Plot Linear RV Recovery",
    "section": "",
    "text": "r: A vector representing the recovery rates sampled from some distribution.\nt: A vector representing the time values.\ntitle: The title of the plot (default is “Uniform distribution”).",
    "crumbs": [
      "Function reference",
      "Plot Linear RV Recovery"
    ]
  },
  {
    "objectID": "docs/reference/plot_linear_rv_recovery.html#details",
    "href": "docs/reference/plot_linear_rv_recovery.html#details",
    "title": "Plot Linear RV Recovery",
    "section": "",
    "text": "Average recovery is given as a red line, while individual trajectories are given as black lines.",
    "crumbs": [
      "Function reference",
      "Plot Linear RV Recovery"
    ]
  },
  {
    "objectID": "docs/reference/plot_linear_rv_recovery.html#value",
    "href": "docs/reference/plot_linear_rv_recovery.html#value",
    "title": "Plot Linear RV Recovery",
    "section": "",
    "text": "A ggplot object representing the plot of linear RV recovery over time.",
    "crumbs": [
      "Function reference",
      "Plot Linear RV Recovery"
    ]
  },
  {
    "objectID": "docs/reference/plot_linear_rv_recovery.html#examples",
    "href": "docs/reference/plot_linear_rv_recovery.html#examples",
    "title": "Plot Linear RV Recovery",
    "section": "",
    "text": "r &lt;- runif(1000, 0, 1)\nt &lt;- seq(0, 5, by = 0.01)\nplot_linear_rv_recovery(r, t, title = \"Uniform distribution\")",
    "crumbs": [
      "Function reference",
      "Plot Linear RV Recovery"
    ]
  },
  {
    "objectID": "docs/reference/aggregate_data.html",
    "href": "docs/reference/aggregate_data.html",
    "title": "Aggregate Data",
    "section": "",
    "text": "This function aggregates the given data by grouping it based on the chunk, gap, and itemtype columns. It then calculates various summary statistics including the total count, the count of correct values, and the proportion of correct values.\n\n\n\naggregate_data(data)\n\n\n\n\ndata: The input data frame.\n\n\n\n\nA new data frame with the aggregated data.\n\n\n\ntar_load(exp1_data)\naggregate_data(exp1_data)",
    "crumbs": [
      "Function reference",
      "Aggregate Data"
    ]
  },
  {
    "objectID": "docs/reference/aggregate_data.html#description",
    "href": "docs/reference/aggregate_data.html#description",
    "title": "Aggregate Data",
    "section": "",
    "text": "This function aggregates the given data by grouping it based on the chunk, gap, and itemtype columns. It then calculates various summary statistics including the total count, the count of correct values, and the proportion of correct values.",
    "crumbs": [
      "Function reference",
      "Aggregate Data"
    ]
  },
  {
    "objectID": "docs/reference/aggregate_data.html#usage",
    "href": "docs/reference/aggregate_data.html#usage",
    "title": "Aggregate Data",
    "section": "",
    "text": "aggregate_data(data)",
    "crumbs": [
      "Function reference",
      "Aggregate Data"
    ]
  },
  {
    "objectID": "docs/reference/aggregate_data.html#arguments",
    "href": "docs/reference/aggregate_data.html#arguments",
    "title": "Aggregate Data",
    "section": "",
    "text": "data: The input data frame.",
    "crumbs": [
      "Function reference",
      "Aggregate Data"
    ]
  },
  {
    "objectID": "docs/reference/aggregate_data.html#value",
    "href": "docs/reference/aggregate_data.html#value",
    "title": "Aggregate Data",
    "section": "",
    "text": "A new data frame with the aggregated data.",
    "crumbs": [
      "Function reference",
      "Aggregate Data"
    ]
  },
  {
    "objectID": "docs/reference/aggregate_data.html#examples",
    "href": "docs/reference/aggregate_data.html#examples",
    "title": "Aggregate Data",
    "section": "",
    "text": "tar_load(exp1_data)\naggregate_data(exp1_data)",
    "crumbs": [
      "Function reference",
      "Aggregate Data"
    ]
  }
]